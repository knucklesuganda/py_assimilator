{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Assimilator - the best Python patterns for the best projects Install now pip install py-assimilator pip install py-assimilator[alchemy] - Optional SQLAlchemy support pip install py-assimilator[kafka] - Optional Kafka support pip install py-assimilator[redis] - Optional Redis support pip install py-assimilator[mongo] - Optional MongoDB support Simple example Example usage of the code to create a user using all the DDD patterns: from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository from assimilator.core.database import UnitOfWork def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : repository = uow . repository # Get Repository pattern new_user = repository . save ( username = username , email = email , balance = 0 ) uow . commit () # Securely save the data return new_user user_repository = AlchemyRepository ( session = alchemy_session , # alchemy db session model = User , # alchemy user model ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) create_user ( username = \"Andrey\" , email = \"python.on.papyrus@gmail.com\" , uow = user_uow , ) Why do I need it? Patterns are very useful for good code, but only to some extent. Most of them are not suitable for real life applications. DDD(Domain-driven design) is one of the most popular ways of development today, but nobody explains how to write most of DDD patterns in Python. Even if they do, life gives you another issue that cannot be solved with a simple algorithm. That is why Andrey created a library for the patterns that he uses in his projects daily. Watch our Demo to find out more about pyAssimilator capabilities. Sources Github PyPI Documentation Github Author's YouTube RU Author's YouTube ENG Video tutorials There are also video tutorials that help you get everything you need to know about the library. Stars history Types of patterns These are different use cases for the patterns implemented: Database - patterns for database/data layer interactions. Events(in development) - projects with events or event-driven architecture. Unidentified - patterns that are useful for different purposes. Available providers Providers are different patterns for external modules like SQLAlchemy or FastAPI. Alchemy(Database, Events) - patterns for SQLAlchemy for both database and events. Kafka(Events) - patterns in Kafka related to events. Internal(Database, Events) - internal is the type of provider that saves everything in memory(dict, list and all the tools within your app). Redis(Database, Events) - redis_ allows us to work with Redis memory database. MongoDB(Database) - mongo allows us to work with MongoDB database.","title":"Introduction"},{"location":"#assimilator-the-best-python-patterns-for-the-best-projects","text":"","title":"Assimilator - the best Python patterns for the best projects"},{"location":"#install-now","text":"pip install py-assimilator pip install py-assimilator[alchemy] - Optional SQLAlchemy support pip install py-assimilator[kafka] - Optional Kafka support pip install py-assimilator[redis] - Optional Redis support pip install py-assimilator[mongo] - Optional MongoDB support","title":"Install now"},{"location":"#simple-example","text":"Example usage of the code to create a user using all the DDD patterns: from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository from assimilator.core.database import UnitOfWork def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : repository = uow . repository # Get Repository pattern new_user = repository . save ( username = username , email = email , balance = 0 ) uow . commit () # Securely save the data return new_user user_repository = AlchemyRepository ( session = alchemy_session , # alchemy db session model = User , # alchemy user model ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) create_user ( username = \"Andrey\" , email = \"python.on.papyrus@gmail.com\" , uow = user_uow , )","title":"Simple example"},{"location":"#why-do-i-need-it","text":"Patterns are very useful for good code, but only to some extent. Most of them are not suitable for real life applications. DDD(Domain-driven design) is one of the most popular ways of development today, but nobody explains how to write most of DDD patterns in Python. Even if they do, life gives you another issue that cannot be solved with a simple algorithm. That is why Andrey created a library for the patterns that he uses in his projects daily. Watch our Demo to find out more about pyAssimilator capabilities.","title":"Why do I need it?"},{"location":"#sources","text":"Github PyPI Documentation Github Author's YouTube RU Author's YouTube ENG","title":"Sources"},{"location":"#video-tutorials","text":"There are also video tutorials that help you get everything you need to know about the library.","title":"Video tutorials"},{"location":"#stars-history","text":"","title":"Stars history"},{"location":"#types-of-patterns","text":"These are different use cases for the patterns implemented: Database - patterns for database/data layer interactions. Events(in development) - projects with events or event-driven architecture. Unidentified - patterns that are useful for different purposes.","title":"Types of patterns"},{"location":"#available-providers","text":"Providers are different patterns for external modules like SQLAlchemy or FastAPI. Alchemy(Database, Events) - patterns for SQLAlchemy for both database and events. Kafka(Events) - patterns in Kafka related to events. Internal(Database, Events) - internal is the type of provider that saves everything in memory(dict, list and all the tools within your app). Redis(Database, Events) - redis_ allows us to work with Redis memory database. MongoDB(Database) - mongo allows us to work with MongoDB database.","title":"Available providers"},{"location":"concepts/","text":"How do we build these patterns 1. Dependency injection Dependency injection is a really important concept in assimilator. We do not use any additional dependency injection frameworks, but all the patterns inject different components into themselves. If you want to know more about DI 2. SOLID SOLID principles are highly used in assimilator. That means, that in theory you can replace one pattern to another and experience no trouble in using them. That is why it is not advised to create your own function in patterns, but you can easily override them. For example, you don't want to create: createUsers() in Repository pattern, but can override save() function without any problems . With that said, it is almost impossible to write such vast variety of patterns without breaking some principles. But, if you have any ideas on how to fix that, then be sure to check out our GitHub 3. Domain-driven design Most of the patterns here are used in Domain driven design. You do not really need to know all the intricacies, but make sure that you know the basics of it. 4. Reusable patterns The best thing about our patterns is that you can write your code for SQLAlchemy, then change it to Redis, then change it to Kafka, and finally test it with Python dictionaries. The thing is, you only have to change your pattern creation code, everything else stays the same. All the functions work the same in all the patterns that we create.","title":"Concepts"},{"location":"concepts/#how-do-we-build-these-patterns","text":"","title":"How do we build these patterns"},{"location":"concepts/#1-dependency-injection","text":"Dependency injection is a really important concept in assimilator. We do not use any additional dependency injection frameworks, but all the patterns inject different components into themselves. If you want to know more about DI","title":"1. Dependency injection"},{"location":"concepts/#2-solid","text":"SOLID principles are highly used in assimilator. That means, that in theory you can replace one pattern to another and experience no trouble in using them. That is why it is not advised to create your own function in patterns, but you can easily override them. For example, you don't want to create: createUsers() in Repository pattern, but can override save() function without any problems . With that said, it is almost impossible to write such vast variety of patterns without breaking some principles. But, if you have any ideas on how to fix that, then be sure to check out our GitHub","title":"2. SOLID"},{"location":"concepts/#3-domain-driven-design","text":"Most of the patterns here are used in Domain driven design. You do not really need to know all the intricacies, but make sure that you know the basics of it.","title":"3. Domain-driven design"},{"location":"concepts/#4-reusable-patterns","text":"The best thing about our patterns is that you can write your code for SQLAlchemy, then change it to Redis, then change it to Kafka, and finally test it with Python dictionaries. The thing is, you only have to change your pattern creation code, everything else stays the same. All the functions work the same in all the patterns that we create.","title":"4. Reusable patterns"},{"location":"services/","text":"Service is the main part of your business logic. It allows you to use all the patterns together, and you will probably write services yourself. But, there are some classes that can help you with that. CRUD Service Example For example, you may want to write your own CRUD services, but, there is a class that helps you with this: # dependencies.py from assimilator.core.database import UnitOfWork from assimilator.core.services.crud import CRUDService def get_repository (): # create repository ... def get_uow () -> UnitOfWork : # create UnitOfWork ... def get_service (): \"\"\" This function creates CRUDService and accepts UnitOfWork as a parameter. \"\"\" return CRUDService ( uow = get_uow ()) Then, we can integrate it with any web framework of our choice. I will use some pseudocode for that: from web_framework import Router , Response from assimilator.core.services import CRUDService from dependencies import get_service router = Router () @router . list ( '/' ) def list_all_users (): service : CRUDService = get_service () # Use list function to get all users from the service return Response ( service . list ()) So, basically, CRUDService allows you to quickly create all the functions for data interactions using any kind of pattern. You can also find a full FastAPI example here . CRUD Service methods list This function allows you to return multiple entities. Used for Read operation in CRUD. *filters - any kind of filters passed to filter specification. lazy - whether to run list() as a lazy command. False by default. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . list ( User . username . not_ ( \"Andrey\" ), # Direct SQLAlchemy filter id__gt = 20 , # only where id > 20 lazy = True , # as a lazy query ) get This function allows you to return one entity. Used for Read operation in CRUD. *filters - any kind of filters passed to filter specification. lazy - whether to run get() as a lazy command. False by default. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . get ( User . username == \"Andrey\" , # Direct SQLAlchemy filter id = 20 , # only where id == 20 lazy = True , # as a lazy query ) create This function allows you to create entities. Used for CREATE operation in CRUD. obj_data - dict with entity data or Model that you want to create. # For example, you may use it like this: service . create ({ \"username\" : \"Andrey\" , \"balances\" : [ # Foreign key { \"amount\" : 100 , \"currency\" : { # Foreign key \"name\" : \"USD\" , \"country\" : \"USA\" , }, }, ], }) # You may also provide the model itself: user = User ( username = \"Andrey\" ) user . balances . add ( Balance ( amount = 100 , currency = Currency ( name = \"USD\" , country = \"USA\" ) ) ) service . create ( user ) The second method is direct, and we would advise you to you indirect methods(the first one with dict) when possible. update This function allows you to update one entity. Used for Update operation in CRUD. update_data - dictionary of updated values *filters - any kind of filters passed to filter specification. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . update ( id = 1 , # user with ID 1 update_data = { \"username\" : \"Andrey-2\" , # will have this new username }, ) Important notice on foreign keys. We do not know how to effectively update them with indirect coding styles. So, update() only works with simple models now. But, you are free to override the function and put your foreign key handlers in there. Also, if you have an idea on how to improve update() or any other thing in Assimilator - be sure to open a pull request! delete This function allows you to delete one entity. Used for Delete operation in CRUD. *filters - any kind of filters passed to filter specification. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . delete ( id = 1 , # delete user with id == 1 username = \"Andrey\" , # and username == \"Andrey\" ) You can also find a full FastAPI example here .","title":"Services Tutorial"},{"location":"services/#crud-service-example","text":"For example, you may want to write your own CRUD services, but, there is a class that helps you with this: # dependencies.py from assimilator.core.database import UnitOfWork from assimilator.core.services.crud import CRUDService def get_repository (): # create repository ... def get_uow () -> UnitOfWork : # create UnitOfWork ... def get_service (): \"\"\" This function creates CRUDService and accepts UnitOfWork as a parameter. \"\"\" return CRUDService ( uow = get_uow ()) Then, we can integrate it with any web framework of our choice. I will use some pseudocode for that: from web_framework import Router , Response from assimilator.core.services import CRUDService from dependencies import get_service router = Router () @router . list ( '/' ) def list_all_users (): service : CRUDService = get_service () # Use list function to get all users from the service return Response ( service . list ()) So, basically, CRUDService allows you to quickly create all the functions for data interactions using any kind of pattern. You can also find a full FastAPI example here .","title":"CRUD Service Example"},{"location":"services/#crud-service-methods","text":"","title":"CRUD Service methods"},{"location":"services/#list","text":"This function allows you to return multiple entities. Used for Read operation in CRUD. *filters - any kind of filters passed to filter specification. lazy - whether to run list() as a lazy command. False by default. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . list ( User . username . not_ ( \"Andrey\" ), # Direct SQLAlchemy filter id__gt = 20 , # only where id > 20 lazy = True , # as a lazy query )","title":"list"},{"location":"services/#get","text":"This function allows you to return one entity. Used for Read operation in CRUD. *filters - any kind of filters passed to filter specification. lazy - whether to run get() as a lazy command. False by default. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . get ( User . username == \"Andrey\" , # Direct SQLAlchemy filter id = 20 , # only where id == 20 lazy = True , # as a lazy query )","title":"get"},{"location":"services/#create","text":"This function allows you to create entities. Used for CREATE operation in CRUD. obj_data - dict with entity data or Model that you want to create. # For example, you may use it like this: service . create ({ \"username\" : \"Andrey\" , \"balances\" : [ # Foreign key { \"amount\" : 100 , \"currency\" : { # Foreign key \"name\" : \"USD\" , \"country\" : \"USA\" , }, }, ], }) # You may also provide the model itself: user = User ( username = \"Andrey\" ) user . balances . add ( Balance ( amount = 100 , currency = Currency ( name = \"USD\" , country = \"USA\" ) ) ) service . create ( user ) The second method is direct, and we would advise you to you indirect methods(the first one with dict) when possible.","title":"create"},{"location":"services/#update","text":"This function allows you to update one entity. Used for Update operation in CRUD. update_data - dictionary of updated values *filters - any kind of filters passed to filter specification. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . update ( id = 1 , # user with ID 1 update_data = { \"username\" : \"Andrey-2\" , # will have this new username }, ) Important notice on foreign keys. We do not know how to effectively update them with indirect coding styles. So, update() only works with simple models now. But, you are free to override the function and put your foreign key handlers in there. Also, if you have an idea on how to improve update() or any other thing in Assimilator - be sure to open a pull request!","title":"update"},{"location":"services/#delete","text":"This function allows you to delete one entity. Used for Delete operation in CRUD. *filters - any kind of filters passed to filter specification. **kwargs_filters - any kind of filters passed to filter specification. # For example, you may use it like this: service . delete ( id = 1 , # delete user with id == 1 username = \"Andrey\" , # and username == \"Andrey\" ) You can also find a full FastAPI example here .","title":"delete"},{"location":"unidentified_patterns/","text":"LazyCommand LazyCommand is an object that allows you to postpone the execution of any kind of code. You will typically use it in some kind of pattern with lazy=True argument. But, if you want to create your own LazyCommand , then you can do it like this: from assimilator.core.patterns import LazyCommand def func ( a : int , b : int ): # Function we want to postpone return a + b - 10 * a ** b lazy_func : LazyCommand [ int ] = LazyCommand ( command = func , # command is the function you want to execute a = 10 , # argument a b = 20 , # argument b ) print ( \"No execution yet\" ) lazy_func () # function is executed here Operations You can do the following things with LazyCommand : # call it lazy_func () # iterate it(if the result is iterable) for value in lazy_func : print ( value ) # use it as boolean if lazy_func : print ( \"Result gives True\" ) # get attributes from the result print ( lazy_obj . obj_attr ) # compare it lazy_obj > 10 Result retrieval When we use any of the LazyCommand methods, we must run the function for the result. That means, that if we want to use two methods on the same LazyCommand object, we must store the result in a variable not to run calculation twice. from assimilator.core.patterns import LazyCommand lazy_func : LazyCommand [ int ] = LazyCommand ( command = run_api_query , # runs API query # no other arguments present ) print ( \"API result:\" , lazy_func ()) # run_api_query() execution if lazy_func : # The result is stored in the LazyCommand, no execution needed print ( \"API returned true!\" ) Decorator Sometimes you want to make your function lazy, but you don't want to write any additional code for that. If that is the case, then you can use LazyCommand decorator: from assimilator.core.patterns import LazyCommand @LazyCommand . decorate # decorate LazyCommand def get_user_from_api ( id : int , lazy : bool = False , ): ... # Now, we can run it like this: lazy_command = get_user_from_api ( id = 1 , lazy = True ) # We can also execute it normally: user = get_user_from_api ( id = 1 )","title":"Unidentified Patterns"},{"location":"unidentified_patterns/#lazycommand","text":"LazyCommand is an object that allows you to postpone the execution of any kind of code. You will typically use it in some kind of pattern with lazy=True argument. But, if you want to create your own LazyCommand , then you can do it like this: from assimilator.core.patterns import LazyCommand def func ( a : int , b : int ): # Function we want to postpone return a + b - 10 * a ** b lazy_func : LazyCommand [ int ] = LazyCommand ( command = func , # command is the function you want to execute a = 10 , # argument a b = 20 , # argument b ) print ( \"No execution yet\" ) lazy_func () # function is executed here","title":"LazyCommand"},{"location":"unidentified_patterns/#operations","text":"You can do the following things with LazyCommand : # call it lazy_func () # iterate it(if the result is iterable) for value in lazy_func : print ( value ) # use it as boolean if lazy_func : print ( \"Result gives True\" ) # get attributes from the result print ( lazy_obj . obj_attr ) # compare it lazy_obj > 10","title":"Operations"},{"location":"unidentified_patterns/#result-retrieval","text":"When we use any of the LazyCommand methods, we must run the function for the result. That means, that if we want to use two methods on the same LazyCommand object, we must store the result in a variable not to run calculation twice. from assimilator.core.patterns import LazyCommand lazy_func : LazyCommand [ int ] = LazyCommand ( command = run_api_query , # runs API query # no other arguments present ) print ( \"API result:\" , lazy_func ()) # run_api_query() execution if lazy_func : # The result is stored in the LazyCommand, no execution needed print ( \"API returned true!\" )","title":"Result retrieval"},{"location":"unidentified_patterns/#decorator","text":"Sometimes you want to make your function lazy, but you don't want to write any additional code for that. If that is the case, then you can use LazyCommand decorator: from assimilator.core.patterns import LazyCommand @LazyCommand . decorate # decorate LazyCommand def get_user_from_api ( id : int , lazy : bool = False , ): ... # Now, we can run it like this: lazy_command = get_user_from_api ( id = 1 , lazy = True ) # We can also execute it normally: user = get_user_from_api ( id = 1 )","title":"Decorator"},{"location":"video_tutorials/","text":"Tutorial 1 PyAssimilator introduction Pattern types Various concepts Tutorial 2 Repository pattern Basic User save/read Internal module Tutorial 3 Transaction management Unit Of Work","title":"Video Tutorials"},{"location":"video_tutorials/#tutorial-1","text":"PyAssimilator introduction Pattern types Various concepts","title":"Tutorial 1"},{"location":"video_tutorials/#tutorial-2","text":"Repository pattern Basic User save/read Internal module","title":"Tutorial 2"},{"location":"video_tutorials/#tutorial-3","text":"Transaction management Unit Of Work","title":"Tutorial 3"},{"location":"alchemy/database/","text":"SQLAlchemy Database patterns SQLAlchemy is a Python ORM that is used with Relational databases. This page tells you about pyAssimilator patterns for SQLAlchemy. IMPORTANT ! SQLAlchemy team have just released SQLAlchemy 2.0. Fortunately, all of our patterns work with both versions 1 and 2 of SQLAlchemy. But, that is going to change in the future, so we recommend switching to SQLAlchemy 2.0. Here is a guide on how to do that . What patterns do we use? Database, in our case, is any data storage that can be connected to SQLAlchemy. It can be PostgreSQL, MySQL, Oracle and others. Please, check SQLAlchemy docs to find out more. 5 main patterns with databases: AlchemyRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. AlchemyUnitOfWork - works with SQLAlchemy transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in AlchemyRepository . AlchemySpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results. Starting with SQLAlchemy You should read our Basic Tutorials to be able to understand basic things. We also show SQLAlchemy examples in there! Run this command to install our library with SQLAlchemy dependencies: pip install py-assimilator[alchemy] You don't need to run this command if you have PyAssimilator and SQLAlchemy installed. Creating your models The first thing that you have to do is create your SQLAlchemy models. You can use anything from SQLAlchemy module to create your models. All mapping types, column types, tables, foreign keys, and other things are supported by PyAssimilator. We are going to create User model using declarative mappings: # models.py from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) Base = declarative_base () DatabaseSession = sessionmaker ( bind = engine ) class User ( Base ): __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) balance = Column ( Float ()) Base . metadata . create_all ( engine ) It's the same model as we have in our Basic Tutorial. Creating our patterns Once you have your models, you are going to start creating your Alchemy patterns. We are going to create AlchemyUnitOfWork and AlchemyRepository to manage data and transactions: # dependencies.py from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository from models import DatabaseSession , User def get_repository (): return AlchemyRepository ( session = DatabaseSession (), # database session model = User , # our main model ) def get_uow (): return AlchemyUnitOfWork ( repository = get_repository ()) AlchemyRepository must accept an SQLAlchemy session and the model that we created. session - SQLAlchemy session to the database. model - the model that you created. You can read about different model mappings here . You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, HOWEVER, we suggest that you use different objects inside your code. Using our patterns You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their SQLAlchemy descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the AlchemyUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call AlchemyUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # create our AlchemyUnitOfWork def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) That's it. Nothing really changes from the basic tutorial, as all the things stay the same in all of our patterns. However, there are specific things that you may not use, but still need to know. You can read about them below: Alchemy Specifications When we sort, filter, join and mutate our SQLAlchemy query, we use Alchemy specifications. Here is how they work, and what you can do with them. AlchemyFilter AlchemyFilter is used to filter your data using some filter. You can access it with one of the following methods(ordered by code quality): Indirect access from Repository: # AlchemyFilter for repositories that use AlchemySpecificationList repository . specs . filter () Direct access using an import statement: from assimilator.alchemy.database.specifications import alchemy_filter , AlchemyFilter # Both objects are the same alchemy_filter () AlchemyFilter () Direct access from an AlchemySpecificationList : from assimilator.alchemy.database.specifications import AlchemySpecificationList AlchemySpecificationList . filter () If you want to add filters, then you can use filtering options or direct alchemy filters: Filtering options: repository . specs . filter ( id = 1 , # id == 1 age__gt = 18 , # age > 18 username = \"Andrey\" , # username == \"Andrey\" user_domain__like = \"%.com%\" , # user_domain LIKE \"%.com%\" ) You can check out our Basic Tutorials for more filtering options. Direct filters: Sometimes you don't want to use filtering options, or you have such a complicated query, that filtering options do not allow you to fully execute it. Then, you can provide direct filters: repository . specs . filter ( # The same as in filtering options username=\"Andrey\" User . username == \"Andrey\" ) Direct filter is anything that can be added to an SQLAlchemy query to specify the results. But, we expose our SQLAlchemy dependency when using direct filters. If you want to also accounts for that, then you can create your own specification that you can use later: # specifications.py from assimilator.core.database import specification @specification def filter_by_username ( username : str , query ): return query . where ( User . username == username ) # services.py # Then, you call your specification like this: repository . filter ( filter_by_username ( \"Andrey\" ) ) This way, you can remove SQLAlchemy dependency from your code. alchemy_order specification alchemy_order is a specification that you can use in order to sort your results. You just provide the columns in your model that are going to be use for sorting. For example: # order by username repository . specs . order ( 'username' ) # order by username and id repository . specs . order ( 'id' , 'username' ) # order by balance DESC with direct import from assimilator.alchemy.database.specifications import alchemy_order alchemy_order ( '-balance' ) alchemy_paginate specification alchemy_paginate is a specification that you can use to limit your results You can provide limit and offset to paginate your data. For example: # only first 10 repository . specs . paginate ( limit = 10 ) # all except for first 5 repository . specs . paginate ( offset = 5 ) # offset by 10 and get the next 10 results # with direct import from assimilator.alchemy.database.specifications import alchemy_paginate alchemy_paginate ( limit = 10 , offset = 10 ) alchemy_join specification alchemy_join is a specification that you can use to join multiple models together. You can provide the name of the relationship or the model itself. It's better to provide just the name, as it is indirect style. For example: # join by relationship name with addresses table repository . specs . join ( 'addresses' ) # join by relationship itself with addresses table repository . specs . join ( User . addresses ) # join by model repository . specs . join ( Address ) # join multiple entities # with direct import from assimilator.alchemy.database.specifications import alchemy_join alchemy_join ( User . addresses , 'friends' , 'he_he_he_ha' ) Join arguments: Sometimes you need to provide additional arguments to your joins(You can read about them in SQLAlchemy docs). You can do that as follows: repository . specs . join ( 'addresses' , join_args = [ { 'full' : True # full join for addresses }, ], ) # Here is an example with multiple joins repository . specs . join ( 'addresses' , User . friends , join_args = [ { 'full' : True }, # full join for addresses { # custom join clause for User.friends \"onclause\" : Friend . user_id == 1 } ], ) alchemy_only specification alchemy_only is a specification that you can use to only select specific columns and optimize your queries. Examples: # Get only id and username columns repository . specs . only ( 'id' , 'username' ) # Get only balance column with direct import from assimilator.alchemy.database.specifications import alchemy_only alchemy_only ( 'balance' ) AlchemySpecificationList If you want to create your custom SpecificationList using AlchemySpecificationList as a basis, then you can import it like this: from assimilator.alchemy.database.specifications import AlchemySpecificationList","title":"Database"},{"location":"alchemy/database/#sqlalchemy-database-patterns","text":"SQLAlchemy is a Python ORM that is used with Relational databases. This page tells you about pyAssimilator patterns for SQLAlchemy. IMPORTANT ! SQLAlchemy team have just released SQLAlchemy 2.0. Fortunately, all of our patterns work with both versions 1 and 2 of SQLAlchemy. But, that is going to change in the future, so we recommend switching to SQLAlchemy 2.0. Here is a guide on how to do that .","title":"SQLAlchemy Database patterns"},{"location":"alchemy/database/#what-patterns-do-we-use","text":"Database, in our case, is any data storage that can be connected to SQLAlchemy. It can be PostgreSQL, MySQL, Oracle and others. Please, check SQLAlchemy docs to find out more. 5 main patterns with databases: AlchemyRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. AlchemyUnitOfWork - works with SQLAlchemy transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in AlchemyRepository . AlchemySpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results.","title":"What patterns do we use?"},{"location":"alchemy/database/#starting-with-sqlalchemy","text":"You should read our Basic Tutorials to be able to understand basic things. We also show SQLAlchemy examples in there! Run this command to install our library with SQLAlchemy dependencies: pip install py-assimilator[alchemy] You don't need to run this command if you have PyAssimilator and SQLAlchemy installed.","title":"Starting with SQLAlchemy"},{"location":"alchemy/database/#creating-your-models","text":"The first thing that you have to do is create your SQLAlchemy models. You can use anything from SQLAlchemy module to create your models. All mapping types, column types, tables, foreign keys, and other things are supported by PyAssimilator. We are going to create User model using declarative mappings: # models.py from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) Base = declarative_base () DatabaseSession = sessionmaker ( bind = engine ) class User ( Base ): __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) balance = Column ( Float ()) Base . metadata . create_all ( engine ) It's the same model as we have in our Basic Tutorial.","title":"Creating your models"},{"location":"alchemy/database/#creating-our-patterns","text":"Once you have your models, you are going to start creating your Alchemy patterns. We are going to create AlchemyUnitOfWork and AlchemyRepository to manage data and transactions: # dependencies.py from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository from models import DatabaseSession , User def get_repository (): return AlchemyRepository ( session = DatabaseSession (), # database session model = User , # our main model ) def get_uow (): return AlchemyUnitOfWork ( repository = get_repository ()) AlchemyRepository must accept an SQLAlchemy session and the model that we created. session - SQLAlchemy session to the database. model - the model that you created. You can read about different model mappings here . You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, HOWEVER, we suggest that you use different objects inside your code.","title":"Creating our patterns"},{"location":"alchemy/database/#using-our-patterns","text":"You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their SQLAlchemy descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the AlchemyUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call AlchemyUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # create our AlchemyUnitOfWork def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) That's it. Nothing really changes from the basic tutorial, as all the things stay the same in all of our patterns. However, there are specific things that you may not use, but still need to know. You can read about them below:","title":"Using our patterns"},{"location":"alchemy/database/#alchemy-specifications","text":"When we sort, filter, join and mutate our SQLAlchemy query, we use Alchemy specifications. Here is how they work, and what you can do with them.","title":"Alchemy Specifications"},{"location":"alchemy/database/#alchemyfilter","text":"AlchemyFilter is used to filter your data using some filter. You can access it with one of the following methods(ordered by code quality): Indirect access from Repository: # AlchemyFilter for repositories that use AlchemySpecificationList repository . specs . filter () Direct access using an import statement: from assimilator.alchemy.database.specifications import alchemy_filter , AlchemyFilter # Both objects are the same alchemy_filter () AlchemyFilter () Direct access from an AlchemySpecificationList : from assimilator.alchemy.database.specifications import AlchemySpecificationList AlchemySpecificationList . filter () If you want to add filters, then you can use filtering options or direct alchemy filters:","title":"AlchemyFilter"},{"location":"alchemy/database/#filtering-options","text":"repository . specs . filter ( id = 1 , # id == 1 age__gt = 18 , # age > 18 username = \"Andrey\" , # username == \"Andrey\" user_domain__like = \"%.com%\" , # user_domain LIKE \"%.com%\" ) You can check out our Basic Tutorials for more filtering options.","title":"Filtering options:"},{"location":"alchemy/database/#direct-filters","text":"Sometimes you don't want to use filtering options, or you have such a complicated query, that filtering options do not allow you to fully execute it. Then, you can provide direct filters: repository . specs . filter ( # The same as in filtering options username=\"Andrey\" User . username == \"Andrey\" ) Direct filter is anything that can be added to an SQLAlchemy query to specify the results. But, we expose our SQLAlchemy dependency when using direct filters. If you want to also accounts for that, then you can create your own specification that you can use later: # specifications.py from assimilator.core.database import specification @specification def filter_by_username ( username : str , query ): return query . where ( User . username == username ) # services.py # Then, you call your specification like this: repository . filter ( filter_by_username ( \"Andrey\" ) ) This way, you can remove SQLAlchemy dependency from your code.","title":"Direct filters:"},{"location":"alchemy/database/#alchemy_order-specification","text":"alchemy_order is a specification that you can use in order to sort your results. You just provide the columns in your model that are going to be use for sorting. For example: # order by username repository . specs . order ( 'username' ) # order by username and id repository . specs . order ( 'id' , 'username' ) # order by balance DESC with direct import from assimilator.alchemy.database.specifications import alchemy_order alchemy_order ( '-balance' )","title":"alchemy_order specification"},{"location":"alchemy/database/#alchemy_paginate-specification","text":"alchemy_paginate is a specification that you can use to limit your results You can provide limit and offset to paginate your data. For example: # only first 10 repository . specs . paginate ( limit = 10 ) # all except for first 5 repository . specs . paginate ( offset = 5 ) # offset by 10 and get the next 10 results # with direct import from assimilator.alchemy.database.specifications import alchemy_paginate alchemy_paginate ( limit = 10 , offset = 10 )","title":"alchemy_paginate specification"},{"location":"alchemy/database/#alchemy_join-specification","text":"alchemy_join is a specification that you can use to join multiple models together. You can provide the name of the relationship or the model itself. It's better to provide just the name, as it is indirect style. For example: # join by relationship name with addresses table repository . specs . join ( 'addresses' ) # join by relationship itself with addresses table repository . specs . join ( User . addresses ) # join by model repository . specs . join ( Address ) # join multiple entities # with direct import from assimilator.alchemy.database.specifications import alchemy_join alchemy_join ( User . addresses , 'friends' , 'he_he_he_ha' )","title":"alchemy_join specification"},{"location":"alchemy/database/#join-arguments","text":"Sometimes you need to provide additional arguments to your joins(You can read about them in SQLAlchemy docs). You can do that as follows: repository . specs . join ( 'addresses' , join_args = [ { 'full' : True # full join for addresses }, ], ) # Here is an example with multiple joins repository . specs . join ( 'addresses' , User . friends , join_args = [ { 'full' : True }, # full join for addresses { # custom join clause for User.friends \"onclause\" : Friend . user_id == 1 } ], )","title":"Join arguments:"},{"location":"alchemy/database/#alchemy_only-specification","text":"alchemy_only is a specification that you can use to only select specific columns and optimize your queries. Examples: # Get only id and username columns repository . specs . only ( 'id' , 'username' ) # Get only balance column with direct import from assimilator.alchemy.database.specifications import alchemy_only alchemy_only ( 'balance' )","title":"alchemy_only specification"},{"location":"alchemy/database/#alchemyspecificationlist","text":"If you want to create your custom SpecificationList using AlchemySpecificationList as a basis, then you can import it like this: from assimilator.alchemy.database.specifications import AlchemySpecificationList","title":"AlchemySpecificationList"},{"location":"alchemy/events/","text":"Alchemy Events - STILL IN DEVELOPMENT","title":"Alchemy Events - STILL IN DEVELOPMENT"},{"location":"alchemy/events/#alchemy-events-still-in-development","text":"","title":"Alchemy Events - STILL IN DEVELOPMENT"},{"location":"api_reference/core/","text":"Repository __init__() session: SessionT - each repository has a session that works as the primary data source. It can be your database connection, a text file or a data structure. initial_query: Optional[SessionT] = None - the initial query that you use in the data storage. We will show how it works later. It can be an SQL query, a key in the dictionary or anything else. specifications: SpecificationList - an object that contains links to the specifications that are used to create your queries model: Type[ModelT] - class of the model that we want to work with. It can be an SQLAlchemy model, Pydantic model, or your custom class. specifications: Type[SpecificationList] - SpecificationList class with all the specifications that you need error_wrapper: Optional[ErrorWrapper] = ErrorWrapper() - ErrorWrapper class that allows you to convert external providers errors to your custom errors. _check_obj_is_specification() -> Tuple[Optional[ModelT], Iterable[SpecificationType]] This function is called for parts of the code that use both obj and *specifications. We check that if the obj is a model. If that is the case, we swap it around. obj: ModelT specifications: Iterable[SpecificationType] specs -> Type[SpecificationList] Property for a shorter name version of specifications attribute. _get_initial_query() -> QueryT Returns initial query for the data querying. Can return override_query as the initial query if it is present. override_query: Optional[QueryT] = None - overrides the initial query. Used for more flexibility in querying. _apply_specifications() -> QueryT Applies Specifications to the query. Must not be used directly. apply specifications gets a list of specifications and applies them to the query returned in _get_initial_query(). The idea is the following: each specification gets a query and adds some filters to it. At the end we get a fully working query modified with the specifications provided by the user. specifications: Iterable[Specifications] - an iterable of specifications that can be used to specify some conditions in the query Specification is a pattern that adds filters or anything else that specifies what kind of data we want. get() get is the function used to query the data storage and return one entity. You supply a list of specifications that get you the entity from the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument filter() filters is the function used to query the data storage and return many entities. You supply a list of specifications that filter entities in the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument save() Adds the objects to the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just add them ). delete() Deletes the objects from the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just delete them from your session ). update() Updates the objects in the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just update them in your session ). is_modified() Checks whether an obj was modified or not. If any value changes within the object, then it must return True refresh() Updates the object values with the values in the data storage. That can be useful if you want to create an object and get its id that was generated in the storage, or if you just want to have the latest saved version of the object. count() Counts the objects while applying specifications to the query. Give no specifications to count the whole data storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future Creating your own repository: If you want to create your own repository, then you are going to have to override all the functions above. But, please, do not make new functions available to the outer world. You can do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def _users_private_func ( self ): # Cannot be called outside return 'Do something' And call that function inside of your repository. But, never do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def get_ser_by_id ( self ): # Cannot be called outside return self . get ( filter_specification ( id = 1 )) Since it is going to be really hard for you to replace one repository to another. Example: from assimilator.core.database import Repository from users.repository import UserRepository from products.repository import ProductRepository def get_by_id ( id , repository : Repository ): return repository . get ( filter_specification ( id = 1 )) get_by_id ( UserRepository ()) get_by_id ( ProductRepository ()) # You can call the function with both repositories, and it will probably work fine How to create my repositories? You want to create a repository for entities in your projects. That means, that if you have some auxiliary table in your app, then it probably should not have a repository. But, things like Users, Products, Orders and others might have their own repository. That does not mean that you will create a lot of classes, but please do not add repositories for every class in your system . If you want to read more, please, look into the Domain-Driven Development books. Specification Specification is the pattern that adds values, filters, joins or anything else to the query in your repository. It can also work as a filter for your objects. Class-based specifications class Specification ( ABC ): @abstractmethod def apply ( self , query ): raise NotImplementedError ( \"Specification must specify apply()\" ) def __call__ ( self , query ): return self . apply ( query ) apply(query) -> query apply is the main functions that you are going to override. It is used in order to specify new things in your query. You get the query in the specification and return an updated version of query. For example, if your query has a filter function, and you want to filter by username, then you can create this: from assimilator.core.database import Specification class UsernameSpecification ( Specification ): def __init__ ( self , username : str ): super ( UsernameSpecification , self ) . __init__ () self . username = username def apply ( self , query ): return query . filter ( username = username ) Here we do the following: 1) save the username in the constructor 2) override apply() and return an updated version of the query provided The usage of this specification will look like this: repository = UserRepository ( session ) user = repository . get ( UsernameSpecification ( username = \"python.on.papyrus\" )) The repository will apply the specification and return the results. Functional specifications __call__() functions receives the query and call apply() in the specification. This is used in order to make functional specifications possible. __call__() allows you to call an object as a function: obj() . This way, we can use functional specifications and class-based specifications together. If you want to create a functional specification, then you need to use the @specification decorator: from assimilator.core.database import specification @specification def username_filter ( query , username : str ): return query . filter ( username = username ) The function above is the equivalent of the UsernameSpecification class. Then, you are going to use it like this: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" )) Both types work the same, so you can choose the type of specifications that you like. But, you can also use them together: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" ), AgeGreaterSpecification ( age_gt = 18 ), JoinSpecification ( Friends ), ) SpecificationList SpecificationList is a static class that contains basic specifications for our repository. Specifications: filter() filters the data order() orders the data paginate() paginates the data(limits the results, offsets them). join() joins entities together(join a table, get related data). The reason we use SpecificationList is because we want to have an abstraction for our specifications. Take two examples: from dependencies import UserRepository from assimilator.alchemy.database import alchemy_filter def get_user ( id : int , repository : UserRepository ): return repository . get ( alchemy_filter ( id = id )) # we use alchemy_filter() directly In that example, we use alchemy_filter() directly, which may not seem as an issue, however, if we would want to change our UserRepository to work with RedisRepository , then we would have to change all of our specifications ourselves. In order to fix this, we can use SpecificationList: from dependencies import UserRepository def get_user ( id : int , repository : UserRepository ): return repository . get ( repository . specifications . filter ( id = id )) # we call the filter from repository specifications. Now, we only have to change the repository without worrying about other parts of the code. Here is how you can create your own SpecificationList: from assimilator.core.database import SpecificationList , specification , Specification from pagination_func import paginate_data @specification def filter_specification ( filters , query ): return query . do_filter ( filters ) @specification def join_specification ( query ): return query # we do not need joins in our data structure, so we leave it class OrderSpecification ( Specification ): def __init__ ( self , orders ): self . orders = orders def apply ( self , query ): return query . make_order ( self . orders ) class MySpecificationList ( SpecificationList ): filter = filter_specification # we use function name as the specification order = OrderSpecification # lambda specification paginate = paginate_data # imported specification join = join_specification Notice that we never call the functions, cause the only thing we need are links to the specifications. Then, when you build your Repository: from specifications import MySpecificationList repository = MyRepository ( session = session , specifications = MySpecificationList ) Once you have done that, the repository will use your specifications. Of course, you can still use specifications directly, but if you ever need to change the repository, then it may be a problem. LazyCommand Sometimes we don't want to execute the query right away. For example, for optimization purposes or some other purpose that requires us to delay the execution. In that case, you want to find lazy argument in the function that you are calling and set it to True . After that, a LazyCommand is going to be returned. That object allows you to call it as a function or iterate over it to get the results: from assimilator.core.database import Repository def print_all_usernames ( repository : Repository ): for user in repository . filter ( lazy = True ): print ( user . username ) # we don't want to receive a list of all the users, but want to iterate # through it and only get 1 user at a time def count_users_if_argument_true ( do_count , repository : Repository ): count_command = repository . count ( lazy = True ) # turn on lazy and get LazyCommand if do_count : return count_command () # call for the result return - 1 Unit of Work Unit of work allows us to work with transactions and repositories that change the data. The problem with repository is the transaction management. We want to make our transaction management as easy as possible, but repositories are not responsible for that. That is why we have units of work. They allow us to do the following: 1. Start the transaction 2. Provide the repository to the client code 3. If there are exceptions, errors, issues with the client code, rollback the transaction and remove all the changes 4. If everything is good, the client code calls the commit() function and finishes the data change 5. Unit of work closes the transaction __init__() repository: BaseRepository - The repository is provided in the UnitOfWork when we create it. The session to the data storage is going to be taken from the repository. begin() Starts the transaction. The function is called automatically. rollback() Removes all the changes from the transaction. You do not need to call that function, as it is called automatically if there is an error in your code. commit() Saves the changes to the data storage. While the repository only adds the temporary, this function is responsible for the final save. You need to call it yourself, it will not be called automatically like rollback() close() Closes the transaction. The function is called automatically. Here is how you can use UnitOfWork in your code: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork uow . commit () # commit the changes making them final. If the function is not called, nothing is saved. As you can see, you do not need to call any function except for commit() . You should also use context managers( with uow: ) to start the transaction and rollback if there is an exception: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork 1 / 0 # ZeroDivisionError. UnitOfWork calls rollback automatically. uow . commit () # nothing is saved, since the rollback was called.","title":"Core"},{"location":"api_reference/core/#repository","text":"","title":"Repository"},{"location":"api_reference/core/#__init__","text":"session: SessionT - each repository has a session that works as the primary data source. It can be your database connection, a text file or a data structure. initial_query: Optional[SessionT] = None - the initial query that you use in the data storage. We will show how it works later. It can be an SQL query, a key in the dictionary or anything else. specifications: SpecificationList - an object that contains links to the specifications that are used to create your queries model: Type[ModelT] - class of the model that we want to work with. It can be an SQLAlchemy model, Pydantic model, or your custom class. specifications: Type[SpecificationList] - SpecificationList class with all the specifications that you need error_wrapper: Optional[ErrorWrapper] = ErrorWrapper() - ErrorWrapper class that allows you to convert external providers errors to your custom errors.","title":"__init__()"},{"location":"api_reference/core/#_check_obj_is_specification-tupleoptionalmodelt-iterablespecificationtype","text":"This function is called for parts of the code that use both obj and *specifications. We check that if the obj is a model. If that is the case, we swap it around. obj: ModelT specifications: Iterable[SpecificationType]","title":"_check_obj_is_specification() -&gt; Tuple[Optional[ModelT], Iterable[SpecificationType]]"},{"location":"api_reference/core/#specs-typespecificationlist","text":"Property for a shorter name version of specifications attribute.","title":"specs -&gt; Type[SpecificationList]"},{"location":"api_reference/core/#_get_initial_query-queryt","text":"Returns initial query for the data querying. Can return override_query as the initial query if it is present. override_query: Optional[QueryT] = None - overrides the initial query. Used for more flexibility in querying.","title":"_get_initial_query() -&gt; QueryT"},{"location":"api_reference/core/#_apply_specifications-queryt","text":"Applies Specifications to the query. Must not be used directly. apply specifications gets a list of specifications and applies them to the query returned in _get_initial_query(). The idea is the following: each specification gets a query and adds some filters to it. At the end we get a fully working query modified with the specifications provided by the user. specifications: Iterable[Specifications] - an iterable of specifications that can be used to specify some conditions in the query Specification is a pattern that adds filters or anything else that specifies what kind of data we want.","title":"_apply_specifications() -&gt; QueryT"},{"location":"api_reference/core/#get","text":"get is the function used to query the data storage and return one entity. You supply a list of specifications that get you the entity from the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument","title":"get()"},{"location":"api_reference/core/#filter","text":"filters is the function used to query the data storage and return many entities. You supply a list of specifications that filter entities in the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument","title":"filter()"},{"location":"api_reference/core/#save","text":"Adds the objects to the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just add them ).","title":"save()"},{"location":"api_reference/core/#delete","text":"Deletes the objects from the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just delete them from your session ).","title":"delete()"},{"location":"api_reference/core/#update","text":"Updates the objects in the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just update them in your session ).","title":"update()"},{"location":"api_reference/core/#is_modified","text":"Checks whether an obj was modified or not. If any value changes within the object, then it must return True","title":"is_modified()"},{"location":"api_reference/core/#refresh","text":"Updates the object values with the values in the data storage. That can be useful if you want to create an object and get its id that was generated in the storage, or if you just want to have the latest saved version of the object.","title":"refresh()"},{"location":"api_reference/core/#count","text":"Counts the objects while applying specifications to the query. Give no specifications to count the whole data storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future","title":"count()"},{"location":"api_reference/core/#creating-your-own-repository","text":"If you want to create your own repository, then you are going to have to override all the functions above. But, please, do not make new functions available to the outer world. You can do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def _users_private_func ( self ): # Cannot be called outside return 'Do something' And call that function inside of your repository. But, never do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def get_ser_by_id ( self ): # Cannot be called outside return self . get ( filter_specification ( id = 1 )) Since it is going to be really hard for you to replace one repository to another. Example: from assimilator.core.database import Repository from users.repository import UserRepository from products.repository import ProductRepository def get_by_id ( id , repository : Repository ): return repository . get ( filter_specification ( id = 1 )) get_by_id ( UserRepository ()) get_by_id ( ProductRepository ()) # You can call the function with both repositories, and it will probably work fine","title":"Creating your own repository:"},{"location":"api_reference/core/#how-to-create-my-repositories","text":"You want to create a repository for entities in your projects. That means, that if you have some auxiliary table in your app, then it probably should not have a repository. But, things like Users, Products, Orders and others might have their own repository. That does not mean that you will create a lot of classes, but please do not add repositories for every class in your system . If you want to read more, please, look into the Domain-Driven Development books.","title":"How to create  my repositories?"},{"location":"api_reference/core/#specification","text":"Specification is the pattern that adds values, filters, joins or anything else to the query in your repository. It can also work as a filter for your objects.","title":"Specification"},{"location":"api_reference/core/#class-based-specifications","text":"class Specification ( ABC ): @abstractmethod def apply ( self , query ): raise NotImplementedError ( \"Specification must specify apply()\" ) def __call__ ( self , query ): return self . apply ( query ) apply(query) -> query apply is the main functions that you are going to override. It is used in order to specify new things in your query. You get the query in the specification and return an updated version of query. For example, if your query has a filter function, and you want to filter by username, then you can create this: from assimilator.core.database import Specification class UsernameSpecification ( Specification ): def __init__ ( self , username : str ): super ( UsernameSpecification , self ) . __init__ () self . username = username def apply ( self , query ): return query . filter ( username = username ) Here we do the following: 1) save the username in the constructor 2) override apply() and return an updated version of the query provided The usage of this specification will look like this: repository = UserRepository ( session ) user = repository . get ( UsernameSpecification ( username = \"python.on.papyrus\" )) The repository will apply the specification and return the results.","title":"Class-based specifications"},{"location":"api_reference/core/#functional-specifications","text":"__call__() functions receives the query and call apply() in the specification. This is used in order to make functional specifications possible. __call__() allows you to call an object as a function: obj() . This way, we can use functional specifications and class-based specifications together. If you want to create a functional specification, then you need to use the @specification decorator: from assimilator.core.database import specification @specification def username_filter ( query , username : str ): return query . filter ( username = username ) The function above is the equivalent of the UsernameSpecification class. Then, you are going to use it like this: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" )) Both types work the same, so you can choose the type of specifications that you like. But, you can also use them together: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" ), AgeGreaterSpecification ( age_gt = 18 ), JoinSpecification ( Friends ), )","title":"Functional specifications"},{"location":"api_reference/core/#specificationlist","text":"SpecificationList is a static class that contains basic specifications for our repository. Specifications:","title":"SpecificationList"},{"location":"api_reference/core/#filter_1","text":"filters the data","title":"filter()"},{"location":"api_reference/core/#order","text":"orders the data","title":"order()"},{"location":"api_reference/core/#paginate","text":"paginates the data(limits the results, offsets them).","title":"paginate()"},{"location":"api_reference/core/#join","text":"joins entities together(join a table, get related data). The reason we use SpecificationList is because we want to have an abstraction for our specifications. Take two examples: from dependencies import UserRepository from assimilator.alchemy.database import alchemy_filter def get_user ( id : int , repository : UserRepository ): return repository . get ( alchemy_filter ( id = id )) # we use alchemy_filter() directly In that example, we use alchemy_filter() directly, which may not seem as an issue, however, if we would want to change our UserRepository to work with RedisRepository , then we would have to change all of our specifications ourselves. In order to fix this, we can use SpecificationList: from dependencies import UserRepository def get_user ( id : int , repository : UserRepository ): return repository . get ( repository . specifications . filter ( id = id )) # we call the filter from repository specifications. Now, we only have to change the repository without worrying about other parts of the code. Here is how you can create your own SpecificationList: from assimilator.core.database import SpecificationList , specification , Specification from pagination_func import paginate_data @specification def filter_specification ( filters , query ): return query . do_filter ( filters ) @specification def join_specification ( query ): return query # we do not need joins in our data structure, so we leave it class OrderSpecification ( Specification ): def __init__ ( self , orders ): self . orders = orders def apply ( self , query ): return query . make_order ( self . orders ) class MySpecificationList ( SpecificationList ): filter = filter_specification # we use function name as the specification order = OrderSpecification # lambda specification paginate = paginate_data # imported specification join = join_specification Notice that we never call the functions, cause the only thing we need are links to the specifications. Then, when you build your Repository: from specifications import MySpecificationList repository = MyRepository ( session = session , specifications = MySpecificationList ) Once you have done that, the repository will use your specifications. Of course, you can still use specifications directly, but if you ever need to change the repository, then it may be a problem.","title":"join()"},{"location":"api_reference/core/#lazycommand","text":"Sometimes we don't want to execute the query right away. For example, for optimization purposes or some other purpose that requires us to delay the execution. In that case, you want to find lazy argument in the function that you are calling and set it to True . After that, a LazyCommand is going to be returned. That object allows you to call it as a function or iterate over it to get the results: from assimilator.core.database import Repository def print_all_usernames ( repository : Repository ): for user in repository . filter ( lazy = True ): print ( user . username ) # we don't want to receive a list of all the users, but want to iterate # through it and only get 1 user at a time def count_users_if_argument_true ( do_count , repository : Repository ): count_command = repository . count ( lazy = True ) # turn on lazy and get LazyCommand if do_count : return count_command () # call for the result return - 1","title":"LazyCommand"},{"location":"api_reference/core/#unit-of-work","text":"Unit of work allows us to work with transactions and repositories that change the data. The problem with repository is the transaction management. We want to make our transaction management as easy as possible, but repositories are not responsible for that. That is why we have units of work. They allow us to do the following: 1. Start the transaction 2. Provide the repository to the client code 3. If there are exceptions, errors, issues with the client code, rollback the transaction and remove all the changes 4. If everything is good, the client code calls the commit() function and finishes the data change 5. Unit of work closes the transaction","title":"Unit of Work"},{"location":"api_reference/core/#__init___1","text":"repository: BaseRepository - The repository is provided in the UnitOfWork when we create it. The session to the data storage is going to be taken from the repository.","title":"__init__()"},{"location":"api_reference/core/#begin","text":"Starts the transaction. The function is called automatically.","title":"begin()"},{"location":"api_reference/core/#rollback","text":"Removes all the changes from the transaction. You do not need to call that function, as it is called automatically if there is an error in your code.","title":"rollback()"},{"location":"api_reference/core/#commit","text":"Saves the changes to the data storage. While the repository only adds the temporary, this function is responsible for the final save. You need to call it yourself, it will not be called automatically like rollback()","title":"commit()"},{"location":"api_reference/core/#close","text":"Closes the transaction. The function is called automatically.","title":"close()"},{"location":"api_reference/core/#here-is-how-you-can-use-unitofwork-in-your-code","text":"from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork uow . commit () # commit the changes making them final. If the function is not called, nothing is saved. As you can see, you do not need to call any function except for commit() . You should also use context managers( with uow: ) to start the transaction and rollback if there is an exception: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork 1 / 0 # ZeroDivisionError. UnitOfWork calls rollback automatically. uow . commit () # nothing is saved, since the rollback was called.","title":"Here is how you can use UnitOfWork in your code:"},{"location":"internal/database/","text":"Internal Database patterns Internal patterns work with Python data structures like dictionary, list, set, and others. You can use that if you don't have a database yet, or just want to test how things will work in memory. What patterns do we use? Database, in our case, is a dictionary object. 5 main patterns with databases: InternalRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. InternalUnitOfWork - works with internal transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in InternalRepository . InternalSpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results. Creating your models The first thing that you have to do is create your internal models. PyAssimilator uses Pydantic to develop models that are used as entities. We recommend using our BaseModel class to create your models: # models.py from assimilator.core.database.models import BaseModel # Just a normal Pydantic model with cool things in it class User ( BaseModel ): username : str email : str balance : float We recommend using BaseModel because it has various features such as: Automatic PyAssimilator configuration for better experience Parsing from/to JSON Automatic ID generation But, with that said, you could potentially use BaseModel from Pydantic . However, you are going to need to create an ID field: # models.py from pydantic import BaseModel # Just a normal Pydantic model class User ( BaseModel ): id : int # id can be any type, but must be unique username : str email : str balance : float Creating our patterns Once you have your models, you are going to start creating your Internal patterns. We are going to create InternalUnitOfWork and InternalRepository to manage data and transactions: # dependencies.py from assimilator.internal.database import InternalRepository , InternalUnitOfWork from models import User database = {} # our data storage def get_repository (): return InternalRepository ( session = database , # database session is a dict model = User , # our main model ) def get_uow (): return InternalUnitOfWork ( repository = get_repository ()) InternalRepository must accept a dict() for the session and the model that we created. session - Python dictionary or it's descendants. model - Pydantic or BaseModel entity that you created. You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, however, we suggest that you use different objects inside your code. Extending the session Sometimes we want to use multiple repositories with different entities in them. If that is the case, then we can use our sessions like this: session = { \"users\" : {}, # for User entity \"products\" : {}, # for Product entity } def get_repository (): return InternalRepository ( session = database [ 'users' ], # using nested dict as a session model = User , # our main model ) Using our patterns You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their Internal descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the InternalUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call InternalUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # create our InternalUnitOfWork def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) That's it. Nothing really changes from the basic tutorial, as all the things stay the same in all of our patterns. However, there are specific things that you may not use, but still need to know. You can read about them below: Internal Specifications When we sort, filter, join and mutate our Internal data, we use Internal specifications. Here is how they work, and what you can do with them. InternalFilter InternalFilter is used to filter your data using some filter. You can access it with one of the following methods(ordered by code quality): Indirect access from Repository: # InternalFilter for repositories that use InternalSpecificationList repository . specs . filter () Direct access using an import statement: from assimilator.internal.database.specifications import InternalFilter , internal_filter # Both objects are the same internal_filter () InternalFilter () Direct access from an InternalSpecificationList : from assimilator.internal.database.specifications import InternalSpecificationList InternalSpecificationList . filter () If you want to add filters, then you can use filtering options or direct internal filters: Filtering options: repository . specs . filter ( id = 1 , # id == 1 age__gt = 18 , # age > 18 username = \"Andrey\" , # username == \"Andrey\" user_domain__like = \"%.com%\" , # user_domain LIKE \"%.com%\"(yes, it works without SQL!) ) You can check out our Basic Tutorials for more filtering options. Direct filters: Sometimes you don't want to use filtering options, or you have such a complicated query, that filtering options do not allow you to fully execute it. Then, you can provide direct filters: from assimilator.internal.database.specifications import eq repository . specs . filter ( # The same as username=\"Andrey\" in filtering options eq ( 'username' , \"Andrey\" ) ) Internal direct filters are special functions that can change the query and specify what we want to get. There is a module with all of them that you can import like this: from assimilator.internal.database.specifications import internal_operator internal_operator . eq ( 'username' , \"Andrey\" ) internal_operator . gt ( 'age' , 18 ) They are written to replicate Python's operator library. However, there is another function called find_attribute() , and it is used to find any attribute and call a boolean function on it. Here is how it works: from assimilator.internal.database.specifications import find_attribute # The following function is the same as internal_operator.eq('username', \"Andrey\") find_attribute ( func = lambda a , b : a == b , # boolean function that compares field to value field = 'username' , # field that we want to get from an object value = \"Andrey\" , # value that we want to compare our object's field to ) You can use that function to create your own filters. Be sure to pass a real field with a boolean function: from assimilator.internal.database.specifications import find_attribute def only_adult_users (): return find_attribute ( func = lambda age_field , val : age_field == val , field = 'age' , value = 18 , ) repository . specs . filter ( only_adult_users (), ) But, you can also create specifications. However, when work with internal specifications, we need to make them a little differently. The thing is, that default Python structures do not support: regex, complex filters, joins, pagination and so on. So, what we can do is just run the specifications on results instead of the query itself. But, that would highly impact our algorithms, because sometimes we just need to get a value by its key. So, what do we do? We break one important programming principle\ud83d\udc80 called Single Responsibility Principle. Instead of just working with the query, we also write a specification for the list of models. Here is an example: # specifications.py from assimilator.core.database import specification @specification def filter_by_username ( username : str , query ): if username == \"Andrey\" and isinstance ( query , str ): return \"1\" for model in query : if model . username == username : return model return None # services.py # Then, you call your specification like this: repository . filter ( filter_by_username ( \"Andrey\" ) ) Here is what we did: Internal Repository runs your specifications twice - with query as a string(dict key), and with query as a list of models. We check that if query is a string(the first run), and our username is \"Andrey\", then we can return a dictionary key(id) which represents that user in the database Otherwise, we go through each model and check that it's username is \"Andrey\". Here is what we achieve with that code: If we want to find Andrey in the database - we just return a string key for a dictionary: O(1) If we can't use a simple algorithm - we go through each user: O(N) It's still a debate if we will leave that or not, but that can optimize our code a lot. Imagine that we have a big database with millions of users, and we want to find our entity by its ID. If that is the case, that would be very beneficial, but maybe there is another way of doing so. IMPORTANT: You don't always need to do something with string query in your specifications. If you are sure that the operation cannot be used in a normal dictionary indexing, then you can do the following: # specifications.py @specification def filter_by_username ( username : str , query ): # This specification only works with list query if isinstance ( query , str ): return query # Do not modify the query for model in query : if model . username == username : return model return None internal_order specification internal_order is a specification that you can use in order to sort your results. You just provide the columns in your model that are going to be use for sorting. For example: # order by username repository . specs . order ( 'username' ) # order by username and id repository . specs . order ( 'id' , 'username' ) # order by balance DESC with direct import from assimilator.internal.database.specifications import internal_order internal_order ( '-balance' ) internal_paginate specification internal_paginate is a specification that you can use to limit your results You can provide limit and offset to paginate your data. For example: # only first 10 repository . specs . paginate ( limit = 10 ) # all except for first 5 repository . specs . paginate ( offset = 5 ) # offset by 10 and get the next 10 results # with direct import from assimilator.internal.database.specifications import internal_paginate internal_paginate ( limit = 10 , offset = 10 ) internal_join specification internal_join is a specification that you can use to join multiple models together. You only use it for back compatibility with other Repositories and to show that you are joining two entities together. This specification only works as a dummy now. Here it's full code: @specification def internal_join ( * targets : Collection , query : QueryT , ** join_args : dict ) -> QueryT : return query We don't think that it is necessary to join multiple entities together, and it's really memory inefficient. Instead, we suggest that you use composition(store joined object in the model itself) to replicate foreign keys. We have some ideas on how to do real joins, but it is not implemented yet. For example: # join by relationship name with addresses table repository . specs . join ( 'addresses' ) # join multiple entities # with direct import from assimilator.internal.database.specifications import internal_join internal_join ( 'friends' , 'he_he_he_ha' ) internal_only specification internal_only is a specification that you can use to only select specific columns and optimize your queries. Examples: # Get only id and username columns repository . specs . only ( 'id' , 'username' ) # Get only balance column with direct import from assimilator.internal.database.specifications import internal_only internal_only ( 'balance' ) InternalSpecificationList If you want to create your custom SpecificationList using InternalSpecificationList as a basis, then you can import it like this: from assimilator.internal.database.specifications import InternalSpecificationList BaseModel BaseModel is a special Pydantic model provided by PyAssimilator. It has a lot of interesting settings, and that part will tell you about that. BaseModel configuration BaseModel has a special inner-class called AssimilatorConfig . That class allows you to set up various values for your model: from typing import ClassVar from assimilator.core.database import BaseModel class User ( BaseModel ): username : str class AssimilatorConfig : autogenerate_id : ClassVar [ bool ] = True # should you autogenerate your ids You can recreate that class and change the following values: autogenerate_id - whether to generate model's ID automatically. True by default. If your autogenerate_id is True , then you can still provide a custom ID to the mode: User ( username = \"Andrey\" , id = ObjectId (), # generate a new mongo ObjectId ) But, if your autogenerate_id is False , then you must provide an ID to the constructor.","title":"Database"},{"location":"internal/database/#internal-database-patterns","text":"Internal patterns work with Python data structures like dictionary, list, set, and others. You can use that if you don't have a database yet, or just want to test how things will work in memory.","title":"Internal Database patterns"},{"location":"internal/database/#what-patterns-do-we-use","text":"Database, in our case, is a dictionary object. 5 main patterns with databases: InternalRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. InternalUnitOfWork - works with internal transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in InternalRepository . InternalSpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results.","title":"What patterns do we use?"},{"location":"internal/database/#creating-your-models","text":"The first thing that you have to do is create your internal models. PyAssimilator uses Pydantic to develop models that are used as entities. We recommend using our BaseModel class to create your models: # models.py from assimilator.core.database.models import BaseModel # Just a normal Pydantic model with cool things in it class User ( BaseModel ): username : str email : str balance : float We recommend using BaseModel because it has various features such as: Automatic PyAssimilator configuration for better experience Parsing from/to JSON Automatic ID generation But, with that said, you could potentially use BaseModel from Pydantic . However, you are going to need to create an ID field: # models.py from pydantic import BaseModel # Just a normal Pydantic model class User ( BaseModel ): id : int # id can be any type, but must be unique username : str email : str balance : float","title":"Creating your models"},{"location":"internal/database/#creating-our-patterns","text":"Once you have your models, you are going to start creating your Internal patterns. We are going to create InternalUnitOfWork and InternalRepository to manage data and transactions: # dependencies.py from assimilator.internal.database import InternalRepository , InternalUnitOfWork from models import User database = {} # our data storage def get_repository (): return InternalRepository ( session = database , # database session is a dict model = User , # our main model ) def get_uow (): return InternalUnitOfWork ( repository = get_repository ()) InternalRepository must accept a dict() for the session and the model that we created. session - Python dictionary or it's descendants. model - Pydantic or BaseModel entity that you created. You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, however, we suggest that you use different objects inside your code.","title":"Creating our patterns"},{"location":"internal/database/#extending-the-session","text":"Sometimes we want to use multiple repositories with different entities in them. If that is the case, then we can use our sessions like this: session = { \"users\" : {}, # for User entity \"products\" : {}, # for Product entity } def get_repository (): return InternalRepository ( session = database [ 'users' ], # using nested dict as a session model = User , # our main model )","title":"Extending the session"},{"location":"internal/database/#using-our-patterns","text":"You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their Internal descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the InternalUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call InternalUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # create our InternalUnitOfWork def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) That's it. Nothing really changes from the basic tutorial, as all the things stay the same in all of our patterns. However, there are specific things that you may not use, but still need to know. You can read about them below:","title":"Using our patterns"},{"location":"internal/database/#internal-specifications","text":"When we sort, filter, join and mutate our Internal data, we use Internal specifications. Here is how they work, and what you can do with them.","title":"Internal Specifications"},{"location":"internal/database/#internalfilter","text":"InternalFilter is used to filter your data using some filter. You can access it with one of the following methods(ordered by code quality): Indirect access from Repository: # InternalFilter for repositories that use InternalSpecificationList repository . specs . filter () Direct access using an import statement: from assimilator.internal.database.specifications import InternalFilter , internal_filter # Both objects are the same internal_filter () InternalFilter () Direct access from an InternalSpecificationList : from assimilator.internal.database.specifications import InternalSpecificationList InternalSpecificationList . filter () If you want to add filters, then you can use filtering options or direct internal filters:","title":"InternalFilter"},{"location":"internal/database/#filtering-options","text":"repository . specs . filter ( id = 1 , # id == 1 age__gt = 18 , # age > 18 username = \"Andrey\" , # username == \"Andrey\" user_domain__like = \"%.com%\" , # user_domain LIKE \"%.com%\"(yes, it works without SQL!) ) You can check out our Basic Tutorials for more filtering options.","title":"Filtering options:"},{"location":"internal/database/#direct-filters","text":"Sometimes you don't want to use filtering options, or you have such a complicated query, that filtering options do not allow you to fully execute it. Then, you can provide direct filters: from assimilator.internal.database.specifications import eq repository . specs . filter ( # The same as username=\"Andrey\" in filtering options eq ( 'username' , \"Andrey\" ) ) Internal direct filters are special functions that can change the query and specify what we want to get. There is a module with all of them that you can import like this: from assimilator.internal.database.specifications import internal_operator internal_operator . eq ( 'username' , \"Andrey\" ) internal_operator . gt ( 'age' , 18 ) They are written to replicate Python's operator library. However, there is another function called find_attribute() , and it is used to find any attribute and call a boolean function on it. Here is how it works: from assimilator.internal.database.specifications import find_attribute # The following function is the same as internal_operator.eq('username', \"Andrey\") find_attribute ( func = lambda a , b : a == b , # boolean function that compares field to value field = 'username' , # field that we want to get from an object value = \"Andrey\" , # value that we want to compare our object's field to ) You can use that function to create your own filters. Be sure to pass a real field with a boolean function: from assimilator.internal.database.specifications import find_attribute def only_adult_users (): return find_attribute ( func = lambda age_field , val : age_field == val , field = 'age' , value = 18 , ) repository . specs . filter ( only_adult_users (), ) But, you can also create specifications. However, when work with internal specifications, we need to make them a little differently. The thing is, that default Python structures do not support: regex, complex filters, joins, pagination and so on. So, what we can do is just run the specifications on results instead of the query itself. But, that would highly impact our algorithms, because sometimes we just need to get a value by its key. So, what do we do? We break one important programming principle\ud83d\udc80 called Single Responsibility Principle. Instead of just working with the query, we also write a specification for the list of models. Here is an example: # specifications.py from assimilator.core.database import specification @specification def filter_by_username ( username : str , query ): if username == \"Andrey\" and isinstance ( query , str ): return \"1\" for model in query : if model . username == username : return model return None # services.py # Then, you call your specification like this: repository . filter ( filter_by_username ( \"Andrey\" ) ) Here is what we did: Internal Repository runs your specifications twice - with query as a string(dict key), and with query as a list of models. We check that if query is a string(the first run), and our username is \"Andrey\", then we can return a dictionary key(id) which represents that user in the database Otherwise, we go through each model and check that it's username is \"Andrey\". Here is what we achieve with that code: If we want to find Andrey in the database - we just return a string key for a dictionary: O(1) If we can't use a simple algorithm - we go through each user: O(N) It's still a debate if we will leave that or not, but that can optimize our code a lot. Imagine that we have a big database with millions of users, and we want to find our entity by its ID. If that is the case, that would be very beneficial, but maybe there is another way of doing so. IMPORTANT: You don't always need to do something with string query in your specifications. If you are sure that the operation cannot be used in a normal dictionary indexing, then you can do the following: # specifications.py @specification def filter_by_username ( username : str , query ): # This specification only works with list query if isinstance ( query , str ): return query # Do not modify the query for model in query : if model . username == username : return model return None","title":"Direct filters:"},{"location":"internal/database/#internal_order-specification","text":"internal_order is a specification that you can use in order to sort your results. You just provide the columns in your model that are going to be use for sorting. For example: # order by username repository . specs . order ( 'username' ) # order by username and id repository . specs . order ( 'id' , 'username' ) # order by balance DESC with direct import from assimilator.internal.database.specifications import internal_order internal_order ( '-balance' )","title":"internal_order specification"},{"location":"internal/database/#internal_paginate-specification","text":"internal_paginate is a specification that you can use to limit your results You can provide limit and offset to paginate your data. For example: # only first 10 repository . specs . paginate ( limit = 10 ) # all except for first 5 repository . specs . paginate ( offset = 5 ) # offset by 10 and get the next 10 results # with direct import from assimilator.internal.database.specifications import internal_paginate internal_paginate ( limit = 10 , offset = 10 )","title":"internal_paginate specification"},{"location":"internal/database/#internal_join-specification","text":"internal_join is a specification that you can use to join multiple models together. You only use it for back compatibility with other Repositories and to show that you are joining two entities together. This specification only works as a dummy now. Here it's full code: @specification def internal_join ( * targets : Collection , query : QueryT , ** join_args : dict ) -> QueryT : return query We don't think that it is necessary to join multiple entities together, and it's really memory inefficient. Instead, we suggest that you use composition(store joined object in the model itself) to replicate foreign keys. We have some ideas on how to do real joins, but it is not implemented yet. For example: # join by relationship name with addresses table repository . specs . join ( 'addresses' ) # join multiple entities # with direct import from assimilator.internal.database.specifications import internal_join internal_join ( 'friends' , 'he_he_he_ha' )","title":"internal_join specification"},{"location":"internal/database/#internal_only-specification","text":"internal_only is a specification that you can use to only select specific columns and optimize your queries. Examples: # Get only id and username columns repository . specs . only ( 'id' , 'username' ) # Get only balance column with direct import from assimilator.internal.database.specifications import internal_only internal_only ( 'balance' )","title":"internal_only specification"},{"location":"internal/database/#internalspecificationlist","text":"If you want to create your custom SpecificationList using InternalSpecificationList as a basis, then you can import it like this: from assimilator.internal.database.specifications import InternalSpecificationList","title":"InternalSpecificationList"},{"location":"internal/database/#basemodel","text":"BaseModel is a special Pydantic model provided by PyAssimilator. It has a lot of interesting settings, and that part will tell you about that.","title":"BaseModel"},{"location":"internal/database/#basemodel-configuration","text":"BaseModel has a special inner-class called AssimilatorConfig . That class allows you to set up various values for your model: from typing import ClassVar from assimilator.core.database import BaseModel class User ( BaseModel ): username : str class AssimilatorConfig : autogenerate_id : ClassVar [ bool ] = True # should you autogenerate your ids","title":"BaseModel configuration"},{"location":"internal/database/#you-can-recreate-that-class-and-change-the-following-values","text":"autogenerate_id - whether to generate model's ID automatically. True by default. If your autogenerate_id is True , then you can still provide a custom ID to the mode: User ( username = \"Andrey\" , id = ObjectId (), # generate a new mongo ObjectId ) But, if your autogenerate_id is False , then you must provide an ID to the constructor.","title":"You can recreate that class and change the following values:"},{"location":"internal/events/","text":"Internal Events - STILL IN DEVELOPMENT","title":"Internal Events - STILL IN DEVELOPMENT"},{"location":"internal/events/#internal-events-still-in-development","text":"","title":"Internal Events - STILL IN DEVELOPMENT"},{"location":"kafka/events/","text":"Kafka events - STILL IN DEVELOPMENT","title":"Kafka events - STILL IN DEVELOPMENT"},{"location":"kafka/events/#kafka-events-still-in-development","text":"","title":"Kafka events - STILL IN DEVELOPMENT"},{"location":"mongo/database/","text":"Mongo Database patterns Mongo patterns work with MongoDB NoSQL database. What patterns do we use? Database, in our case, is a connection to MongoDB. 5 main patterns with databases: MongoRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. MongoUnitOfWork - works with MongoDB transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in MongoRepository . MongoSpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results. Creating your models The first thing that you have to do is create your Mongo models. PyAssimilator uses Pydantic to develop models that are used as entities. We recommend using our MongoModel class to create your models: # models.py from assimilator.mongo.database.models import MongoModel # Just a normal Pydantic model with cool things in it class User ( MongoModel ): username : str email : str balance : float class AssimilatorConfig : collection = \"users\" # collection in MongoDB We recommend using MongoModel because it has various features such as: Automatic PyAssimilator configuration for better experience Parsing from/to JSON Automatic ObjectID generation Config for MongoDB collection But, with that said, you could potentially use BaseModel from Pydantic . However, you are going to need to create an ID field and add collection name: # models.py from bson import ObjectId from pydantic import BaseModel # Just a normal Pydantic model class User ( BaseModel ): id : ObjectId # id can be any type, but we use ObjectId from mongodb username : str email : str balance : float class AssimilatorConfig : # config for assimilator collection = \"users\" # add collection name Creating our patterns Once you have your models, you are going to start creating your Mongo patterns. We are going to create MongoUnitOfWork and MongoRepository to manage data and transactions: # dependencies.py from assimilator.mongo.database import MongoRepository , MongoUnitOfWork from pymongo import MongoClient from models import User client = MongoClient () # our mongodb connection def get_repository (): return MongoRepository ( session = client , # database session model = User , # our main model database = 'assimilator_database' , # database name ) def get_uow (): return MongoUnitOfWork ( repository = get_repository ()) MongoRepository must accept a pymongo.MongoClient for the session, the model that we created, and the name of the database that we want to work with. session - MongoClient connection/ model - Pydantic or MongoModel entity that you created. database - name of your database. You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, however, we suggest that you use different objects inside your code. Using our patterns You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their Mongo descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the MongoUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call MongoUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # Create our MongoUnitOfWork def get_user ( repository : Repository ): # Only pass the Repository because we want to read the data return repository . get ( # Use get() to query one user from the database repository . specs . filter ( # Use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) That's it. Nothing really changes from the basic tutorial, as all the things stay the same in all of our patterns. However, there are specific things that you may not use, but still need to know. You can read about them below: Mongo Specifications When we sort, filter, join and mutate our MongoDB data, we use Mongo specifications. Here is how they work, and what you can do with them. MongoFilter MongoFilter is used to filter your data using some filter. You can access it with one of the following methods(ordered by code quality): Indirect access from Repository: # MongoFilter for repositories that use MongoSpecificationList repository . specs . filter () Direct access using an import statement: from assimilator.mongo.database.specifications import MongoFilter , mongo_filter # Both objects are the same mongo_filter () MongoFilter () Direct access from an MongoSpecificationList : from assimilator.mongo.database.specifications import MongoSpecificationList MongoSpecificationList . filter () If you want to add filters, then you can use filtering options or direct Mongo filters: Filtering options: repository . specs . filter ( id = 1 , # id == 1 age__gt = 18 , # age > 18 username = \"Andrey\" , # username == \"Andrey\" user_domain__like = \"%.com%\" , # user_domain LIKE \"%.com%\"(yes, it works without SQL!) ) You can check out our Basic Tutorials for more filtering options. Direct filters: Sometimes you don't want to use filtering options, or you have such a complicated query, that filtering options do not allow you to fully execute it. Then, you can provide direct filters: repository . specs . filter ( # The same as username=\"Andrey\" in filtering options # We use MongoDB query language here { 'username' : { \"$eq\" : username }}, ) You can find all the possible queries in MongoDB documentation . You can use that to create your own filters: def only_adult_users (): return { \"age\" : { \"$gt\" : 18 }, } repository . specs . filter ( only_adult_users (), ) mongo_order specification mongo_order is a specification that you can use in order to sort your results. You just provide the columns in your model that are going to be use for sorting. For example: # order by username repository . specs . order ( 'username' ) # order by username and id repository . specs . order ( 'id' , 'username' ) # order by balance DESC with direct import from assimilator.mongo.database.specifications import mongo_order mongo_order ( '-balance' ) mongo_paginate specification mongo_paginate is a specification that you can use to limit your results You can provide limit and offset to paginate your data. For example: # only first 10 repository . specs . paginate ( limit = 10 ) # all except for first 5 repository . specs . paginate ( offset = 5 ) # offset by 10 and get the next 10 results # with direct import from assimilator.mongo.database.specifications import mongo_paginate mongo_paginate ( limit = 10 , offset = 10 ) mongo_join specification mongo_join is a specification that you can use to join multiple models together. You only use it for back compatibility with other Repositories and to show that you are joining two entities together. This specification only works as a dummy now. Here it's full code: @specification def Mongo_join ( * targets : Collection , query : QueryT , ** join_args : dict ) -> QueryT : return query We don't think that it is necessary to join multiple entities together, and it's really memory inefficient. Instead, we suggest that you use composition(store joined object in the model itself) to replicate foreign keys. We have some ideas on how to do real joins, but it is not implemented yet. For example: # join by relationship name with addresses collection repository . specs . join ( 'addresses' ) # join multiple entities # with direct import from assimilator.mongo.database.specifications import mongo_join mongo_join ( 'friends' , 'he_he_he_ha' ) mongo_only specification mongo_only is a specification that you can use to only select specific columns and optimize your queries. Examples: # Get only id and username columns repository . specs . only ( 'id' , 'username' ) # Get only balance column with direct import from assimilator.mongo.database.specifications import mongo_only mongo_only ( 'balance' ) MongoSpecificationList If you want to create your custom SpecificationList using MongoSpecificationList as a basis, then you can import it like this: from assimilator.mongo.database.specifications import MongoSpecificationList MongoModel MongoModel is a special Pydantic model provided by PyAssimilator. It has a lot of interesting settings, and that part will tell you about that. MongoModel configuration MongoModel has a special inner-class called AssimilatorConfig . That class allows you to set up various values for your model: from typing import ClassVar from assimilator.mongo.database import MongoModel class User ( MongoModel ): username : str class AssimilatorConfig : collection : ClassVar [ str ] # what is the name of the collection autogenerate_id : ClassVar [ bool ] = True # should you autogenerate your ids You can recreate that class and change the following values: collection - name of the collection for the model. autogenerate_id - whether to generate model's ID automatically. True by default. If your autogenerate_id is True , then you can still provide a custom ID to the mode: User ( username = \"Andrey\" , id = ObjectId (), # generate a new mongo ObjectId ) But, if your autogenerate_id is False , then you must provide an ID to the constructor. Special model values upsert - Whether to upsert the model. False by default.","title":"Database"},{"location":"mongo/database/#mongo-database-patterns","text":"Mongo patterns work with MongoDB NoSQL database.","title":"Mongo Database patterns"},{"location":"mongo/database/#what-patterns-do-we-use","text":"Database, in our case, is a connection to MongoDB. 5 main patterns with databases: MongoRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. MongoUnitOfWork - works with MongoDB transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in MongoRepository . MongoSpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results.","title":"What patterns do we use?"},{"location":"mongo/database/#creating-your-models","text":"The first thing that you have to do is create your Mongo models. PyAssimilator uses Pydantic to develop models that are used as entities. We recommend using our MongoModel class to create your models: # models.py from assimilator.mongo.database.models import MongoModel # Just a normal Pydantic model with cool things in it class User ( MongoModel ): username : str email : str balance : float class AssimilatorConfig : collection = \"users\" # collection in MongoDB We recommend using MongoModel because it has various features such as: Automatic PyAssimilator configuration for better experience Parsing from/to JSON Automatic ObjectID generation Config for MongoDB collection But, with that said, you could potentially use BaseModel from Pydantic . However, you are going to need to create an ID field and add collection name: # models.py from bson import ObjectId from pydantic import BaseModel # Just a normal Pydantic model class User ( BaseModel ): id : ObjectId # id can be any type, but we use ObjectId from mongodb username : str email : str balance : float class AssimilatorConfig : # config for assimilator collection = \"users\" # add collection name","title":"Creating your models"},{"location":"mongo/database/#creating-our-patterns","text":"Once you have your models, you are going to start creating your Mongo patterns. We are going to create MongoUnitOfWork and MongoRepository to manage data and transactions: # dependencies.py from assimilator.mongo.database import MongoRepository , MongoUnitOfWork from pymongo import MongoClient from models import User client = MongoClient () # our mongodb connection def get_repository (): return MongoRepository ( session = client , # database session model = User , # our main model database = 'assimilator_database' , # database name ) def get_uow (): return MongoUnitOfWork ( repository = get_repository ()) MongoRepository must accept a pymongo.MongoClient for the session, the model that we created, and the name of the database that we want to work with. session - MongoClient connection/ model - Pydantic or MongoModel entity that you created. database - name of your database. You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, however, we suggest that you use different objects inside your code.","title":"Creating our patterns"},{"location":"mongo/database/#using-our-patterns","text":"You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their Mongo descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the MongoUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call MongoUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # Create our MongoUnitOfWork def get_user ( repository : Repository ): # Only pass the Repository because we want to read the data return repository . get ( # Use get() to query one user from the database repository . specs . filter ( # Use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) That's it. Nothing really changes from the basic tutorial, as all the things stay the same in all of our patterns. However, there are specific things that you may not use, but still need to know. You can read about them below:","title":"Using our patterns"},{"location":"mongo/database/#mongo-specifications","text":"When we sort, filter, join and mutate our MongoDB data, we use Mongo specifications. Here is how they work, and what you can do with them.","title":"Mongo Specifications"},{"location":"mongo/database/#mongofilter","text":"MongoFilter is used to filter your data using some filter. You can access it with one of the following methods(ordered by code quality): Indirect access from Repository: # MongoFilter for repositories that use MongoSpecificationList repository . specs . filter () Direct access using an import statement: from assimilator.mongo.database.specifications import MongoFilter , mongo_filter # Both objects are the same mongo_filter () MongoFilter () Direct access from an MongoSpecificationList : from assimilator.mongo.database.specifications import MongoSpecificationList MongoSpecificationList . filter () If you want to add filters, then you can use filtering options or direct Mongo filters:","title":"MongoFilter"},{"location":"mongo/database/#filtering-options","text":"repository . specs . filter ( id = 1 , # id == 1 age__gt = 18 , # age > 18 username = \"Andrey\" , # username == \"Andrey\" user_domain__like = \"%.com%\" , # user_domain LIKE \"%.com%\"(yes, it works without SQL!) ) You can check out our Basic Tutorials for more filtering options.","title":"Filtering options:"},{"location":"mongo/database/#direct-filters","text":"Sometimes you don't want to use filtering options, or you have such a complicated query, that filtering options do not allow you to fully execute it. Then, you can provide direct filters: repository . specs . filter ( # The same as username=\"Andrey\" in filtering options # We use MongoDB query language here { 'username' : { \"$eq\" : username }}, ) You can find all the possible queries in MongoDB documentation . You can use that to create your own filters: def only_adult_users (): return { \"age\" : { \"$gt\" : 18 }, } repository . specs . filter ( only_adult_users (), )","title":"Direct filters:"},{"location":"mongo/database/#mongo_order-specification","text":"mongo_order is a specification that you can use in order to sort your results. You just provide the columns in your model that are going to be use for sorting. For example: # order by username repository . specs . order ( 'username' ) # order by username and id repository . specs . order ( 'id' , 'username' ) # order by balance DESC with direct import from assimilator.mongo.database.specifications import mongo_order mongo_order ( '-balance' )","title":"mongo_order specification"},{"location":"mongo/database/#mongo_paginate-specification","text":"mongo_paginate is a specification that you can use to limit your results You can provide limit and offset to paginate your data. For example: # only first 10 repository . specs . paginate ( limit = 10 ) # all except for first 5 repository . specs . paginate ( offset = 5 ) # offset by 10 and get the next 10 results # with direct import from assimilator.mongo.database.specifications import mongo_paginate mongo_paginate ( limit = 10 , offset = 10 )","title":"mongo_paginate specification"},{"location":"mongo/database/#mongo_join-specification","text":"mongo_join is a specification that you can use to join multiple models together. You only use it for back compatibility with other Repositories and to show that you are joining two entities together. This specification only works as a dummy now. Here it's full code: @specification def Mongo_join ( * targets : Collection , query : QueryT , ** join_args : dict ) -> QueryT : return query We don't think that it is necessary to join multiple entities together, and it's really memory inefficient. Instead, we suggest that you use composition(store joined object in the model itself) to replicate foreign keys. We have some ideas on how to do real joins, but it is not implemented yet. For example: # join by relationship name with addresses collection repository . specs . join ( 'addresses' ) # join multiple entities # with direct import from assimilator.mongo.database.specifications import mongo_join mongo_join ( 'friends' , 'he_he_he_ha' )","title":"mongo_join specification"},{"location":"mongo/database/#mongo_only-specification","text":"mongo_only is a specification that you can use to only select specific columns and optimize your queries. Examples: # Get only id and username columns repository . specs . only ( 'id' , 'username' ) # Get only balance column with direct import from assimilator.mongo.database.specifications import mongo_only mongo_only ( 'balance' )","title":"mongo_only specification"},{"location":"mongo/database/#mongospecificationlist","text":"If you want to create your custom SpecificationList using MongoSpecificationList as a basis, then you can import it like this: from assimilator.mongo.database.specifications import MongoSpecificationList","title":"MongoSpecificationList"},{"location":"mongo/database/#mongomodel","text":"MongoModel is a special Pydantic model provided by PyAssimilator. It has a lot of interesting settings, and that part will tell you about that.","title":"MongoModel"},{"location":"mongo/database/#mongomodel-configuration","text":"MongoModel has a special inner-class called AssimilatorConfig . That class allows you to set up various values for your model: from typing import ClassVar from assimilator.mongo.database import MongoModel class User ( MongoModel ): username : str class AssimilatorConfig : collection : ClassVar [ str ] # what is the name of the collection autogenerate_id : ClassVar [ bool ] = True # should you autogenerate your ids You can recreate that class and change the following values: collection - name of the collection for the model. autogenerate_id - whether to generate model's ID automatically. True by default. If your autogenerate_id is True , then you can still provide a custom ID to the mode: User ( username = \"Andrey\" , id = ObjectId (), # generate a new mongo ObjectId ) But, if your autogenerate_id is False , then you must provide an ID to the constructor.","title":"MongoModel configuration"},{"location":"mongo/database/#special-model-values","text":"upsert - Whether to upsert the model. False by default.","title":"Special model values"},{"location":"redis/database/","text":"Redis Database patterns Redis patterns work with Redis in-memory database. We use PyRedis library to interact with it. What patterns do we use? Database, in our case, is a dictionary object. 5 main patterns with databases: RedisRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. RedisUnitOfWork - works with Redis transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in RedisRepository . InternalSpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results. Creating your models The first thing that you have to do is create your Redis models. PyAssimilator uses Pydantic to develop models that are used as entities. We recommend using our RedisModel class to create your models: # models.py from assimilator.redis_.database.models import RedisModel # Just a normal Pydantic model with cool things in it class User ( RedisModel ): username : str email : str balance : float We recommend using RedisModel because it has various features such as: Automatic PyAssimilator configuration for better experience Parsing from/to JSON Automatic ID generation Redis-related features RedisModel inherits BaseModel that we used in Internal patterns. But, with that said, you could potentially use BaseModel from Pydantic . However, you are going to need to create an ID field and set extra=True : # models.py from pydantic import BaseModel # Just a normal Pydantic model class User ( BaseModel , extra = True ): # extra values are allowed id : int # id can be any type, but must be unique username : str email : str balance : float Creating our patterns Once you have your models, you are going to start creating your Redis patterns. We are going to create RedisUnitOfWork and RedisRepository to manage data and transactions: # dependencies.py from redis import Redis from assimilator.redis_.database import RedisRepository , RedisUnitOfWork from models import User database = Redis () # connect to redis def get_repository (): return RedisRepository ( session = database , # database session is a dict model = User , # our main model ) def get_uow (): return RedisUnitOfWork ( repository = get_repository ()) RedisRepository must accept Redis object from pyredis library and the model that we created. session - Redis connection object. model - Pydantic or RedisModel entity that you created. You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, however, we suggest that you use different objects inside your code. Using our patterns You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their redis descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the RedisUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call RedisUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # create our RedisUnitOfWork def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ()) Redis problems\ud83e\udd74 Redis is really fast, and we want to use that speed as our main advantage, however, it is really difficult to do now. We are going to change the way RedisRepository stores data once we find the best way to query it quickly. For now, we just store the keys in the database and query most of them when we need to do something with our data. This may decrease the performance, but it is a viable solution for lost of the projects. Redis Specifications Redis does not have its own specifications yet. We use Internal Specifications for now. The only thing that you should know is that it is always faster to query your data using the ID since we store our entities as redis keys. RedisModel RedisModel is a special Pydantic model provided by PyAssimilator. It has a lot of interesting settings, and that part will tell you about that. RedisModel configuration RedisModel has a special inner-class called AssimilatorConfig . That class allows you to set up various values for your model: from typing import ClassVar from assimilator.redis_.database import RedisModel class User ( RedisModel ): username : str class AssimilatorConfig : autogenerate_id : ClassVar [ bool ] = True # should you autogenerate your ids You can recreate that class and change the following values: autogenerate_id - whether to generate model's ID automatically. True by default. If your autogenerate_id is True , then you can still provide a custom ID to the model: User ( username = \"Andrey\" , id = \"custom-id\" , # generate a new redis key ) But, if your autogenerate_id is False , then you must provide an ID to the constructor. Special model values expire_in - how many seconds should pass for that entity to be deleted. None by default. expire_in_px - how many milliseconds should pass for that entity to be deleted. None by default. only_update - only update this entity(no creation). False by default. only_create - only create this entity(no update). False by default. keep_ttl - whether to keep the TTL associated with this entity. False by default.","title":"Database"},{"location":"redis/database/#redis-database-patterns","text":"Redis patterns work with Redis in-memory database. We use PyRedis library to interact with it.","title":"Redis Database patterns"},{"location":"redis/database/#what-patterns-do-we-use","text":"Database, in our case, is a dictionary object. 5 main patterns with databases: RedisRepository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. RedisUnitOfWork - works with Redis transactions. Ensures data integrity. Should only be used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in RedisRepository . InternalSpecificationList - contains links to Specification patterns for indirect coding. LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results.","title":"What patterns do we use?"},{"location":"redis/database/#creating-your-models","text":"The first thing that you have to do is create your Redis models. PyAssimilator uses Pydantic to develop models that are used as entities. We recommend using our RedisModel class to create your models: # models.py from assimilator.redis_.database.models import RedisModel # Just a normal Pydantic model with cool things in it class User ( RedisModel ): username : str email : str balance : float We recommend using RedisModel because it has various features such as: Automatic PyAssimilator configuration for better experience Parsing from/to JSON Automatic ID generation Redis-related features RedisModel inherits BaseModel that we used in Internal patterns. But, with that said, you could potentially use BaseModel from Pydantic . However, you are going to need to create an ID field and set extra=True : # models.py from pydantic import BaseModel # Just a normal Pydantic model class User ( BaseModel , extra = True ): # extra values are allowed id : int # id can be any type, but must be unique username : str email : str balance : float","title":"Creating your models"},{"location":"redis/database/#creating-our-patterns","text":"Once you have your models, you are going to start creating your Redis patterns. We are going to create RedisUnitOfWork and RedisRepository to manage data and transactions: # dependencies.py from redis import Redis from assimilator.redis_.database import RedisRepository , RedisUnitOfWork from models import User database = Redis () # connect to redis def get_repository (): return RedisRepository ( session = database , # database session is a dict model = User , # our main model ) def get_uow (): return RedisUnitOfWork ( repository = get_repository ()) RedisRepository must accept Redis object from pyredis library and the model that we created. session - Redis connection object. model - Pydantic or RedisModel entity that you created. You can also see that instead of exporting our patterns as objects, we create a function that can be called to create multiple objects whenever needed. The behaviour of creating one object or using object factories depends on your use case, however, we suggest that you use different objects inside your code.","title":"Creating our patterns"},{"location":"redis/database/#using-our-patterns","text":"You already know how to use the patterns from our Basic Tutorial. So, here is that code again: from assimilator.core.database import UnitOfWork , Repository from dependencies import get_repository , get_uow \"\"\" We use base UnitOfWork and Repository annotations in our parameters instead of their redis descendants. We suggest you do the same in order to follow SOLID principles. \"\"\" def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the RedisUnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call RedisUnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( get_uow ()) # create our RedisUnitOfWork def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( get_repository ())","title":"Using our patterns"},{"location":"redis/database/#redis-problems","text":"Redis is really fast, and we want to use that speed as our main advantage, however, it is really difficult to do now. We are going to change the way RedisRepository stores data once we find the best way to query it quickly. For now, we just store the keys in the database and query most of them when we need to do something with our data. This may decrease the performance, but it is a viable solution for lost of the projects.","title":"Redis problems\ud83e\udd74"},{"location":"redis/database/#redis-specifications","text":"Redis does not have its own specifications yet. We use Internal Specifications for now. The only thing that you should know is that it is always faster to query your data using the ID since we store our entities as redis keys.","title":"Redis Specifications"},{"location":"redis/database/#redismodel","text":"RedisModel is a special Pydantic model provided by PyAssimilator. It has a lot of interesting settings, and that part will tell you about that.","title":"RedisModel"},{"location":"redis/database/#redismodel-configuration","text":"RedisModel has a special inner-class called AssimilatorConfig . That class allows you to set up various values for your model: from typing import ClassVar from assimilator.redis_.database import RedisModel class User ( RedisModel ): username : str class AssimilatorConfig : autogenerate_id : ClassVar [ bool ] = True # should you autogenerate your ids You can recreate that class and change the following values: autogenerate_id - whether to generate model's ID automatically. True by default. If your autogenerate_id is True , then you can still provide a custom ID to the model: User ( username = \"Andrey\" , id = \"custom-id\" , # generate a new redis key ) But, if your autogenerate_id is False , then you must provide an ID to the constructor.","title":"RedisModel configuration"},{"location":"redis/database/#special-model-values","text":"expire_in - how many seconds should pass for that entity to be deleted. None by default. expire_in_px - how many milliseconds should pass for that entity to be deleted. None by default. only_update - only update this entity(no creation). False by default. only_create - only create this entity(no update). False by default. keep_ttl - whether to keep the TTL associated with this entity. False by default.","title":"Special model values"},{"location":"redis/events/","text":"Redis events - still in development","title":"Redis events - still in development"},{"location":"redis/events/#redis-events-still-in-development","text":"","title":"Redis events - still in development"},{"location":"tutorial/advanced_database/","text":"Advanced database patterns usage Unit of work When you are using UnitOfWork pattern you will typically call context managers( with statement) and commit() function. But, if you want to use all the functions of this pattern manually, you can do it like this: from assimilator.core.database import UnitOfWork def example ( uow : UnitOfWork ): uow . begin () # begin the transaction try : uow . repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () # apply the changes except Exception as exc : uow . rollback () # remove all the changes if there is an exception finally : uow . close () # close the transaction However, most of the time, there is no need in doing that, and we recommend using with uow: instead. Writing your own specifications Sometimes you are copying your specifications. That is bad, and can lead to many bugs and legacy code. Instead, you need to identify the places where you copy your specifications, and your own specification that can be used easily. Now, let's see how to do that. How do specifications work? Specification is a class or a function that does the following: it gets your initial query, changes it, and returns the new version. That is a simplified version of the source code for one of the internal specifications: from typing import Optional from assimilator.core.database import specification @specification # specification decorator def internal_paginate ( query : list , # query from the repository limit : Optional [ int ] = None , offset : Optional [ int ] = None , ) -> list : # we update the query with Python slices and return new variant return query [ offset : limit ] Each specification must return an updated version of the query, or something that can be used logically with other specifications. In this case, we get our query as a list of object, change the list, and return the updated version. So, each specification adds something to the query, and that updated query goes to the next specification. After that, our Repository pattern applies the specifications and returns our new result! SUPER IMPORTANT . You are probably wondering: when do we get this query argument and how will the users pass it to the specification? The initial_query argument that you provide in the Repository is your query, and you NEVER use query when you call specifications in your repository. The query is supplied automatically. If you want to write your own specification, then you must choose between functional specifications and class-based specifications. We would suggest to use functional specifications whenever you can! Functional specifications Functional specifications are created with @specification decorator. You must do the following to create a functional specification: Create a function with @specification pattern. Add any parameters and add query parameter as the last one. Do something with the query in the function. Return an updated query. Let's say that we want to create a specification that only allows validated objects from a list: from assimilator.core.database import specification @specification def validated_only ( query : list ): return list ( filter ( lambda obj : obj . validated , query )) Now, we can use that specification in our patterns: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( validated_only () # always call the specification. ) If we want to add some arguments, we can easily do it. But, you must be sure that you are adding your arguments before the query parameter: from assimilator.core.database import specification @specification def validated_only ( check_vip : bool , query : list ): if check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our updated specification like this: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( # We do not pass our query parameter. It is added automatically. validated_only ( is_vip = True ) ) Class-based specifications Class-based specifications are a little more advanced. You use them when you need auxiliary methods, inheritance, abstractions or anything else that classes can offer. You can create them with these steps: Create a class that inherits from Specification . Add all the arguments that you need in the __init__() function. Add apply() function from the Specification class. Change the query and return it. from assimilator.core.database import Specification class ValidatedOnly ( Specification ): # inherits from Specification abstract class def __init__ ( self , check_vip : bool ): # all the arguments go here super ( ValidatedOnly , self ) . __init__ () self . check_vip = check_vip # apply() will only get query as the argument. def apply ( self , query : list ) -> list : if self . check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our class-based specification like this: from assimilator.core.database import Repository from specifications import ValidatedOnly def filter_all_validated ( repository : Repository ): return repository . filter ( ValidatedOnly ( is_vip = True ) # Usage techniques are the same ) Using SpecificationList pattern What is the problem with our custom specifications? They are direct. We cannot use them if the query type changes, so that means that we cannot use them with different repositories. We can solve that issue with another pattern called SpecificationList . It allows you to map your specifications to classes that can call different specifications for different repositories. When you use repository.specifications or repository.specs , you are using SpecificationList objects from these repositories. The most basic SpecificationList looks like this: from typing import Type from assimilator.core.database import FilterSpecification from assimilator.core.database.specifications.types import ( OrderSpecificationProtocol , PaginateSpecificationProtocol , OnlySpecificationProtocol , JoinSpecificationProtocol , ) class SpecificationList : filter : Type [ FilterSpecification ] order : OrderSpecificationProtocol paginate : PaginateSpecificationProtocol join : JoinSpecificationProtocol only : OnlySpecificationProtocol We have all the specifications that we talked about, and each specification has a Protocol typing that allows us to specify what arguments we need for specific pre-built specifications. If you want to create your own SpecificationList , then be sure to do the following: Create a class that inherits from SpecificationList . Add all the pre-built specifications if your base class doesn't specify them. Add your custom specifications. Add your custom specification list to your repository. from sqlalchemy.orm import Query from assimilator.alchemy.database import AlchemySpecificationList from assimilator.core.database import specification @specification def alchemy_validate ( validate_vip : bool , query : Query ): if validate_vip : return query . filter ( is_vip = True , is_validated = True ) return query . filter ( is_validated = True ) class CustomSpecificationList ( AlchemySpecificationList ): # We don't need to specify pre-built specifications, # we already have them in the AlchemySpecificationList validated = alchemy_validate Then, we can use that specification list with any repository that works with alchemy: from assimilator.alchemy.database import AlchemyRepository from specifications import CustomSpecificationList repository = AlchemyRepository ( session = DatabaseSession (), model = User , specifications = CustomSpecificationList , # Change specifications list ) repository . filter ( repository . specs . validated () # use the specification indirectly ) But, how do we make it so that the specification can work with other repositories? We have to write different specifications and specification lists. There is just no other way(yet\ud83d\ude0e). So, if you want to use your specification with other patterns, rewrite it to work with their data types and create a new SpecificationList that is going to be supplied in the Repository.","title":"Advanced database tutorial"},{"location":"tutorial/advanced_database/#advanced-database-patterns-usage","text":"","title":"Advanced database patterns usage"},{"location":"tutorial/advanced_database/#unit-of-work","text":"When you are using UnitOfWork pattern you will typically call context managers( with statement) and commit() function. But, if you want to use all the functions of this pattern manually, you can do it like this: from assimilator.core.database import UnitOfWork def example ( uow : UnitOfWork ): uow . begin () # begin the transaction try : uow . repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () # apply the changes except Exception as exc : uow . rollback () # remove all the changes if there is an exception finally : uow . close () # close the transaction However, most of the time, there is no need in doing that, and we recommend using with uow: instead.","title":"Unit of work"},{"location":"tutorial/advanced_database/#writing-your-own-specifications","text":"Sometimes you are copying your specifications. That is bad, and can lead to many bugs and legacy code. Instead, you need to identify the places where you copy your specifications, and your own specification that can be used easily. Now, let's see how to do that.","title":"Writing your own specifications"},{"location":"tutorial/advanced_database/#how-do-specifications-work","text":"Specification is a class or a function that does the following: it gets your initial query, changes it, and returns the new version. That is a simplified version of the source code for one of the internal specifications: from typing import Optional from assimilator.core.database import specification @specification # specification decorator def internal_paginate ( query : list , # query from the repository limit : Optional [ int ] = None , offset : Optional [ int ] = None , ) -> list : # we update the query with Python slices and return new variant return query [ offset : limit ] Each specification must return an updated version of the query, or something that can be used logically with other specifications. In this case, we get our query as a list of object, change the list, and return the updated version. So, each specification adds something to the query, and that updated query goes to the next specification. After that, our Repository pattern applies the specifications and returns our new result! SUPER IMPORTANT . You are probably wondering: when do we get this query argument and how will the users pass it to the specification? The initial_query argument that you provide in the Repository is your query, and you NEVER use query when you call specifications in your repository. The query is supplied automatically. If you want to write your own specification, then you must choose between functional specifications and class-based specifications. We would suggest to use functional specifications whenever you can!","title":"How do specifications work?"},{"location":"tutorial/advanced_database/#functional-specifications","text":"Functional specifications are created with @specification decorator. You must do the following to create a functional specification: Create a function with @specification pattern. Add any parameters and add query parameter as the last one. Do something with the query in the function. Return an updated query. Let's say that we want to create a specification that only allows validated objects from a list: from assimilator.core.database import specification @specification def validated_only ( query : list ): return list ( filter ( lambda obj : obj . validated , query )) Now, we can use that specification in our patterns: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( validated_only () # always call the specification. ) If we want to add some arguments, we can easily do it. But, you must be sure that you are adding your arguments before the query parameter: from assimilator.core.database import specification @specification def validated_only ( check_vip : bool , query : list ): if check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our updated specification like this: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( # We do not pass our query parameter. It is added automatically. validated_only ( is_vip = True ) )","title":"Functional specifications"},{"location":"tutorial/advanced_database/#class-based-specifications","text":"Class-based specifications are a little more advanced. You use them when you need auxiliary methods, inheritance, abstractions or anything else that classes can offer. You can create them with these steps: Create a class that inherits from Specification . Add all the arguments that you need in the __init__() function. Add apply() function from the Specification class. Change the query and return it. from assimilator.core.database import Specification class ValidatedOnly ( Specification ): # inherits from Specification abstract class def __init__ ( self , check_vip : bool ): # all the arguments go here super ( ValidatedOnly , self ) . __init__ () self . check_vip = check_vip # apply() will only get query as the argument. def apply ( self , query : list ) -> list : if self . check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our class-based specification like this: from assimilator.core.database import Repository from specifications import ValidatedOnly def filter_all_validated ( repository : Repository ): return repository . filter ( ValidatedOnly ( is_vip = True ) # Usage techniques are the same )","title":"Class-based specifications"},{"location":"tutorial/advanced_database/#using-specificationlist-pattern","text":"What is the problem with our custom specifications? They are direct. We cannot use them if the query type changes, so that means that we cannot use them with different repositories. We can solve that issue with another pattern called SpecificationList . It allows you to map your specifications to classes that can call different specifications for different repositories. When you use repository.specifications or repository.specs , you are using SpecificationList objects from these repositories. The most basic SpecificationList looks like this: from typing import Type from assimilator.core.database import FilterSpecification from assimilator.core.database.specifications.types import ( OrderSpecificationProtocol , PaginateSpecificationProtocol , OnlySpecificationProtocol , JoinSpecificationProtocol , ) class SpecificationList : filter : Type [ FilterSpecification ] order : OrderSpecificationProtocol paginate : PaginateSpecificationProtocol join : JoinSpecificationProtocol only : OnlySpecificationProtocol We have all the specifications that we talked about, and each specification has a Protocol typing that allows us to specify what arguments we need for specific pre-built specifications. If you want to create your own SpecificationList , then be sure to do the following: Create a class that inherits from SpecificationList . Add all the pre-built specifications if your base class doesn't specify them. Add your custom specifications. Add your custom specification list to your repository. from sqlalchemy.orm import Query from assimilator.alchemy.database import AlchemySpecificationList from assimilator.core.database import specification @specification def alchemy_validate ( validate_vip : bool , query : Query ): if validate_vip : return query . filter ( is_vip = True , is_validated = True ) return query . filter ( is_validated = True ) class CustomSpecificationList ( AlchemySpecificationList ): # We don't need to specify pre-built specifications, # we already have them in the AlchemySpecificationList validated = alchemy_validate Then, we can use that specification list with any repository that works with alchemy: from assimilator.alchemy.database import AlchemyRepository from specifications import CustomSpecificationList repository = AlchemyRepository ( session = DatabaseSession (), model = User , specifications = CustomSpecificationList , # Change specifications list ) repository . filter ( repository . specs . validated () # use the specification indirectly ) But, how do we make it so that the specification can work with other repositories? We have to write different specifications and specification lists. There is just no other way(yet\ud83d\ude0e). So, if you want to use your specification with other patterns, rewrite it to work with their data types and create a new SpecificationList that is going to be supplied in the Repository.","title":"Using SpecificationList pattern"},{"location":"tutorial/architecture_tutorial/","text":"Architecture tutorial This tutorial will bring up some points on how to create your apps. Repository creation When we create our repositories, we need to provide our model type in the constructor. Let's say that we have a program with User, Product, Order, UserAddress, Billing entities. Do we create five repositories for each model? Do we only have one repository for User? It depends. What you want to do in general is find your primary entities. Primary entity is a model that can live by itself, and does not add any information to other models. User is a primary entity in the majority of cases, because our users can be stored by themselves. UserAddress, on the other hand, cannot live without a User that it is bound to. So, there is no need to create a Repository for UserAddress. What you want to do is a new Repository for User, and write your code in such a way that your primary entity manages auxiliary entities. If you want to know more about that, please, read Domain-Driven design books. PyAssimilator does not follow them maniacally, but they have good basis that we use in here. Repository and Unit Of Work synergy When you want to change your data you are always going to use UnitOfWork . You have to make sure that you don't create any additional repositories in your business logic code. That is, you want to use UnitOfWork as your Repository source: uow.repository . We do that because we want to remove any kind of dependency from our business logic code, and because we don't want to open multiple sessions when we don't need it. Pattern creations It's better if you create your patterns in separate files and use Dependency Injection to provide them in your business logic code. Dependency Injections allow you to remove dependencies from your business logic code, and they are very useful for pattern substitution. There are multiple ways of using them. You could look at the way Django does that with string imports or find a Dependency Injection framework that can help you with that.","title":"Architecture tutorial"},{"location":"tutorial/architecture_tutorial/#architecture-tutorial","text":"This tutorial will bring up some points on how to create your apps.","title":"Architecture tutorial"},{"location":"tutorial/architecture_tutorial/#repository-creation","text":"When we create our repositories, we need to provide our model type in the constructor. Let's say that we have a program with User, Product, Order, UserAddress, Billing entities. Do we create five repositories for each model? Do we only have one repository for User? It depends. What you want to do in general is find your primary entities. Primary entity is a model that can live by itself, and does not add any information to other models. User is a primary entity in the majority of cases, because our users can be stored by themselves. UserAddress, on the other hand, cannot live without a User that it is bound to. So, there is no need to create a Repository for UserAddress. What you want to do is a new Repository for User, and write your code in such a way that your primary entity manages auxiliary entities. If you want to know more about that, please, read Domain-Driven design books. PyAssimilator does not follow them maniacally, but they have good basis that we use in here.","title":"Repository creation"},{"location":"tutorial/architecture_tutorial/#repository-and-unit-of-work-synergy","text":"When you want to change your data you are always going to use UnitOfWork . You have to make sure that you don't create any additional repositories in your business logic code. That is, you want to use UnitOfWork as your Repository source: uow.repository . We do that because we want to remove any kind of dependency from our business logic code, and because we don't want to open multiple sessions when we don't need it.","title":"Repository and Unit Of Work synergy"},{"location":"tutorial/architecture_tutorial/#pattern-creations","text":"It's better if you create your patterns in separate files and use Dependency Injection to provide them in your business logic code. Dependency Injections allow you to remove dependencies from your business logic code, and they are very useful for pattern substitution. There are multiple ways of using them. You could look at the way Django does that with string imports or find a Dependency Injection framework that can help you with that.","title":"Pattern creations"},{"location":"tutorial/database/","text":"Database patterns What patterns do we use? Database, in our case, is any data storage. It can be PostgreSQL, MySQL, Redis, File, external API or others. We use 5 main patterns with databases: Repository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. UnitOfWork - works with transactions. Ensures data integrity. Only used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in Repository . SpecificationList - contains links to Specification patterns to completely remove imports inside of Repository . LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results. Create/Read example\ud83d\ude00 Let's say that we use SQLAlchemy library in Python. We want to make a program that can save and read our users. Each user has a username and balance. The first thing that we do is we need to create SQLAlchemy tables. There is no Assimilator in that step . # models.py from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) # create engine to the SQLite Database Base = declarative_base () # Create a base class for our tables DatabaseSession = sessionmaker ( bind = engine ) # create a database connection(session) class User ( Base ): # Create user model __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) # username column balance = Column ( Float ()) # balance column Base . metadata . create_all ( engine ) # Create User table in the database! Most of you have probably seen that. We just create a new SQLAlchemy model in our project. Now, let's add our Assimilator patterns. Two patterns that we are going to use are Repository and UnitOfWork . Repository is responsible for data management. We use it to save, read, update, delete, modify, check and basically work with our database. UnitOfWork is responsible for transactions. We use it to apply the changes made by Repository . # dependencies.py # We import our patterns from alchemy submodule from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository # We also import User and DatabaseSession we created before from models import DatabaseSession , User user_repository = AlchemyRepository ( session = DatabaseSession (), model = User , ) # We create our repository and pass session and model to it # UnitOfWork just gets the repository pattern inside it. user_uow = AlchemyUnitOfWork ( repository = user_repository ) Now, we will use those patterns to create a user. We need to do the following things: Start a new transaction using UnitOfWork . Create new user using Repository . Apply the changes using UnitOfWork . The main idea here is that even if we create millions of users, the changes will not be applied to the database until UnitOfWork.commit() function is called. We do that so that if there is an error during our operations, new changes are not applied. If you still don't get the idea of database transactions - watch this video on my channel . from assimilator.core.database import UnitOfWork from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the UnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call UnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( user_uow ) We saved the user in the database. Now, let's read it. We use Specification pattern to limit the results using different criteria. If we want to filter the results(SQL WHERE), then we must use filter() specification. We can either import the specifications from the alchemy submodule, or we can access them from the Repository.specs property. When you are sure that your function is only going to read your database, then you should only use Repository pattern without UnitOfWork. This way, we will not commit any data to our database, and can be sure that the function only reads the data, without changing it. from assimilator.core.database import Repository from dependencies import user_repository def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( user_repository ) If you want to import the specification: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # everything else is the same except for the specification username = \"Andrey\" , ) ) user = get_user ( user_repository ) As you probably know, those were direct and indirect coding styles. You can read about them here . We would suggest you to use indirect(the first) coding style. You remove a lot of imports and can do pattern substitutions which are going to be discussed in the next example. Pattern substitution example\ud83d\ude42 What is pattern substitution? It is a technique where you can change the external providers in one step. Let's see what that means... When we write code, it is a good idea to not have dependencies. They are really hard to get rid of, hard to update or change. So, if we have as little dependencies as possible, this is only going to be better. Sometimes we want to change our database to another one, or we want to change the ORM(database library) that we are using. There is a possibility that we need caching in our program, our we want to run tests of our code without using a real database. All these examples are perfect for pattern substitution. For that case, we will change SQLAlchemy patterns that we wrote in the first example to Internal patterns with one line of code. Internal patterns work with Python data structures. Your database is a dictionary, list, class or anything else within your program. Internal patterns are really useful for testing! The first thing that we need to do is change the models in our models.py file: # models.py from assimilator.core.database import BaseModel # BaseModel is just a Pydantic model with id class User ( BaseModel ): # id is supplied by default username : str balance : float So, we changed our SQLAlchemy models to a BaseModel . Then, we need to change the patterns: # dependencies.py # We import the same patterns from internal submodule from assimilator.internal.database import InternalRepository , InternalUnitOfWork from models import User # import our BaseModel User # Session is a dictionary in InternalRepository. # That means, that we will store all our data in there. session = {} # We create our repository and pass session and model to it user_repository = InternalRepository ( session = session , model = User ) # UnitOfWork just gets the repository pattern inside it. user_uow = InternalUnitOfWork ( repository = user_repository ) That's it. After you changed that, your other code like create_user() and get_user() will work with dictionary and your new User as a data storage! Now, imagine that you have 200 functions, and in order to change one data storage to another you just need 1 line of code! About direct coding All of that magic with pattern substitution was possible because we used indirect coding. But, if we use direct coding, the situation may not be that sweet: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # We use AlchemyFilter with InternalRepository. # Those do not work together\ud83d\ude2d username = \"Andrey\" , ) ) user = get_user ( user_repository ) AlchemyFilter will not work with InternalRepository . We must go into our code and change it to InternalFilter . That is why we advise you to use indirect coding whenever possible. Errors example\ud83d\ude30 We have already seen perfect examples of working code. But, errors and exceptions happen! That is why we need to ensure that the code that we write is error-proof. However, we have already done everything to do that\ud83d\ude33. As I said earlier, UnitOfWork pattern is used for transaction management. That means, that we use it to commit the data if everything is OK, or rollback the changes if there are errors. When we use context managers( with uow ) with UnitOfWork pattern we make it so that if there are errors, all the pending changes are dropped. So, if you want to add try and except to your code, then just do it like this: from assimilator.core.database import UnitOfWork , InvalidQueryError from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): try : with uow : # We use that to start the transaction repository = uow . repository new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () return new_user except InvalidQueryError : print ( \"Error in user creation\" ) return None # no user created created_user = create_user ( user_uow ) But, even if you do not use try and except, you are sure that your database does not have any weird changes in it! That's the power of UnitOfWork . If you ever want to rollback yourself, then use UnitOfWork.rollback() function. It will remove all the pending changes. That is the function that is called if there is an exception in the with uow block. Other functions you must know Here are more short examples regarding Database functions that you might want to use: Data querying from assimilator.core.database import Repository def example ( repository : Repository ): repository . get () # get one entity from the database # get() function can raise NotFoundError() or MultipleResultsError() repository . filter () # get many entities from the database # When you use those functions, you can add specifications to limit the results: adult_users = repository . filter ( repository . specs . filter ( # we use filter specification age__gte = 18 , # get all the users older than 18 years ) ) for adult_user in adult_users : print ( adult_user . username ) There are different filtering options inside of filter() specification: __eq = equal to. You can omit it and just use field=value as we did before __gt = greater than. Example: age__gt=18 == (age > 18) __gte = greater than equals. Example: age__gte=18 == (age >= 18) __lt = lower than. Example: age__lt=18 == (age < 18) __lte = lower than equals. Example: age__lte=18 == (age <= 18) __not = not equal. Example: age__not=18 == (age != 18) __is = is True or False. Example: validated__is=True == (validated is True) __like = like SQL expression. Converted to regex if not supported. Example: username__like=\"Andrey%\" == all usernames that start with Andrey __regex = regular expression. Example: username__regex=\"[1-3]+And.rey\\w+\" == regular expression, what is there to explain? You can use these options like that: from assimilator.core.database import Repository def filter_example ( repository : Repository ): # Get all users between ages 18 to 25 with username # that has \"And\" inside and those who are validated. repository . filter ( repository . specs . filter ( age__gt = 18 , age__lt = 25 , username__like = \"%And%\" , validated__is = True , ) ) All the users can be queried with Repository.filter() without any specifications: from assimilator.core.database import Repository def get_all_users ( repository : Repository ): all_users = repository . filter () # get all users from the database Pagination is added with paginate() specification: from assimilator.core.database import Repository def paginate ( repository : Repository ): paginated_users = repository . filter ( repository . specs . paginate ( limit = 10 , # limit the results by 10 offset = 20 , # offset the results by 20 ), ) Ordering is added with order() specification: from assimilator.core.database import Repository def order ( repository : Repository ): ordered_users = repository . filter ( repository . specs . order ( 'username' , # order users by username(Ascending ordering) '-balance' , # second order of the users is balance(descending order) ) ) # - in front means descending Entity joins are added with join() specification: from assimilator.core.database import Repository def join_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . join ( 'orders' , # indirect join with user User . products , # direct join with products ) ) If you want to optimize your queries, you can do so by using only() . It will accept fields that will be the only ones on your model: from assimilator.core.database import Repository def only_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . only ( 'id' , 'username' ) # We only query `id` and `username` from the database. # That reduces results size and query execution time ) If you want to count something, you can use count() : from assimilator.core.database import Repository def count_example ( repository : Repository ): users_count : int = repository . count () # Count all users other_users_count : int = repository . count ( repository . specs . filter ( id__gt = 10 ) # Count all users with id > 10 ) Sometimes you want to check if your object was updated or not. You can use is_modified() : is_modified : bool = repository . is_modified ( user ) Lazy evaluation\ud83d\ude34 Let's say that you want to load all the users from your table. But, the thing is, you don't need to use them straight away. Maybe, you want to return the result to another function, or set it as an attribute of an object. If you use the patterns that we gave you, then your code is going to be clean, but memory-heavy. To avoid that, you can prepare your function to be executed with another pattern called LazyCommand . It saves the function and all the arguments that you want to provide, and executes it only when you need it! You can add lazy=True to enable it in your Repository : from assimilator.core.database import Repository def example ( repository : Repository ): # Executes on the spot users_list = repository . filter () # Creates a lazy command that can be executed later users_filter_lazy_command = repository . filter ( lazy = True ) Now, we can optimize our program like this: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User , user_repository # We use typing for LazyCommand and show that it returns a list of Users def caller ( repository : Repository ) -> LazyCommand [ List [ User ]]: return repository . filter ( repository . specs . filter ( age__gt = 18 ), lazy = True , # make it lazy ) def second_function (): return caller ( user_repository ) # Database query not executed yet def first_function (): results = second_function () # Execute more code... for user in results : # The query is executed here ... We can optimize our code drastically. Now, we will only make the queries whenever we need them! But, if your query returns an error, that error is returned to the query execution, not creation! That is going to be first_function() in our case. Be sure to handle exceptions in the right place. Here are the places when your command is executed: from assimilator.core.database import Repository , LazyCommand def lazy_command_execution ( repository : Repository ): lazy_command : LazyCommand = repository . filter ( lazy = True ) if lazy_command : # query is executed in boolean statements print ( \"Executed!\" ) for data in lazy_command : # query is executed in iterators print ( \"Executed!\" ) lazy_user : LazyCommand = repository . get ( repository . specs . filter ( id = 1 )) print ( \"Executed for User id:\" , lazy_user . id ) # attribute access execution print ( lazy_user > 10 ) # Boolean execution Another important thing about LazyCommand is its execution policy. The thing is that if you use the same LazyCommand object many times, the command is only going to be executed once. This code only runs the query once: lazy_command_obj () # command executed lazy_command_obj () lazy_command_obj () lazy_command_obj () lazy_command_obj () # the same result in every other call Another FAR MORE IMPORTANT thing is the return type of your LazyCommand . If you call Repository.filter() , it's going to be an Iterable. If you use Repository.get() , it is just one entity. We suggest you add types with Python typings: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User def lazy_type_example ( repository : Repository ): lazy_command_many : LazyCommand [ List [ User ]] = repository . filter ( lazy = True ) lazy_command_obj : LazyCommand [ User ] = repository . get ( lazy = True ) # Now we know what is returned when the command is executed: users : List [ User ] = lazy_command_many () one_user : User = lazy_command_obj () The last thing is building your own LazyCommand objects: from assimilator.core.patterns import LazyCommand def func ( a , b , c ): return a + b + c command : LazyCommand [ int ] = LazyCommand ( command = func , # NOT func() a = 1 , b = 2 , c = 3 , # function arguments ) assert command == 1 + 2 + 3 Also, you can use the decorator to make your whole function lazy: from assimilator.core.patterns import LazyCommand @LazyCommand . decorate def decorated_lazy ( a , b , c ): return a + b + c print ( decorated_lazy ( 1 , 2 , 3 )) # 6 print ( decorated_lazy ( 1 , 2 , 3 , lazy = True )) # LazyCommand More on filter specification You have probably wondered how to do OR statement in the filter specification. What about AND statement? How are we going to implement all these things without using direct coding? You can use special operations like these: # OR operation. username==\"Andrey\" or username==\"Python\": repository . specs . filter ( username = \"Andrey\" ) | repository . specs . filter ( username = \"Python\" ) # AND operation. username==\"Andrey\" and age==22: repository . specs . filter ( username = \"Andrey\" ) & repository . specs . filter ( age = 22 ) # AND operation, but shorter: repository . specs . fitler ( username = \"Andrey\" , age = 22 ) # NOT operation. age != 55 ~ repository . specs . filter ( age = 55 ) # Combining operations together. # (username=\"Andrey\" and age=22) or (username == \"Python\" and age > 18) repository . specs . filter ( username = \"Andrey\" , age = 22 ) | \\ repository . specs . filter ( username = \"Python\" ) & \\ ! repository . specs . filter ( age__gt = 18 ) Another question that you probably have is how to make all of that shorter. Writing the specification again and again can be tiring. Good thing you can save them(not only filter specification. Any specification in general): andrey_username_spec = repository . specs . filter ( username = \"Andrey\" ) andrey = repository . filter ( andrey_username_spec ) Data changes Let's finally change some data. Repository save() function can be used with arguments or provided model: repository . save ( # indirect method. username = \"Andrey\" , balance = 1000 , ) # OR user = User ( username = \"Andrey\" , balance = 1000 ) repository . save ( user ) # direct method. Repository update() function can be used to update models: user = repository . get ( repository . specs . limit ( limit = 1 )) user . balance += 100 repository . update ( user ) # update the user It can also be used to update a lot of entities at once: repository . update ( # you provide specifications to filter the results repository . specs . filter ( age__gt = 18 ), repository . specs . limit ( limit = 100 ), # then, you provide field=new_value pairs to update the fields is_validated = False , updated_field = \"New value\" , ) Use delete() to delete one model: repository . delete ( user ) Or many models at once: repository . delete ( # delete everyone under 18 repository . specs . filter ( age__lt = 18 ) ) delete() is partially safe. That means that you cannot delete your whole database, cause if you provide nothing, you will delete nothing. But, still check your specifications in mass delete statements. Use refresh() to update the values in your old object. It goes to the database and changes your old values to new if they were updated: repository . refresh ( old_user ) assert old_user . updated_field == repository . get ( repository . specs . filter ( id = old_user . id )) . updated_field Typical flows Data Creation 1) You create Repository and UnitOfWork. from assimilator.alchemy.database import AlchemyRepository , AlchemyUnitOfWork user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) 2) You provide them in the function as parameters. from assimilator.core.database import UnitOfWork def create_user ( new_username : str , uow : UnitOfWork ): ... # UnitOfWork is a parameter 3) You use context manager(with statement in Python) with UnitOfWork. def create_user ( new_username : str , uow : UnitOfWork ): with uow : # Start the transaction in the database ... 4) You get the repository from UnitOfWork and use save() to save the result. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user 5) You use UnitOfWork commit() to apply the changes to the database. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user uow . commit () # Save changes do the database return new_user # return new user Data Filtering 1) You create Repository. from assimilator.alchemy.database import AlchemyRepository user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) 2) You provide it in the function as a parameter. from assimilator.core.database import Repository def filter_users ( age : int , repository : Repository ): ... # Repository is a parameter 3) You use Repository filter() function to filter the results. def filter_users ( age : int , repository : Repository ): return repository . filter ( ... ) 4) You use repository.specs to access the specifications. Then, you choose filter to filter the users who are 18 or older: def filter_users ( age : int , repository : Repository ): return repository . filter ( repository . specs . filter ( age__gte = 18 ) # age >= 18 ) 5) Optional step You can use direct coding style to use the specification like this: from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( AlchemyFilter ( age__gte = 18 ) # age >= 18 ) 6) Optional step You use direct coding style with SQLAlchemy filter from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( User . age >= 18 # Your SQLAlchemy User model. age >= 18 )","title":"Database tutorial"},{"location":"tutorial/database/#database-patterns","text":"","title":"Database patterns"},{"location":"tutorial/database/#what-patterns-do-we-use","text":"Database, in our case, is any data storage. It can be PostgreSQL, MySQL, Redis, File, external API or others. We use 5 main patterns with databases: Repository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. UnitOfWork - works with transactions. Ensures data integrity. Only used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in Repository . SpecificationList - contains links to Specification patterns to completely remove imports inside of Repository . LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results.","title":"What patterns do we use?"},{"location":"tutorial/database/#createread-example","text":"Let's say that we use SQLAlchemy library in Python. We want to make a program that can save and read our users. Each user has a username and balance. The first thing that we do is we need to create SQLAlchemy tables. There is no Assimilator in that step . # models.py from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) # create engine to the SQLite Database Base = declarative_base () # Create a base class for our tables DatabaseSession = sessionmaker ( bind = engine ) # create a database connection(session) class User ( Base ): # Create user model __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) # username column balance = Column ( Float ()) # balance column Base . metadata . create_all ( engine ) # Create User table in the database! Most of you have probably seen that. We just create a new SQLAlchemy model in our project. Now, let's add our Assimilator patterns. Two patterns that we are going to use are Repository and UnitOfWork . Repository is responsible for data management. We use it to save, read, update, delete, modify, check and basically work with our database. UnitOfWork is responsible for transactions. We use it to apply the changes made by Repository . # dependencies.py # We import our patterns from alchemy submodule from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository # We also import User and DatabaseSession we created before from models import DatabaseSession , User user_repository = AlchemyRepository ( session = DatabaseSession (), model = User , ) # We create our repository and pass session and model to it # UnitOfWork just gets the repository pattern inside it. user_uow = AlchemyUnitOfWork ( repository = user_repository ) Now, we will use those patterns to create a user. We need to do the following things: Start a new transaction using UnitOfWork . Create new user using Repository . Apply the changes using UnitOfWork . The main idea here is that even if we create millions of users, the changes will not be applied to the database until UnitOfWork.commit() function is called. We do that so that if there is an error during our operations, new changes are not applied. If you still don't get the idea of database transactions - watch this video on my channel . from assimilator.core.database import UnitOfWork from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the UnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call UnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( user_uow ) We saved the user in the database. Now, let's read it. We use Specification pattern to limit the results using different criteria. If we want to filter the results(SQL WHERE), then we must use filter() specification. We can either import the specifications from the alchemy submodule, or we can access them from the Repository.specs property. When you are sure that your function is only going to read your database, then you should only use Repository pattern without UnitOfWork. This way, we will not commit any data to our database, and can be sure that the function only reads the data, without changing it. from assimilator.core.database import Repository from dependencies import user_repository def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( user_repository ) If you want to import the specification: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # everything else is the same except for the specification username = \"Andrey\" , ) ) user = get_user ( user_repository ) As you probably know, those were direct and indirect coding styles. You can read about them here . We would suggest you to use indirect(the first) coding style. You remove a lot of imports and can do pattern substitutions which are going to be discussed in the next example.","title":"Create/Read example\ud83d\ude00"},{"location":"tutorial/database/#pattern-substitution-example","text":"What is pattern substitution? It is a technique where you can change the external providers in one step. Let's see what that means... When we write code, it is a good idea to not have dependencies. They are really hard to get rid of, hard to update or change. So, if we have as little dependencies as possible, this is only going to be better. Sometimes we want to change our database to another one, or we want to change the ORM(database library) that we are using. There is a possibility that we need caching in our program, our we want to run tests of our code without using a real database. All these examples are perfect for pattern substitution. For that case, we will change SQLAlchemy patterns that we wrote in the first example to Internal patterns with one line of code. Internal patterns work with Python data structures. Your database is a dictionary, list, class or anything else within your program. Internal patterns are really useful for testing! The first thing that we need to do is change the models in our models.py file: # models.py from assimilator.core.database import BaseModel # BaseModel is just a Pydantic model with id class User ( BaseModel ): # id is supplied by default username : str balance : float So, we changed our SQLAlchemy models to a BaseModel . Then, we need to change the patterns: # dependencies.py # We import the same patterns from internal submodule from assimilator.internal.database import InternalRepository , InternalUnitOfWork from models import User # import our BaseModel User # Session is a dictionary in InternalRepository. # That means, that we will store all our data in there. session = {} # We create our repository and pass session and model to it user_repository = InternalRepository ( session = session , model = User ) # UnitOfWork just gets the repository pattern inside it. user_uow = InternalUnitOfWork ( repository = user_repository ) That's it. After you changed that, your other code like create_user() and get_user() will work with dictionary and your new User as a data storage! Now, imagine that you have 200 functions, and in order to change one data storage to another you just need 1 line of code!","title":"Pattern substitution example\ud83d\ude42"},{"location":"tutorial/database/#about-direct-coding","text":"All of that magic with pattern substitution was possible because we used indirect coding. But, if we use direct coding, the situation may not be that sweet: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # We use AlchemyFilter with InternalRepository. # Those do not work together\ud83d\ude2d username = \"Andrey\" , ) ) user = get_user ( user_repository ) AlchemyFilter will not work with InternalRepository . We must go into our code and change it to InternalFilter . That is why we advise you to use indirect coding whenever possible.","title":"About direct coding"},{"location":"tutorial/database/#errors-example","text":"We have already seen perfect examples of working code. But, errors and exceptions happen! That is why we need to ensure that the code that we write is error-proof. However, we have already done everything to do that\ud83d\ude33. As I said earlier, UnitOfWork pattern is used for transaction management. That means, that we use it to commit the data if everything is OK, or rollback the changes if there are errors. When we use context managers( with uow ) with UnitOfWork pattern we make it so that if there are errors, all the pending changes are dropped. So, if you want to add try and except to your code, then just do it like this: from assimilator.core.database import UnitOfWork , InvalidQueryError from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): try : with uow : # We use that to start the transaction repository = uow . repository new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () return new_user except InvalidQueryError : print ( \"Error in user creation\" ) return None # no user created created_user = create_user ( user_uow ) But, even if you do not use try and except, you are sure that your database does not have any weird changes in it! That's the power of UnitOfWork . If you ever want to rollback yourself, then use UnitOfWork.rollback() function. It will remove all the pending changes. That is the function that is called if there is an exception in the with uow block.","title":"Errors example\ud83d\ude30"},{"location":"tutorial/database/#other-functions-you-must-know","text":"Here are more short examples regarding Database functions that you might want to use:","title":"Other functions you must know"},{"location":"tutorial/database/#data-querying","text":"from assimilator.core.database import Repository def example ( repository : Repository ): repository . get () # get one entity from the database # get() function can raise NotFoundError() or MultipleResultsError() repository . filter () # get many entities from the database # When you use those functions, you can add specifications to limit the results: adult_users = repository . filter ( repository . specs . filter ( # we use filter specification age__gte = 18 , # get all the users older than 18 years ) ) for adult_user in adult_users : print ( adult_user . username ) There are different filtering options inside of filter() specification: __eq = equal to. You can omit it and just use field=value as we did before __gt = greater than. Example: age__gt=18 == (age > 18) __gte = greater than equals. Example: age__gte=18 == (age >= 18) __lt = lower than. Example: age__lt=18 == (age < 18) __lte = lower than equals. Example: age__lte=18 == (age <= 18) __not = not equal. Example: age__not=18 == (age != 18) __is = is True or False. Example: validated__is=True == (validated is True) __like = like SQL expression. Converted to regex if not supported. Example: username__like=\"Andrey%\" == all usernames that start with Andrey __regex = regular expression. Example: username__regex=\"[1-3]+And.rey\\w+\" == regular expression, what is there to explain? You can use these options like that: from assimilator.core.database import Repository def filter_example ( repository : Repository ): # Get all users between ages 18 to 25 with username # that has \"And\" inside and those who are validated. repository . filter ( repository . specs . filter ( age__gt = 18 , age__lt = 25 , username__like = \"%And%\" , validated__is = True , ) ) All the users can be queried with Repository.filter() without any specifications: from assimilator.core.database import Repository def get_all_users ( repository : Repository ): all_users = repository . filter () # get all users from the database Pagination is added with paginate() specification: from assimilator.core.database import Repository def paginate ( repository : Repository ): paginated_users = repository . filter ( repository . specs . paginate ( limit = 10 , # limit the results by 10 offset = 20 , # offset the results by 20 ), ) Ordering is added with order() specification: from assimilator.core.database import Repository def order ( repository : Repository ): ordered_users = repository . filter ( repository . specs . order ( 'username' , # order users by username(Ascending ordering) '-balance' , # second order of the users is balance(descending order) ) ) # - in front means descending Entity joins are added with join() specification: from assimilator.core.database import Repository def join_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . join ( 'orders' , # indirect join with user User . products , # direct join with products ) ) If you want to optimize your queries, you can do so by using only() . It will accept fields that will be the only ones on your model: from assimilator.core.database import Repository def only_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . only ( 'id' , 'username' ) # We only query `id` and `username` from the database. # That reduces results size and query execution time ) If you want to count something, you can use count() : from assimilator.core.database import Repository def count_example ( repository : Repository ): users_count : int = repository . count () # Count all users other_users_count : int = repository . count ( repository . specs . filter ( id__gt = 10 ) # Count all users with id > 10 ) Sometimes you want to check if your object was updated or not. You can use is_modified() : is_modified : bool = repository . is_modified ( user )","title":"Data querying"},{"location":"tutorial/database/#lazy-evaluation","text":"Let's say that you want to load all the users from your table. But, the thing is, you don't need to use them straight away. Maybe, you want to return the result to another function, or set it as an attribute of an object. If you use the patterns that we gave you, then your code is going to be clean, but memory-heavy. To avoid that, you can prepare your function to be executed with another pattern called LazyCommand . It saves the function and all the arguments that you want to provide, and executes it only when you need it! You can add lazy=True to enable it in your Repository : from assimilator.core.database import Repository def example ( repository : Repository ): # Executes on the spot users_list = repository . filter () # Creates a lazy command that can be executed later users_filter_lazy_command = repository . filter ( lazy = True ) Now, we can optimize our program like this: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User , user_repository # We use typing for LazyCommand and show that it returns a list of Users def caller ( repository : Repository ) -> LazyCommand [ List [ User ]]: return repository . filter ( repository . specs . filter ( age__gt = 18 ), lazy = True , # make it lazy ) def second_function (): return caller ( user_repository ) # Database query not executed yet def first_function (): results = second_function () # Execute more code... for user in results : # The query is executed here ... We can optimize our code drastically. Now, we will only make the queries whenever we need them! But, if your query returns an error, that error is returned to the query execution, not creation! That is going to be first_function() in our case. Be sure to handle exceptions in the right place. Here are the places when your command is executed: from assimilator.core.database import Repository , LazyCommand def lazy_command_execution ( repository : Repository ): lazy_command : LazyCommand = repository . filter ( lazy = True ) if lazy_command : # query is executed in boolean statements print ( \"Executed!\" ) for data in lazy_command : # query is executed in iterators print ( \"Executed!\" ) lazy_user : LazyCommand = repository . get ( repository . specs . filter ( id = 1 )) print ( \"Executed for User id:\" , lazy_user . id ) # attribute access execution print ( lazy_user > 10 ) # Boolean execution Another important thing about LazyCommand is its execution policy. The thing is that if you use the same LazyCommand object many times, the command is only going to be executed once. This code only runs the query once: lazy_command_obj () # command executed lazy_command_obj () lazy_command_obj () lazy_command_obj () lazy_command_obj () # the same result in every other call Another FAR MORE IMPORTANT thing is the return type of your LazyCommand . If you call Repository.filter() , it's going to be an Iterable. If you use Repository.get() , it is just one entity. We suggest you add types with Python typings: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User def lazy_type_example ( repository : Repository ): lazy_command_many : LazyCommand [ List [ User ]] = repository . filter ( lazy = True ) lazy_command_obj : LazyCommand [ User ] = repository . get ( lazy = True ) # Now we know what is returned when the command is executed: users : List [ User ] = lazy_command_many () one_user : User = lazy_command_obj () The last thing is building your own LazyCommand objects: from assimilator.core.patterns import LazyCommand def func ( a , b , c ): return a + b + c command : LazyCommand [ int ] = LazyCommand ( command = func , # NOT func() a = 1 , b = 2 , c = 3 , # function arguments ) assert command == 1 + 2 + 3 Also, you can use the decorator to make your whole function lazy: from assimilator.core.patterns import LazyCommand @LazyCommand . decorate def decorated_lazy ( a , b , c ): return a + b + c print ( decorated_lazy ( 1 , 2 , 3 )) # 6 print ( decorated_lazy ( 1 , 2 , 3 , lazy = True )) # LazyCommand","title":"Lazy evaluation\ud83d\ude34"},{"location":"tutorial/database/#more-on-filter-specification","text":"You have probably wondered how to do OR statement in the filter specification. What about AND statement? How are we going to implement all these things without using direct coding? You can use special operations like these: # OR operation. username==\"Andrey\" or username==\"Python\": repository . specs . filter ( username = \"Andrey\" ) | repository . specs . filter ( username = \"Python\" ) # AND operation. username==\"Andrey\" and age==22: repository . specs . filter ( username = \"Andrey\" ) & repository . specs . filter ( age = 22 ) # AND operation, but shorter: repository . specs . fitler ( username = \"Andrey\" , age = 22 ) # NOT operation. age != 55 ~ repository . specs . filter ( age = 55 ) # Combining operations together. # (username=\"Andrey\" and age=22) or (username == \"Python\" and age > 18) repository . specs . filter ( username = \"Andrey\" , age = 22 ) | \\ repository . specs . filter ( username = \"Python\" ) & \\ ! repository . specs . filter ( age__gt = 18 ) Another question that you probably have is how to make all of that shorter. Writing the specification again and again can be tiring. Good thing you can save them(not only filter specification. Any specification in general): andrey_username_spec = repository . specs . filter ( username = \"Andrey\" ) andrey = repository . filter ( andrey_username_spec )","title":"More on filter specification"},{"location":"tutorial/database/#data-changes","text":"Let's finally change some data. Repository save() function can be used with arguments or provided model: repository . save ( # indirect method. username = \"Andrey\" , balance = 1000 , ) # OR user = User ( username = \"Andrey\" , balance = 1000 ) repository . save ( user ) # direct method. Repository update() function can be used to update models: user = repository . get ( repository . specs . limit ( limit = 1 )) user . balance += 100 repository . update ( user ) # update the user It can also be used to update a lot of entities at once: repository . update ( # you provide specifications to filter the results repository . specs . filter ( age__gt = 18 ), repository . specs . limit ( limit = 100 ), # then, you provide field=new_value pairs to update the fields is_validated = False , updated_field = \"New value\" , ) Use delete() to delete one model: repository . delete ( user ) Or many models at once: repository . delete ( # delete everyone under 18 repository . specs . filter ( age__lt = 18 ) ) delete() is partially safe. That means that you cannot delete your whole database, cause if you provide nothing, you will delete nothing. But, still check your specifications in mass delete statements. Use refresh() to update the values in your old object. It goes to the database and changes your old values to new if they were updated: repository . refresh ( old_user ) assert old_user . updated_field == repository . get ( repository . specs . filter ( id = old_user . id )) . updated_field","title":"Data changes"},{"location":"tutorial/database/#typical-flows","text":"","title":"Typical flows"},{"location":"tutorial/database/#data-creation","text":"1) You create Repository and UnitOfWork. from assimilator.alchemy.database import AlchemyRepository , AlchemyUnitOfWork user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) 2) You provide them in the function as parameters. from assimilator.core.database import UnitOfWork def create_user ( new_username : str , uow : UnitOfWork ): ... # UnitOfWork is a parameter 3) You use context manager(with statement in Python) with UnitOfWork. def create_user ( new_username : str , uow : UnitOfWork ): with uow : # Start the transaction in the database ... 4) You get the repository from UnitOfWork and use save() to save the result. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user 5) You use UnitOfWork commit() to apply the changes to the database. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user uow . commit () # Save changes do the database return new_user # return new user","title":"Data Creation"},{"location":"tutorial/database/#data-filtering","text":"1) You create Repository. from assimilator.alchemy.database import AlchemyRepository user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) 2) You provide it in the function as a parameter. from assimilator.core.database import Repository def filter_users ( age : int , repository : Repository ): ... # Repository is a parameter 3) You use Repository filter() function to filter the results. def filter_users ( age : int , repository : Repository ): return repository . filter ( ... ) 4) You use repository.specs to access the specifications. Then, you choose filter to filter the users who are 18 or older: def filter_users ( age : int , repository : Repository ): return repository . filter ( repository . specs . filter ( age__gte = 18 ) # age >= 18 ) 5) Optional step You can use direct coding style to use the specification like this: from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( AlchemyFilter ( age__gte = 18 ) # age >= 18 ) 6) Optional step You use direct coding style with SQLAlchemy filter from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( User . age >= 18 # Your SQLAlchemy User model. age >= 18 )","title":"Data Filtering"},{"location":"tutorial/events/","text":"Events patterns - STILL IN DEVELOPMENT Events and how they work Event shows changes in your system and listeners(consumers) respond to them. Events contain all the possible things that other parts of the system may need once they respond to them. That is useful in lots of systems, and this page will describe the basics of assimilator events. Events use Pydantic module to ease the process of creation, integration and parsing. Event based systems with Assimilator Event - representation of a change in your system that carries all the useful data EventProducer - something that produces events(executes the changes in the system and shows it with events) EventConsumer - something that waits for the producer to emit various events for it to consume them and execute various operations based on the other changes EventBus - combines both producers and consumers in one Entity that can produce and consume simultaneously Event example with user registration: User sends his registration data to our website We create a new user in the database and emit an UserCreated event using an EventProducer EventConsumers listen to our UserCreated event and executes all the operations that must be done once the user is registered Event id: int Unique identification for the event. event_name: str Name of the event. We can have different events in our system. For example, if we have an event for User creation and an event for User deletion, then we can name them: User creation: event_name = user_created User deletion: event_name = user_deleted Those names can help us sort and only listen to specific kind of events. All the names must be in the past, since an event is the change in the past. event_date: datetime Date of the event. You don't need to change this field since it is assigned by default when an event is created. from_json() from_json() is a function that is used to convert json data to an event. That method is in the JSONParsedMixin class, and it allows us to quickly convert json to a Python object. cls: Type['BaseModel'] - Any Pydantic class, but typically an Event data: str - json data for our event Create a custom event events.py : from assimilator.core.events import Event class UserCreated ( Event ): user_id : int username : str email : str # all the data that could be useful is in the event. # Since Event is a Pydantic model, we can just create new fields like this logic.py : from assimilator.core.database import UnitOfWork from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) event = UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) In that example, we only create an event without publishing it anywhere. Find out how to emit your events below. EventConsumer EventConsumer reads all the incoming events and yields them to the functions that use it. start() Starts the event consumer by connecting to all the required systems close() Closes the consumer and finishes the work consume() Yields incoming events EventConsumer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventConsumer : events_bus.py : from assimilator.core.events import EventConsumer , ExternalEvent class MyEventConsumer ( EventConsumer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def consume ( self ): while True : message = self . api . listen () # we receive a message from the API yield ExternalEvent ( ** message . convert_to_json ()) # parse it logic.py : from events_bus import MyEventConsumer def consume_events ( consumer : MyEventConsumer ): with consumer : for event in events_bus . consume (): if event . event_name == \"user_created\" : user_created_handler ( UserCreated ( ** event . data )) elif event . event_name == \"user_deleted\" : user_deleted_handler ( UserDeleted ( ** event . data )) We create a new EventConsumer called MyEventConsumer . Then, we use an api object to implement all the functions. After that, we use it in logic.py file where we consume all the events and handle them depending on the event_name . As you have already noticed, we use something called an ExternalEvent . We do that because all the events that are coming from external sources are unidentified and can only use specific later. ExternalEvent contains all the event data in the data: dict field which can be used later. ExternalEvent When we listen to external systems, it is sometimes hard to make an event class that represents a specific class. That is why we use an ExternalEvent . It contains all the data in the data: dict field, which can be accessed later in order to use an event class that represents that specific event. data: dict - all the data for the event AckEvent AckEvent is an event that has acknowledgement in it. If you want to show that your event was processed(acknowledged), then use AckEvent . ack: bool - whether an event was processed. False by default EventProducer EventProducer is the class that produces all the events and sends them. start() Starts the event producer by connecting to all the required systems. close() Closes the producer and finishes the work. produce() Sends an event to an external system for it to be consumed. event: Event - the event that must be sent. EventProducer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventProducer : events_bus.py : from assimilator.core.events import EventProducer class MyEventProducer ( EventProducer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def produce ( self , event : Event ) -> None : self . api . send_event ( event . json ()) # parse event to json and send it logic.py : from events_bus import MyEventProducer from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork , producer : MyEventProducer , ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) with producer : producer . produce ( UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) ) # send an event to an external system ExternalEvent must not be used in the producer, since when we emit the events we are the ones creating them, so we have a separate class for them with all the data inside. EventBus EventBus combines both EventProducer and EventConsumer together. You can use those classes separately, but sometimes you need one object that combines them. __init__() consumer: EventConsumer - the consumer that we want to use producer: EventProducer - the producer that we want to use produce() produces the event using producer event: Event - an event that has to be emitted consume() consumes the events using consumer . Returns an Iterator[Event] Event fails and transaction management Sometimes we want to be sure that our events are emitted. But, if we use normal event producers and Unit Of Work separately, we may run into a problem: 1) User is created(added in the database and unit of work committed it) 2) Event producer encounters an error(the event is not published) 3) Inconsistency: User exists, but consumers do not know about that Because of that, we may employ Outbox Relay. It is a pattern that allows us to save all the events in the database in the same transaction as the main entity. Then, another program(thread, task, function) gets all the events from the database and ensures that they are published. We basically save the events to the database in one transaction, emit them in a separate thing and delete them afterwards. OutboxRelay This class gets all the events using UnitOfWork provided, emits all events, and acknowledges them. __init__() uow: UnitOfWork - unit of work that is used in order to get the events, acknowledge them producer: EventProducer - event producer that we use to publish the events start() Start the relay. This function must run forever, must get the events from the repository from unit of work, and produce the events. After that, it must call acknowledge() to show that these events are produced. acknowledge() Acknowledges the events in the database. It might change a boolean column for these events, might delete them, but the idea is that those events will not be produced twice. events: Iterable[Event] - events that must be acknowledged","title":"Events patterns - STILL IN DEVELOPMENT"},{"location":"tutorial/events/#events-patterns-still-in-development","text":"","title":"Events patterns - STILL IN DEVELOPMENT"},{"location":"tutorial/events/#events-and-how-they-work","text":"Event shows changes in your system and listeners(consumers) respond to them. Events contain all the possible things that other parts of the system may need once they respond to them. That is useful in lots of systems, and this page will describe the basics of assimilator events. Events use Pydantic module to ease the process of creation, integration and parsing.","title":"Events and how they work"},{"location":"tutorial/events/#event-based-systems-with-assimilator","text":"Event - representation of a change in your system that carries all the useful data EventProducer - something that produces events(executes the changes in the system and shows it with events) EventConsumer - something that waits for the producer to emit various events for it to consume them and execute various operations based on the other changes EventBus - combines both producers and consumers in one Entity that can produce and consume simultaneously","title":"Event based systems with Assimilator"},{"location":"tutorial/events/#event-example-with-user-registration","text":"User sends his registration data to our website We create a new user in the database and emit an UserCreated event using an EventProducer EventConsumers listen to our UserCreated event and executes all the operations that must be done once the user is registered","title":"Event example with user registration:"},{"location":"tutorial/events/#event","text":"","title":"Event"},{"location":"tutorial/events/#id-int","text":"Unique identification for the event.","title":"id: int"},{"location":"tutorial/events/#event_name-str","text":"Name of the event. We can have different events in our system. For example, if we have an event for User creation and an event for User deletion, then we can name them: User creation: event_name = user_created User deletion: event_name = user_deleted Those names can help us sort and only listen to specific kind of events. All the names must be in the past, since an event is the change in the past.","title":"event_name: str"},{"location":"tutorial/events/#event_date-datetime","text":"Date of the event. You don't need to change this field since it is assigned by default when an event is created.","title":"event_date: datetime"},{"location":"tutorial/events/#from_json","text":"from_json() is a function that is used to convert json data to an event. That method is in the JSONParsedMixin class, and it allows us to quickly convert json to a Python object. cls: Type['BaseModel'] - Any Pydantic class, but typically an Event data: str - json data for our event","title":"from_json()"},{"location":"tutorial/events/#create-a-custom-event","text":"events.py : from assimilator.core.events import Event class UserCreated ( Event ): user_id : int username : str email : str # all the data that could be useful is in the event. # Since Event is a Pydantic model, we can just create new fields like this logic.py : from assimilator.core.database import UnitOfWork from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) event = UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) In that example, we only create an event without publishing it anywhere. Find out how to emit your events below.","title":"Create a custom event"},{"location":"tutorial/events/#eventconsumer","text":"EventConsumer reads all the incoming events and yields them to the functions that use it.","title":"EventConsumer"},{"location":"tutorial/events/#start","text":"Starts the event consumer by connecting to all the required systems","title":"start()"},{"location":"tutorial/events/#close","text":"Closes the consumer and finishes the work","title":"close()"},{"location":"tutorial/events/#consume","text":"Yields incoming events EventConsumer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventConsumer : events_bus.py : from assimilator.core.events import EventConsumer , ExternalEvent class MyEventConsumer ( EventConsumer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def consume ( self ): while True : message = self . api . listen () # we receive a message from the API yield ExternalEvent ( ** message . convert_to_json ()) # parse it logic.py : from events_bus import MyEventConsumer def consume_events ( consumer : MyEventConsumer ): with consumer : for event in events_bus . consume (): if event . event_name == \"user_created\" : user_created_handler ( UserCreated ( ** event . data )) elif event . event_name == \"user_deleted\" : user_deleted_handler ( UserDeleted ( ** event . data )) We create a new EventConsumer called MyEventConsumer . Then, we use an api object to implement all the functions. After that, we use it in logic.py file where we consume all the events and handle them depending on the event_name . As you have already noticed, we use something called an ExternalEvent . We do that because all the events that are coming from external sources are unidentified and can only use specific later. ExternalEvent contains all the event data in the data: dict field which can be used later.","title":"consume()"},{"location":"tutorial/events/#externalevent","text":"When we listen to external systems, it is sometimes hard to make an event class that represents a specific class. That is why we use an ExternalEvent . It contains all the data in the data: dict field, which can be accessed later in order to use an event class that represents that specific event. data: dict - all the data for the event","title":"ExternalEvent"},{"location":"tutorial/events/#ackevent","text":"AckEvent is an event that has acknowledgement in it. If you want to show that your event was processed(acknowledged), then use AckEvent . ack: bool - whether an event was processed. False by default","title":"AckEvent"},{"location":"tutorial/events/#eventproducer","text":"EventProducer is the class that produces all the events and sends them.","title":"EventProducer"},{"location":"tutorial/events/#start_1","text":"Starts the event producer by connecting to all the required systems.","title":"start()"},{"location":"tutorial/events/#close_1","text":"Closes the producer and finishes the work.","title":"close()"},{"location":"tutorial/events/#produce","text":"Sends an event to an external system for it to be consumed. event: Event - the event that must be sent. EventProducer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventProducer : events_bus.py : from assimilator.core.events import EventProducer class MyEventProducer ( EventProducer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def produce ( self , event : Event ) -> None : self . api . send_event ( event . json ()) # parse event to json and send it logic.py : from events_bus import MyEventProducer from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork , producer : MyEventProducer , ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) with producer : producer . produce ( UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) ) # send an event to an external system ExternalEvent must not be used in the producer, since when we emit the events we are the ones creating them, so we have a separate class for them with all the data inside.","title":"produce()"},{"location":"tutorial/events/#eventbus","text":"EventBus combines both EventProducer and EventConsumer together. You can use those classes separately, but sometimes you need one object that combines them.","title":"EventBus"},{"location":"tutorial/events/#__init__","text":"consumer: EventConsumer - the consumer that we want to use producer: EventProducer - the producer that we want to use","title":"__init__()"},{"location":"tutorial/events/#produce_1","text":"produces the event using producer event: Event - an event that has to be emitted","title":"produce()"},{"location":"tutorial/events/#consume_1","text":"consumes the events using consumer . Returns an Iterator[Event]","title":"consume()"},{"location":"tutorial/events/#event-fails-and-transaction-management","text":"Sometimes we want to be sure that our events are emitted. But, if we use normal event producers and Unit Of Work separately, we may run into a problem: 1) User is created(added in the database and unit of work committed it) 2) Event producer encounters an error(the event is not published) 3) Inconsistency: User exists, but consumers do not know about that Because of that, we may employ Outbox Relay. It is a pattern that allows us to save all the events in the database in the same transaction as the main entity. Then, another program(thread, task, function) gets all the events from the database and ensures that they are published. We basically save the events to the database in one transaction, emit them in a separate thing and delete them afterwards.","title":"Event fails and transaction management"},{"location":"tutorial/events/#outboxrelay","text":"This class gets all the events using UnitOfWork provided, emits all events, and acknowledges them.","title":"OutboxRelay"},{"location":"tutorial/events/#__init___1","text":"uow: UnitOfWork - unit of work that is used in order to get the events, acknowledge them producer: EventProducer - event producer that we use to publish the events","title":"__init__()"},{"location":"tutorial/events/#start_2","text":"Start the relay. This function must run forever, must get the events from the repository from unit of work, and produce the events. After that, it must call acknowledge() to show that these events are produced.","title":"start()"},{"location":"tutorial/events/#acknowledge","text":"Acknowledges the events in the database. It might change a boolean column for these events, might delete them, but the idea is that those events will not be produced twice. events: Iterable[Event] - events that must be acknowledged","title":"acknowledge()"},{"location":"tutorial/important/","text":"Important things related to all patterns What is a pattern??? Pattern - typical solution to commonly occurring problems. In our case, the problems are: Database communication Data integrity Event-based systems Coding speed Good code External dependencies The best code that some people cannot write easily We solve all of them with different classes like: Repository , UnitOfWork , Producer , and so on. Each class is a pattern. Lots of them are used with each other: Repository is always used with UnitOfWork and Specification . Indirect vs Direct code When you write your code, you can choose two styles: direct and indirect. What does that mean? We use different libraries like SQLAlchemy, PyRedis, PyMongo and others to ease the use of our patterns. We did not want to create a module that allows you to completely remove these modules from your code. But, we made it so our patterns are interchangeable. That means that you can write some code for SQLAlchemy, and change it to Redis 2 minutes later, even if you coded 20 000 lines. Indirect coding style You do not import any functions from assimilator, every useful thing is directly in the pattern. You do not use anything from external providers(except for pattern creation) in your code. You only use our patterns. Indirect coding example: def create_user ( uow : UnitOfWork ): with uow : uow . repository . save ( username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( balance__gt = 20 )) # only using arguments # Patterns Configuration # External library(SQLAlchemy) is only found in the pattern creation repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository ) Direct coding style You import functions and objects from assimilator. You use things from external libraries in your code with assimilator patterns Direct coding example: def create_user ( uow : UnitOfWork ): with uow : new_user = User ( # SQLAlchemy model is used directly username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) uow . repository . save ( new_user ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( User . balance > 20 ), # SQLAlchemy filter user # AlchemyFilter(User.balance > 20), # AlchemyFilter is imported from assimilator, direct use ) # repository.specs.filter == AlchemyFilter for AlchemyRepository, but you either use it directly or indirectly # Patterns Configuration. Everything is the same repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository ) Why do you need all that? Indirect style pluses \u2714\ufe0f You won't have any external dependencies. For example, you don't want to use SQLAlchemy directly. You can change data storages by only changing the configuration: def create_user ( uow : UnitOfWork ): \"\"\" Stays the same using indirect coding \"\"\" def filter_users ( repository : Repository ): \"\"\" Stays the same using indirect coding \"\"\" # Patterns Configuration # You can change pattern creation and move to another data storage without any issues. repository = RedisRepository ( Redis (), model = RedisUser ) ####### LOOK HERE uow = RedisUnitOfWork ( repository ) Indirect minuses \u274c Indirect coding is a little slower than the direct one. It may not include all the features that your app needs. For example, what if you need to run a MongoDB pipeline with aggregation framework\ud83d\ude35(even though you can do this specific thing with indirect coding). Direct style pluses \u2714\ufe0f Your app is very complex, and you don't have all the features in indirect variant. You are 100% sure that you will not change your code to other external libraries with Assimilator patterns. A little faster since we do not parse anything, we just use external objects and methods. Direct minuses \u274c Very hard to move to other data storages or libraries since you are using external features directly. External dependencies in your code. How to choose? We prefer to use indirect style, since it hides dependencies. But, what you need to do is adapt to your project. Start with indirect style and use direct features only when needed.","title":"Important things"},{"location":"tutorial/important/#important-things-related-to-all-patterns","text":"","title":"Important things related to all patterns"},{"location":"tutorial/important/#what-is-a-pattern","text":"Pattern - typical solution to commonly occurring problems. In our case, the problems are: Database communication Data integrity Event-based systems Coding speed Good code External dependencies The best code that some people cannot write easily We solve all of them with different classes like: Repository , UnitOfWork , Producer , and so on. Each class is a pattern. Lots of them are used with each other: Repository is always used with UnitOfWork and Specification .","title":"What is a pattern???"},{"location":"tutorial/important/#indirect-vs-direct-code","text":"When you write your code, you can choose two styles: direct and indirect. What does that mean? We use different libraries like SQLAlchemy, PyRedis, PyMongo and others to ease the use of our patterns. We did not want to create a module that allows you to completely remove these modules from your code. But, we made it so our patterns are interchangeable. That means that you can write some code for SQLAlchemy, and change it to Redis 2 minutes later, even if you coded 20 000 lines.","title":"Indirect vs Direct code"},{"location":"tutorial/important/#indirect-coding-style","text":"You do not import any functions from assimilator, every useful thing is directly in the pattern. You do not use anything from external providers(except for pattern creation) in your code. You only use our patterns. Indirect coding example: def create_user ( uow : UnitOfWork ): with uow : uow . repository . save ( username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( balance__gt = 20 )) # only using arguments # Patterns Configuration # External library(SQLAlchemy) is only found in the pattern creation repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository )","title":"Indirect coding style"},{"location":"tutorial/important/#direct-coding-style","text":"You import functions and objects from assimilator. You use things from external libraries in your code with assimilator patterns Direct coding example: def create_user ( uow : UnitOfWork ): with uow : new_user = User ( # SQLAlchemy model is used directly username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) uow . repository . save ( new_user ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( User . balance > 20 ), # SQLAlchemy filter user # AlchemyFilter(User.balance > 20), # AlchemyFilter is imported from assimilator, direct use ) # repository.specs.filter == AlchemyFilter for AlchemyRepository, but you either use it directly or indirectly # Patterns Configuration. Everything is the same repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository )","title":"Direct coding style"},{"location":"tutorial/important/#why-do-you-need-all-that","text":"","title":"Why do you need all that?"},{"location":"tutorial/important/#indirect-style-pluses","text":"You won't have any external dependencies. For example, you don't want to use SQLAlchemy directly. You can change data storages by only changing the configuration: def create_user ( uow : UnitOfWork ): \"\"\" Stays the same using indirect coding \"\"\" def filter_users ( repository : Repository ): \"\"\" Stays the same using indirect coding \"\"\" # Patterns Configuration # You can change pattern creation and move to another data storage without any issues. repository = RedisRepository ( Redis (), model = RedisUser ) ####### LOOK HERE uow = RedisUnitOfWork ( repository )","title":"Indirect style pluses \u2714\ufe0f"},{"location":"tutorial/important/#indirect-minuses","text":"Indirect coding is a little slower than the direct one. It may not include all the features that your app needs. For example, what if you need to run a MongoDB pipeline with aggregation framework\ud83d\ude35(even though you can do this specific thing with indirect coding).","title":"Indirect minuses \u274c"},{"location":"tutorial/important/#direct-style-pluses","text":"Your app is very complex, and you don't have all the features in indirect variant. You are 100% sure that you will not change your code to other external libraries with Assimilator patterns. A little faster since we do not parse anything, we just use external objects and methods.","title":"Direct style pluses \u2714\ufe0f"},{"location":"tutorial/important/#direct-minuses","text":"Very hard to move to other data storages or libraries since you are using external features directly. External dependencies in your code.","title":"Direct minuses \u274c"},{"location":"tutorial/important/#how-to-choose","text":"We prefer to use indirect style, since it hides dependencies. But, what you need to do is adapt to your project. Start with indirect style and use direct features only when needed.","title":"How to choose?"}]}