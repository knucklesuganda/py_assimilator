{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Assimilator - the best Python patterns for the best projects Install now pip install py_assimilator Simple example Example usage of the code to create a user using all the DDD patterns: from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository from assimilator.core.database import UnitOfWork from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) Base = declarative_base () DatabaseSession = sessionmaker ( bind = engine ) class User ( Base ): # Create user model __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) email = Column ( String ()) balance = Column ( Float ()) def __str__ ( self ): return f \" { self . id } { self . username } { self . email } \" Base . metadata . create_all ( engine ) def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : repository = uow . repository # get Repository pattern from unit of work new_user = repository . save ( username = username , email = email , balance = 0 ) # create the user uow . commit () # Commit session with Unit of work pattern return new_user user_repository = AlchemyRepository ( session = DatabaseSession (), model = User ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) create_user ( username = \"Andrey\" , email = \"python.on.papyrus@gmail.com\" , uow = user_uow , ) Hard? Don't worry, you will only have to memorize it once. After that, you just repeat the code! Source Github PyPI Documentation Github Author's YouTube RU Author's YouTube ENG About patterns in coding They are useful, but only to some extent. Most of them are not suitable for real life applications. DDD(Domain-driven design) is one of the most popular ways of development today, but nobody explains how to write most of DDD patterns in Python. Even if they do, life gives you another issue that cannot be solved with a simple algorithm. That is why Andrey created a library for the patterns that he uses in his projects daily. Types of patterns These are different use cases for the patterns implemented. Database - patterns for database/data layer interactions Events - projects with events or event-driven architecture Available providers Providers are different patterns for external modules like SQLAlchemy or FastAPI. Alchemy(Database, Events) - patterns for SQLAlchemy for both database and events. Kafka(Events) - patterns in Kafka related to events. Internal(Database, Events) - internal is the type of provider that saves everything in memory(dict, list and all the tools within your app). Redis(Database, Events) - redis_ allows us to work with Redis memory database. MongoDB(Database) - mongo allows us to work with MongoDB database.","title":"Introduction"},{"location":"#assimilator-the-best-python-patterns-for-the-best-projects","text":"","title":"Assimilator - the best Python patterns for the best projects"},{"location":"#install-now","text":"pip install py_assimilator","title":"Install now"},{"location":"#simple-example","text":"Example usage of the code to create a user using all the DDD patterns: from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository from assimilator.core.database import UnitOfWork from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) Base = declarative_base () DatabaseSession = sessionmaker ( bind = engine ) class User ( Base ): # Create user model __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) email = Column ( String ()) balance = Column ( Float ()) def __str__ ( self ): return f \" { self . id } { self . username } { self . email } \" Base . metadata . create_all ( engine ) def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : repository = uow . repository # get Repository pattern from unit of work new_user = repository . save ( username = username , email = email , balance = 0 ) # create the user uow . commit () # Commit session with Unit of work pattern return new_user user_repository = AlchemyRepository ( session = DatabaseSession (), model = User ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) create_user ( username = \"Andrey\" , email = \"python.on.papyrus@gmail.com\" , uow = user_uow , ) Hard? Don't worry, you will only have to memorize it once. After that, you just repeat the code!","title":"Simple example"},{"location":"#source","text":"Github PyPI Documentation Github Author's YouTube RU Author's YouTube ENG","title":"Source"},{"location":"#about-patterns-in-coding","text":"They are useful, but only to some extent. Most of them are not suitable for real life applications. DDD(Domain-driven design) is one of the most popular ways of development today, but nobody explains how to write most of DDD patterns in Python. Even if they do, life gives you another issue that cannot be solved with a simple algorithm. That is why Andrey created a library for the patterns that he uses in his projects daily.","title":"About patterns in coding"},{"location":"#types-of-patterns","text":"These are different use cases for the patterns implemented. Database - patterns for database/data layer interactions Events - projects with events or event-driven architecture","title":"Types of patterns"},{"location":"#available-providers","text":"Providers are different patterns for external modules like SQLAlchemy or FastAPI. Alchemy(Database, Events) - patterns for SQLAlchemy for both database and events. Kafka(Events) - patterns in Kafka related to events. Internal(Database, Events) - internal is the type of provider that saves everything in memory(dict, list and all the tools within your app). Redis(Database, Events) - redis_ allows us to work with Redis memory database. MongoDB(Database) - mongo allows us to work with MongoDB database.","title":"Available providers"},{"location":"concepts/","text":"How do we build these patterns 1. Dependency injection Dependency injection is a really important concept in assimilator. We do not use any additional dependency injection frameworks, but all the patterns inject different components into themselves. If you want to know more about DI 2. SOLID SOLID principles are highly used in assimilator. That means, that in theory you can replace one pattern to another and experience no trouble in using them. That is why it is not advised to create your own function in patterns, but you can easily override them. For example, you don't want to create: createUsers() in Repository pattern, but can override save() function without any problems . With that said, it is almost impossible to write such vast variety of patterns without breaking some principles. But, if you have any ideas on how to fix that, then be sure to check out our GitHub 3. Domain-driven design Most of the patterns here are used in Domain driven design. You do not really need to know all the intricacies, but make sure that you know the basics of it. 4. Reusable patterns The best thing about our patterns is that you can write your code for SQLAlchemy, then change it to Redis, then change it to Kafka, and finally test it with Python dictionaries. The thing is, you only have to change your pattern creation code, everything else stays the same. All the functions work the same in all the patterns that we create.","title":"Concepts"},{"location":"concepts/#how-do-we-build-these-patterns","text":"","title":"How do we build these patterns"},{"location":"concepts/#1-dependency-injection","text":"Dependency injection is a really important concept in assimilator. We do not use any additional dependency injection frameworks, but all the patterns inject different components into themselves. If you want to know more about DI","title":"1. Dependency injection"},{"location":"concepts/#2-solid","text":"SOLID principles are highly used in assimilator. That means, that in theory you can replace one pattern to another and experience no trouble in using them. That is why it is not advised to create your own function in patterns, but you can easily override them. For example, you don't want to create: createUsers() in Repository pattern, but can override save() function without any problems . With that said, it is almost impossible to write such vast variety of patterns without breaking some principles. But, if you have any ideas on how to fix that, then be sure to check out our GitHub","title":"2. SOLID"},{"location":"concepts/#3-domain-driven-design","text":"Most of the patterns here are used in Domain driven design. You do not really need to know all the intricacies, but make sure that you know the basics of it.","title":"3. Domain-driven design"},{"location":"concepts/#4-reusable-patterns","text":"The best thing about our patterns is that you can write your code for SQLAlchemy, then change it to Redis, then change it to Kafka, and finally test it with Python dictionaries. The thing is, you only have to change your pattern creation code, everything else stays the same. All the functions work the same in all the patterns that we create.","title":"4. Reusable patterns"},{"location":"alchemy/database/","text":"SQLAlchemy Database patterns a","title":"Database"},{"location":"alchemy/database/#sqlalchemy-database-patterns","text":"","title":"SQLAlchemy Database patterns"},{"location":"alchemy/database/#a","text":"","title":"a"},{"location":"api_reference/core/","text":"Repository __init__() session: SessionT - each repository has a session that works as the primary data source. It can be your database connection, a text file or a data structure. initial_query: Optional[SessionT] = None - the initial query that you use in the data storage. We will show how it works later. It can be an SQL query, a key in the dictionary or anything else. specifications: SpecificationList - an object that contains links to the specifications that are used to create your queries model: Type[ModelT] - class of the model that we want to work with. It can be an SQLAlchemy model, Pydantic model, or your custom class. specifications: Type[SpecificationList] - SpecificationList class with all the specifications that you need error_wrapper: Optional[ErrorWrapper] = ErrorWrapper() - ErrorWrapper class that allows you to convert external providers errors to your custom errors. _check_obj_is_specification() -> Tuple[Optional[ModelT], Iterable[SpecificationType]] This function is called for parts of the code that use both obj and *specifications. We check that if the obj is a model. If that is the case, we swap it around. obj: ModelT specifications: Iterable[SpecificationType] specs -> Type[SpecificationList] Property for a shorter name version of specifications attribute. _get_initial_query() -> QueryT Returns initial query for the data querying. Can return override_query as the initial query if it is present. override_query: Optional[QueryT] = None - overrides the initial query. Used for more flexibility in querying. _apply_specifications() -> QueryT Applies Specifications to the query. Must not be used directly. apply specifications gets a list of specifications and applies them to the query returned in _get_initial_query(). The idea is the following: each specification gets a query and adds some filters to it. At the end we get a fully working query modified with the specifications provided by the user. specifications: Iterable[Specifications] - an iterable of specifications that can be used to specify some conditions in the query Specification is a pattern that adds filters or anything else that specifies what kind of data we want. get() get is the function used to query the data storage and return one entity. You supply a list of specifications that get you the entity from the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument filter() filters is the function used to query the data storage and return many entities. You supply a list of specifications that filter entities in the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument save() Adds the objects to the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just add them ). delete() Deletes the objects from the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just delete them from your session ). update() Updates the objects in the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just update them in your session ). is_modified() Checks whether an obj was modified or not. If any value changes within the object, then it must return True refresh() Updates the object values with the values in the data storage. That can be useful if you want to create an object and get its id that was generated in the storage, or if you just want to have the latest saved version of the object. count() Counts the objects while applying specifications to the query. Give no specifications to count the whole data storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future Creating your own repository: If you want to create your own repository, then you are going to have to override all the functions above. But, please, do not make new functions available to the outer world. You can do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def _users_private_func ( self ): # Cannot be called outside return 'Do something' And call that function inside of your repository. But, never do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def get_ser_by_id ( self ): # Cannot be called outside return self . get ( filter_specification ( id = 1 )) Since it is going to be really hard for you to replace one repository to another. Example: from assimilator.core.database import Repository from users.repository import UserRepository from products.repository import ProductRepository def get_by_id ( id , repository : Repository ): return repository . get ( filter_specification ( id = 1 )) get_by_id ( UserRepository ()) get_by_id ( ProductRepository ()) # You can call the function with both repositories, and it will probably work fine How to create my repositories? You want to create a repository for entities in your projects. That means, that if you have some auxiliary table in your app, then it probably should not have a repository. But, things like Users, Products, Orders and others might have their own repository. That does not mean that you will create a lot of classes, but please do not add repositories for every class in your system . If you want to read more, please, look into the Domain-Driven Development books. Specification Specification is the pattern that adds values, filters, joins or anything else to the query in your repository. It can also work as a filter for your objects. Class-based specifications class Specification ( ABC ): @abstractmethod def apply ( self , query ): raise NotImplementedError ( \"Specification must specify apply()\" ) def __call__ ( self , query ): return self . apply ( query ) apply(query) -> query apply is the main functions that you are going to override. It is used in order to specify new things in your query. You get the query in the specification and return an updated version of query. For example, if your query has a filter function, and you want to filter by username, then you can create this: from assimilator.core.database import Specification class UsernameSpecification ( Specification ): def __init__ ( self , username : str ): super ( UsernameSpecification , self ) . __init__ () self . username = username def apply ( self , query ): return query . filter ( username = username ) Here we do the following: 1) save the username in the constructor 2) override apply() and return an updated version of the query provided The usage of this specification will look like this: repository = UserRepository ( session ) user = repository . get ( UsernameSpecification ( username = \"python.on.papyrus\" )) The repository will apply the specification and return the results. Functional specifications __call__() functions receives the query and call apply() in the specification. This is used in order to make functional specifications possible. __call__() allows you to call an object as a function: obj() . This way, we can use functional specifications and class-based specifications together. If you want to create a functional specification, then you need to use the @specification decorator: from assimilator.core.database import specification @specification def username_filter ( query , username : str ): return query . filter ( username = username ) The function above is the equivalent of the UsernameSpecification class. Then, you are going to use it like this: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" )) Both types work the same, so you can choose the type of specifications that you like. But, you can also use them together: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" ), AgeGreaterSpecification ( age_gt = 18 ), JoinSpecification ( Friends ), ) SpecificationList SpecificationList is a static class that contains basic specifications for our repository. Specifications: filter() filters the data order() orders the data paginate() paginates the data(limits the results, offsets them). join() joins entities together(join a table, get related data). The reason we use SpecificationList is because we want to have an abstraction for our specifications. Take two examples: from dependencies import UserRepository from assimilator.alchemy.database import alchemy_filter def get_user ( id : int , repository : UserRepository ): return repository . get ( alchemy_filter ( id = id )) # we use alchemy_filter() directly In that example, we use alchemy_filter() directly, which may not seem as an issue, however, if we would want to change our UserRepository to work with RedisRepository , then we would have to change all of our specifications ourselves. In order to fix this, we can use SpecificationList: from dependencies import UserRepository def get_user ( id : int , repository : UserRepository ): return repository . get ( repository . specifications . filter ( id = id )) # we call the filter from repository specifications. Now, we only have to change the repository without worrying about other parts of the code. Here is how you can create your own SpecificationList: from assimilator.core.database import SpecificationList , specification , Specification from pagination_func import paginate_data @specification def filter_specification ( filters , query ): return query . do_filter ( filters ) @specification def join_specification ( query ): return query # we do not need joins in our data structure, so we leave it class OrderSpecification ( Specification ): def __init__ ( self , orders ): self . orders = orders def apply ( self , query ): return query . make_order ( self . orders ) class MySpecificationList ( SpecificationList ): filter = filter_specification # we use function name as the specification order = OrderSpecification # lambda specification paginate = paginate_data # imported specification join = join_specification Notice that we never call the functions, cause the only thing we need are links to the specifications. Then, when you build your Repository: from specifications import MySpecificationList repository = MyRepository ( session = session , specifications = MySpecificationList ) Once you have done that, the repository will use your specifications. Of course, you can still use specifications directly, but if you ever need to change the repository, then it may be a problem. LazyCommand Sometimes we don't want to execute the query right away. For example, for optimization purposes or some other purpose that requires us to delay the execution. In that case, you want to find lazy argument in the function that you are calling and set it to True . After that, a LazyCommand is going to be returned. That object allows you to call it as a function or iterate over it to get the results: from assimilator.core.database import Repository def print_all_usernames ( repository : Repository ): for user in repository . filter ( lazy = True ): print ( user . username ) # we don't want to receive a list of all the users, but want to iterate # through it and only get 1 user at a time def count_users_if_argument_true ( do_count , repository : Repository ): count_command = repository . count ( lazy = True ) # turn on lazy and get LazyCommand if do_count : return count_command () # call for the result return - 1 Unit of Work Unit of work allows us to work with transactions and repositories that change the data. The problem with repository is the transaction management. We want to make our transaction management as easy as possible, but repositories are not responsible for that. That is why we have units of work. They allow us to do the following: 1. Start the transaction 2. Provide the repository to the client code 3. If there are exceptions, errors, issues with the client code, rollback the transaction and remove all the changes 4. If everything is good, the client code calls the commit() function and finishes the data change 5. Unit of work closes the transaction __init__() repository: BaseRepository - The repository is provided in the UnitOfWork when we create it. The session to the data storage is going to be taken from the repository. begin() Starts the transaction. The function is called automatically. rollback() Removes all the changes from the transaction. You do not need to call that function, as it is called automatically if there is an error in your code. commit() Saves the changes to the data storage. While the repository only adds the temporary, this function is responsible for the final save. You need to call it yourself, it will not be called automatically like rollback() close() Closes the transaction. The function is called automatically. Here is how you can use UnitOfWork in your code: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork uow . commit () # commit the changes making them final. If the function is not called, nothing is saved. As you can see, you do not need to call any function except for commit() . You should also use context managers( with uow: ) to start the transaction and rollback if there is an exception: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork 1 / 0 # ZeroDivisionError. UnitOfWork calls rollback automatically. uow . commit () # nothing is saved, since the rollback was called.","title":"API Reference | Core Database"},{"location":"api_reference/core/#repository","text":"","title":"Repository"},{"location":"api_reference/core/#__init__","text":"session: SessionT - each repository has a session that works as the primary data source. It can be your database connection, a text file or a data structure. initial_query: Optional[SessionT] = None - the initial query that you use in the data storage. We will show how it works later. It can be an SQL query, a key in the dictionary or anything else. specifications: SpecificationList - an object that contains links to the specifications that are used to create your queries model: Type[ModelT] - class of the model that we want to work with. It can be an SQLAlchemy model, Pydantic model, or your custom class. specifications: Type[SpecificationList] - SpecificationList class with all the specifications that you need error_wrapper: Optional[ErrorWrapper] = ErrorWrapper() - ErrorWrapper class that allows you to convert external providers errors to your custom errors.","title":"__init__()"},{"location":"api_reference/core/#_check_obj_is_specification-tupleoptionalmodelt-iterablespecificationtype","text":"This function is called for parts of the code that use both obj and *specifications. We check that if the obj is a model. If that is the case, we swap it around. obj: ModelT specifications: Iterable[SpecificationType]","title":"_check_obj_is_specification() -&gt; Tuple[Optional[ModelT], Iterable[SpecificationType]]"},{"location":"api_reference/core/#specs-typespecificationlist","text":"Property for a shorter name version of specifications attribute.","title":"specs -&gt; Type[SpecificationList]"},{"location":"api_reference/core/#_get_initial_query-queryt","text":"Returns initial query for the data querying. Can return override_query as the initial query if it is present. override_query: Optional[QueryT] = None - overrides the initial query. Used for more flexibility in querying.","title":"_get_initial_query() -&gt; QueryT"},{"location":"api_reference/core/#_apply_specifications-queryt","text":"Applies Specifications to the query. Must not be used directly. apply specifications gets a list of specifications and applies them to the query returned in _get_initial_query(). The idea is the following: each specification gets a query and adds some filters to it. At the end we get a fully working query modified with the specifications provided by the user. specifications: Iterable[Specifications] - an iterable of specifications that can be used to specify some conditions in the query Specification is a pattern that adds filters or anything else that specifies what kind of data we want.","title":"_apply_specifications() -&gt; QueryT"},{"location":"api_reference/core/#get","text":"get is the function used to query the data storage and return one entity. You supply a list of specifications that get you the entity from the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument","title":"get()"},{"location":"api_reference/core/#filter","text":"filters is the function used to query the data storage and return many entities. You supply a list of specifications that filter entities in the storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument","title":"filter()"},{"location":"api_reference/core/#save","text":"Adds the objects to the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just add them ).","title":"save()"},{"location":"api_reference/core/#delete","text":"Deletes the objects from the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just delete them from your session ).","title":"delete()"},{"location":"api_reference/core/#update","text":"Updates the objects in the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just update them in your session ).","title":"update()"},{"location":"api_reference/core/#is_modified","text":"Checks whether an obj was modified or not. If any value changes within the object, then it must return True","title":"is_modified()"},{"location":"api_reference/core/#refresh","text":"Updates the object values with the values in the data storage. That can be useful if you want to create an object and get its id that was generated in the storage, or if you just want to have the latest saved version of the object.","title":"refresh()"},{"location":"api_reference/core/#count","text":"Counts the objects while applying specifications to the query. Give no specifications to count the whole data storage. specifications: Specifications - specifications that can be used to specify some conditions in the query lazy: bool - whether you want to execute your query straight away or just build it for the future","title":"count()"},{"location":"api_reference/core/#creating-your-own-repository","text":"If you want to create your own repository, then you are going to have to override all the functions above. But, please, do not make new functions available to the outer world. You can do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def _users_private_func ( self ): # Cannot be called outside return 'Do something' And call that function inside of your repository. But, never do this: from assimilator.core.database import Repository class UserRepository ( Repository ): def get_ser_by_id ( self ): # Cannot be called outside return self . get ( filter_specification ( id = 1 )) Since it is going to be really hard for you to replace one repository to another. Example: from assimilator.core.database import Repository from users.repository import UserRepository from products.repository import ProductRepository def get_by_id ( id , repository : Repository ): return repository . get ( filter_specification ( id = 1 )) get_by_id ( UserRepository ()) get_by_id ( ProductRepository ()) # You can call the function with both repositories, and it will probably work fine","title":"Creating your own repository:"},{"location":"api_reference/core/#how-to-create-my-repositories","text":"You want to create a repository for entities in your projects. That means, that if you have some auxiliary table in your app, then it probably should not have a repository. But, things like Users, Products, Orders and others might have their own repository. That does not mean that you will create a lot of classes, but please do not add repositories for every class in your system . If you want to read more, please, look into the Domain-Driven Development books.","title":"How to create  my repositories?"},{"location":"api_reference/core/#specification","text":"Specification is the pattern that adds values, filters, joins or anything else to the query in your repository. It can also work as a filter for your objects.","title":"Specification"},{"location":"api_reference/core/#class-based-specifications","text":"class Specification ( ABC ): @abstractmethod def apply ( self , query ): raise NotImplementedError ( \"Specification must specify apply()\" ) def __call__ ( self , query ): return self . apply ( query ) apply(query) -> query apply is the main functions that you are going to override. It is used in order to specify new things in your query. You get the query in the specification and return an updated version of query. For example, if your query has a filter function, and you want to filter by username, then you can create this: from assimilator.core.database import Specification class UsernameSpecification ( Specification ): def __init__ ( self , username : str ): super ( UsernameSpecification , self ) . __init__ () self . username = username def apply ( self , query ): return query . filter ( username = username ) Here we do the following: 1) save the username in the constructor 2) override apply() and return an updated version of the query provided The usage of this specification will look like this: repository = UserRepository ( session ) user = repository . get ( UsernameSpecification ( username = \"python.on.papyrus\" )) The repository will apply the specification and return the results.","title":"Class-based specifications"},{"location":"api_reference/core/#functional-specifications","text":"__call__() functions receives the query and call apply() in the specification. This is used in order to make functional specifications possible. __call__() allows you to call an object as a function: obj() . This way, we can use functional specifications and class-based specifications together. If you want to create a functional specification, then you need to use the @specification decorator: from assimilator.core.database import specification @specification def username_filter ( query , username : str ): return query . filter ( username = username ) The function above is the equivalent of the UsernameSpecification class. Then, you are going to use it like this: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" )) Both types work the same, so you can choose the type of specifications that you like. But, you can also use them together: repository = UserRepository ( session ) user = repository . get ( username_filter ( username = \"python.on.papyrus\" ), AgeGreaterSpecification ( age_gt = 18 ), JoinSpecification ( Friends ), )","title":"Functional specifications"},{"location":"api_reference/core/#specificationlist","text":"SpecificationList is a static class that contains basic specifications for our repository. Specifications:","title":"SpecificationList"},{"location":"api_reference/core/#filter_1","text":"filters the data","title":"filter()"},{"location":"api_reference/core/#order","text":"orders the data","title":"order()"},{"location":"api_reference/core/#paginate","text":"paginates the data(limits the results, offsets them).","title":"paginate()"},{"location":"api_reference/core/#join","text":"joins entities together(join a table, get related data). The reason we use SpecificationList is because we want to have an abstraction for our specifications. Take two examples: from dependencies import UserRepository from assimilator.alchemy.database import alchemy_filter def get_user ( id : int , repository : UserRepository ): return repository . get ( alchemy_filter ( id = id )) # we use alchemy_filter() directly In that example, we use alchemy_filter() directly, which may not seem as an issue, however, if we would want to change our UserRepository to work with RedisRepository , then we would have to change all of our specifications ourselves. In order to fix this, we can use SpecificationList: from dependencies import UserRepository def get_user ( id : int , repository : UserRepository ): return repository . get ( repository . specifications . filter ( id = id )) # we call the filter from repository specifications. Now, we only have to change the repository without worrying about other parts of the code. Here is how you can create your own SpecificationList: from assimilator.core.database import SpecificationList , specification , Specification from pagination_func import paginate_data @specification def filter_specification ( filters , query ): return query . do_filter ( filters ) @specification def join_specification ( query ): return query # we do not need joins in our data structure, so we leave it class OrderSpecification ( Specification ): def __init__ ( self , orders ): self . orders = orders def apply ( self , query ): return query . make_order ( self . orders ) class MySpecificationList ( SpecificationList ): filter = filter_specification # we use function name as the specification order = OrderSpecification # lambda specification paginate = paginate_data # imported specification join = join_specification Notice that we never call the functions, cause the only thing we need are links to the specifications. Then, when you build your Repository: from specifications import MySpecificationList repository = MyRepository ( session = session , specifications = MySpecificationList ) Once you have done that, the repository will use your specifications. Of course, you can still use specifications directly, but if you ever need to change the repository, then it may be a problem.","title":"join()"},{"location":"api_reference/core/#lazycommand","text":"Sometimes we don't want to execute the query right away. For example, for optimization purposes or some other purpose that requires us to delay the execution. In that case, you want to find lazy argument in the function that you are calling and set it to True . After that, a LazyCommand is going to be returned. That object allows you to call it as a function or iterate over it to get the results: from assimilator.core.database import Repository def print_all_usernames ( repository : Repository ): for user in repository . filter ( lazy = True ): print ( user . username ) # we don't want to receive a list of all the users, but want to iterate # through it and only get 1 user at a time def count_users_if_argument_true ( do_count , repository : Repository ): count_command = repository . count ( lazy = True ) # turn on lazy and get LazyCommand if do_count : return count_command () # call for the result return - 1","title":"LazyCommand"},{"location":"api_reference/core/#unit-of-work","text":"Unit of work allows us to work with transactions and repositories that change the data. The problem with repository is the transaction management. We want to make our transaction management as easy as possible, but repositories are not responsible for that. That is why we have units of work. They allow us to do the following: 1. Start the transaction 2. Provide the repository to the client code 3. If there are exceptions, errors, issues with the client code, rollback the transaction and remove all the changes 4. If everything is good, the client code calls the commit() function and finishes the data change 5. Unit of work closes the transaction","title":"Unit of Work"},{"location":"api_reference/core/#__init___1","text":"repository: BaseRepository - The repository is provided in the UnitOfWork when we create it. The session to the data storage is going to be taken from the repository.","title":"__init__()"},{"location":"api_reference/core/#begin","text":"Starts the transaction. The function is called automatically.","title":"begin()"},{"location":"api_reference/core/#rollback","text":"Removes all the changes from the transaction. You do not need to call that function, as it is called automatically if there is an error in your code.","title":"rollback()"},{"location":"api_reference/core/#commit","text":"Saves the changes to the data storage. While the repository only adds the temporary, this function is responsible for the final save. You need to call it yourself, it will not be called automatically like rollback()","title":"commit()"},{"location":"api_reference/core/#close","text":"Closes the transaction. The function is called automatically.","title":"close()"},{"location":"api_reference/core/#here-is-how-you-can-use-unitofwork-in-your-code","text":"from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork uow . commit () # commit the changes making them final. If the function is not called, nothing is saved. As you can see, you do not need to call any function except for commit() . You should also use context managers( with uow: ) to start the transaction and rollback if there is an exception: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user ( username : str , uow : UnitOfWork ): with uow : # start the transaction # everything in here is within the transaction new_user = User ( username = username ) uow . repository . save ( new_user ) # we get the repository from UnitOfWork 1 / 0 # ZeroDivisionError. UnitOfWork calls rollback automatically. uow . commit () # nothing is saved, since the rollback was called.","title":"Here is how you can use UnitOfWork in your code:"},{"location":"tutorial/advanced_database/","text":"Advanced database patterns usage Unit of work When you are using UnitOfWork pattern you will typically call context managers( with statement) and commit() function. But, if you want to use all the functions of this pattern manually, you can do it like this: from assimilator.core.database import UnitOfWork def example ( uow : UnitOfWork ): uow . begin () # begin the transaction try : uow . repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () # apply the changes except Exception as exc : uow . rollback () # remove all the changes if there is an exception finally : uow . close () # close the transaction However, most of the time, there is no need in doing that, and we recommend using with uow: instead. Writing your own specifications Sometimes you are copying your specifications. That is bad, and can lead to many bugs and legacy code. Instead, you need to identify the places where you copy your specifications, and your own specification that can be used easily. Now, let's see how to do that. How do specifications work? Specification is a class or a function that does the following: it gets your initial query, changes it, and returns the new version. That is a simplified version of the source code for one of the internal specifications: from typing import Optional from assimilator.core.database import specification @specification # specification decorator def internal_paginate ( query : list , # query from the repository limit : Optional [ int ] = None , offset : Optional [ int ] = None , ) -> list : # we update the query with Python slices and return new variant return query [ offset : limit ] Each specification must return an updated version of the query, or something that can be used logically with other specifications. In this case, we get our query as a list of object, change the list, and return the updated version. So, each specification adds something to the query, and that updated query goes to the next specification. After that, our Repository pattern applies the specifications and returns our new result! SUPER IMPORTANT . You are probably wondering: when do we get this query argument and how will the users pass it to the specification? The initial_query argument that you provide in the Repository is your query, and you NEVER use query when you call specifications in your repository. The query is supplied automatically. If you want to write your own specification, then you must choose between functional specifications and class-based specifications. We would suggest to use functional specifications whenever you can! Functional specifications Functional specifications are created with @specification decorator. You must do the following to create a functional specification: Create a function with @specification pattern. Add any parameters and add query parameter as the last one. Do something with the query in the function. Return an updated query. Let's say that we want to create a specification that only allows validated objects from a list: from assimilator.core.database import specification @specification def validated_only ( query : list ): return list ( filter ( lambda obj : obj . validated , query )) Now, we can use that specification in our patterns: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( validated_only () # always call the specification. ) If we want to add some arguments, we can easily do it. But, you must be sure that you are adding your arguments before the query parameter: from assimilator.core.database import specification @specification def validated_only ( check_vip : bool , query : list ): if check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our updated specification like this: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( # We do not pass our query parameter. It is added automatically. validated_only ( is_vip = True ) ) Class-based specifications Class-based specifications are a little more advanced. You use them when you need auxiliary methods, inheritance, abstractions or anything else that classes can offer. You can create them with these steps: Create a class that inherits from Specification . Add all the arguments that you need in the __init__() function. Add apply() function from the Specification class. Change the query and return it. from assimilator.core.database import Specification class ValidatedOnly ( Specification ): # inherits from Specification abstract class def __init__ ( self , check_vip : bool ): # all the arguments go here super ( ValidatedOnly , self ) . __init__ () self . check_vip = check_vip # apply() will only get query as the argument. def apply ( self , query : list ) -> list : if self . check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our class-based specification like this: from assimilator.core.database import Repository from specifications import ValidatedOnly def filter_all_validated ( repository : Repository ): return repository . filter ( ValidatedOnly ( is_vip = True ) # Usage techniques are the same ) Using SpecificationList pattern What is the problem with our custom specifications? They are direct. We cannot use them if the query type changes, so that means that we cannot use them with different repositories. We can solve that issue with another pattern called SpecificationList . It allows you to map your specifications to classes that can call different specifications for different repositories. When you use repository.specifications or repository.specs , you are using SpecificationList objects from these repositories. The most basic SpecificationList looks like this: from typing import Type from assimilator.core.database import FilterSpecification from assimilator.core.database.specifications.types import ( OrderSpecificationProtocol , PaginateSpecificationProtocol , OnlySpecificationProtocol , JoinSpecificationProtocol , ) class SpecificationList : filter : Type [ FilterSpecification ] order : OrderSpecificationProtocol paginate : PaginateSpecificationProtocol join : JoinSpecificationProtocol only : OnlySpecificationProtocol We have all the specifications that we talked about, and each specification has a Protocol typing that allows us to specify what arguments we need for specific pre-built specifications. If you want to create your own SpecificationList , then be sure to do the following: Create a class that inherits from SpecificationList . Add all the pre-built specifications if your base class doesn't specify them. Add your custom specifications. Add your custom specification list to your repository. from sqlalchemy.orm import Query from assimilator.alchemy.database import AlchemySpecificationList from assimilator.core.database import specification @specification def alchemy_validate ( validate_vip : bool , query : Query ): if validate_vip : return query . filter ( is_vip = True , is_validated = True ) return query . filter ( is_validated = True ) class CustomSpecificationList ( AlchemySpecificationList ): # We don't need to specify pre-built specifications, # we already have them in the AlchemySpecificationList validated = alchemy_validate Then, we can use that specification list with any repository that works with alchemy: from assimilator.alchemy.database import AlchemyRepository from specifications import CustomSpecificationList repository = AlchemyRepository ( session = DatabaseSession (), model = User , specifications = CustomSpecificationList , # Change specifications list ) repository . filter ( repository . specs . validated () # use the specification indirectly ) But, how do we make it so that the specification can work with other repositories? We have to write different specifications and specification lists. There is just no other way(yet\ud83d\ude0e). So, if you want to use your specification with other patterns, rewrite it to work with their data types and create a new SpecificationList that is going to be supplied in the Repository.","title":"Advanced database tutorial"},{"location":"tutorial/advanced_database/#advanced-database-patterns-usage","text":"","title":"Advanced database patterns usage"},{"location":"tutorial/advanced_database/#unit-of-work","text":"When you are using UnitOfWork pattern you will typically call context managers( with statement) and commit() function. But, if you want to use all the functions of this pattern manually, you can do it like this: from assimilator.core.database import UnitOfWork def example ( uow : UnitOfWork ): uow . begin () # begin the transaction try : uow . repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () # apply the changes except Exception as exc : uow . rollback () # remove all the changes if there is an exception finally : uow . close () # close the transaction However, most of the time, there is no need in doing that, and we recommend using with uow: instead.","title":"Unit of work"},{"location":"tutorial/advanced_database/#writing-your-own-specifications","text":"Sometimes you are copying your specifications. That is bad, and can lead to many bugs and legacy code. Instead, you need to identify the places where you copy your specifications, and your own specification that can be used easily. Now, let's see how to do that.","title":"Writing your own specifications"},{"location":"tutorial/advanced_database/#how-do-specifications-work","text":"Specification is a class or a function that does the following: it gets your initial query, changes it, and returns the new version. That is a simplified version of the source code for one of the internal specifications: from typing import Optional from assimilator.core.database import specification @specification # specification decorator def internal_paginate ( query : list , # query from the repository limit : Optional [ int ] = None , offset : Optional [ int ] = None , ) -> list : # we update the query with Python slices and return new variant return query [ offset : limit ] Each specification must return an updated version of the query, or something that can be used logically with other specifications. In this case, we get our query as a list of object, change the list, and return the updated version. So, each specification adds something to the query, and that updated query goes to the next specification. After that, our Repository pattern applies the specifications and returns our new result! SUPER IMPORTANT . You are probably wondering: when do we get this query argument and how will the users pass it to the specification? The initial_query argument that you provide in the Repository is your query, and you NEVER use query when you call specifications in your repository. The query is supplied automatically. If you want to write your own specification, then you must choose between functional specifications and class-based specifications. We would suggest to use functional specifications whenever you can!","title":"How do specifications work?"},{"location":"tutorial/advanced_database/#functional-specifications","text":"Functional specifications are created with @specification decorator. You must do the following to create a functional specification: Create a function with @specification pattern. Add any parameters and add query parameter as the last one. Do something with the query in the function. Return an updated query. Let's say that we want to create a specification that only allows validated objects from a list: from assimilator.core.database import specification @specification def validated_only ( query : list ): return list ( filter ( lambda obj : obj . validated , query )) Now, we can use that specification in our patterns: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( validated_only () # always call the specification. ) If we want to add some arguments, we can easily do it. But, you must be sure that you are adding your arguments before the query parameter: from assimilator.core.database import specification @specification def validated_only ( check_vip : bool , query : list ): if check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our updated specification like this: from assimilator.core.database import Repository from specifications import validated_only def filter_all_validated ( repository : Repository ): return repository . filter ( # We do not pass our query parameter. It is added automatically. validated_only ( is_vip = True ) )","title":"Functional specifications"},{"location":"tutorial/advanced_database/#class-based-specifications","text":"Class-based specifications are a little more advanced. You use them when you need auxiliary methods, inheritance, abstractions or anything else that classes can offer. You can create them with these steps: Create a class that inherits from Specification . Add all the arguments that you need in the __init__() function. Add apply() function from the Specification class. Change the query and return it. from assimilator.core.database import Specification class ValidatedOnly ( Specification ): # inherits from Specification abstract class def __init__ ( self , check_vip : bool ): # all the arguments go here super ( ValidatedOnly , self ) . __init__ () self . check_vip = check_vip # apply() will only get query as the argument. def apply ( self , query : list ) -> list : if self . check_vip : return list ( filter ( lambda obj : obj . validated and obj . is_vip , query )) return list ( filter ( lambda obj : obj . validated , query )) We can use our class-based specification like this: from assimilator.core.database import Repository from specifications import ValidatedOnly def filter_all_validated ( repository : Repository ): return repository . filter ( ValidatedOnly ( is_vip = True ) # Usage techniques are the same )","title":"Class-based specifications"},{"location":"tutorial/advanced_database/#using-specificationlist-pattern","text":"What is the problem with our custom specifications? They are direct. We cannot use them if the query type changes, so that means that we cannot use them with different repositories. We can solve that issue with another pattern called SpecificationList . It allows you to map your specifications to classes that can call different specifications for different repositories. When you use repository.specifications or repository.specs , you are using SpecificationList objects from these repositories. The most basic SpecificationList looks like this: from typing import Type from assimilator.core.database import FilterSpecification from assimilator.core.database.specifications.types import ( OrderSpecificationProtocol , PaginateSpecificationProtocol , OnlySpecificationProtocol , JoinSpecificationProtocol , ) class SpecificationList : filter : Type [ FilterSpecification ] order : OrderSpecificationProtocol paginate : PaginateSpecificationProtocol join : JoinSpecificationProtocol only : OnlySpecificationProtocol We have all the specifications that we talked about, and each specification has a Protocol typing that allows us to specify what arguments we need for specific pre-built specifications. If you want to create your own SpecificationList , then be sure to do the following: Create a class that inherits from SpecificationList . Add all the pre-built specifications if your base class doesn't specify them. Add your custom specifications. Add your custom specification list to your repository. from sqlalchemy.orm import Query from assimilator.alchemy.database import AlchemySpecificationList from assimilator.core.database import specification @specification def alchemy_validate ( validate_vip : bool , query : Query ): if validate_vip : return query . filter ( is_vip = True , is_validated = True ) return query . filter ( is_validated = True ) class CustomSpecificationList ( AlchemySpecificationList ): # We don't need to specify pre-built specifications, # we already have them in the AlchemySpecificationList validated = alchemy_validate Then, we can use that specification list with any repository that works with alchemy: from assimilator.alchemy.database import AlchemyRepository from specifications import CustomSpecificationList repository = AlchemyRepository ( session = DatabaseSession (), model = User , specifications = CustomSpecificationList , # Change specifications list ) repository . filter ( repository . specs . validated () # use the specification indirectly ) But, how do we make it so that the specification can work with other repositories? We have to write different specifications and specification lists. There is just no other way(yet\ud83d\ude0e). So, if you want to use your specification with other patterns, rewrite it to work with their data types and create a new SpecificationList that is going to be supplied in the Repository.","title":"Using SpecificationList pattern"},{"location":"tutorial/database/","text":"Database patterns What patterns do we use? Database, in our case, is any data storage. It can be PostgreSQL, MySQL, Redis, File, external API or others. We use 5 main patterns with databases: Repository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. UnitOfWork - works with transactions. Ensures data integrity. Only used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in Repository . SpecificationList - contains links to Specification patterns to completely remove imports inside of Repository . LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results. Create/Read example\ud83d\ude00 Let's say that we use SQLAlchemy library in Python. We want to make a program that can save and read our users. Each user has a username and balance. The first thing that we do is we need to create SQLAlchemy tables. There is no Assimilator in that step . # models.py from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) # create engine to the SQLite Database Base = declarative_base () # Create a base class for our tables DatabaseSession = sessionmaker ( bind = engine ) # create a database connection(session) class User ( Base ): # Create user model __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) # username column balance = Column ( Float ()) # balance column Base . metadata . create_all ( engine ) # Create User table in the database! Most of you have probably seen that. We just create a new SQLAlchemy model in our project. Now, let's add our Assimilator patterns. Two patterns that we are going to use are Repository and UnitOfWork . Repository is responsible for data management. We use it to save, read, update, delete, modify, check and basically work with our database. UnitOfWork is responsible for transactions. We use it to apply the changes made by Repository . # dependencies.py # We import our patterns from alchemy submodule from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository # We also import User and DatabaseSession we created before from models import DatabaseSession , User user_repository = AlchemyRepository ( session = DatabaseSession (), model = User , ) # We create our repository and pass session and model to it # UnitOfWork just gets the repository pattern inside it. user_uow = AlchemyUnitOfWork ( repository = user_repository ) Now, we will use those patterns to create a user. We need to do the following things: Start a new transaction using UnitOfWork . Create new user using Repository . Apply the changes using UnitOfWork . The main idea here is that even if we create millions of users, the changes will not be applied to the database until UnitOfWork.commit() function is called. We do that so that if there is an error during our operations, new changes are not applied. If you still don't get the idea of database transactions - watch this video on my channel . from assimilator.core.database import UnitOfWork from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the UnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call UnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( user_uow ) We saved the user in the database. Now, let's read it. We use Specification pattern to limit the results using different criteria. If we want to filter the results(SQL WHERE), then we must use filter() specification. We can either import the specifications from the alchemy submodule, or we can access them from the Repository.specs property. When you are sure that your function is only going to read your database, then you should only use Repository pattern without UnitOfWork. This way, we will not commit any data to our database, and can be sure that the function only reads the data, without changing it. from assimilator.core.database import Repository from dependencies import user_repository def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( user_repository ) If you want to import the specification: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # everything else is the same except for the specification username = \"Andrey\" , ) ) user = get_user ( user_repository ) As you probably know, those were direct and indirect coding styles. You can read about them here . We would suggest you to use indirect(the first) coding style. You remove a lot of imports and can do pattern substitutions which are going to be discussed in the next example. Pattern substitution example\ud83d\ude42 What is pattern substitution? It is a technique where you can change the external providers in one step. Let's see what that means... When we write code, it is a good idea to not have dependencies. They are really hard to get rid of, hard to update or change. So, if we have as little dependencies as possible, this is only going to be better. Sometimes we want to change our database to another one, or we want to change the ORM(database library) that we are using. There is a possibility that we need caching in our program, our we want to run tests of our code without using a real database. All these examples are perfect for pattern substitution. For that case, we will change SQLAlchemy patterns that we wrote in the first example to Internal patterns with one line of code. Internal patterns work with Python data structures. Your database is a dictionary, list, class or anything else within your program. Internal patterns are really useful for testing! The first thing that we need to do is change the models in our models.py file: # models.py from assimilator.core.database import BaseModel # BaseModel is just a Pydantic model with id class User ( BaseModel ): # id is supplied by default username : str balance : float So, we changed our SQLAlchemy models to a BaseModel . Then, we need to change the patterns: # dependencies.py # We import the same patterns from internal submodule from assimilator.internal.database import InternalRepository , InternalUnitOfWork from models import User # import our BaseModel User # Session is a dictionary in InternalRepository. # That means, that we will store all our data in there. session = {} # We create our repository and pass session and model to it user_repository = InternalRepository ( session = session , model = User ) # UnitOfWork just gets the repository pattern inside it. user_uow = InternalUnitOfWork ( repository = user_repository ) That's it. After you changed that, your other code like create_user() and get_user() will work with dictionary and your new User as a data storage! Now, imagine that you have 200 functions, and in order to change one data storage to another you just need 1 line of code! About direct coding All of that magic with pattern substitution was possible because we used indirect coding. But, if we use direct coding, the situation may not be that sweet: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # We use AlchemyFilter with InternalRepository. # Those do not work together\ud83d\ude2d username = \"Andrey\" , ) ) user = get_user ( user_repository ) AlchemyFilter will not work with InternalRepository . We must go into our code and change it to InternalFilter . That is why we advise you to use indirect coding whenever possible. Errors example\ud83d\ude30 We have already seen perfect examples of working code. But, errors and exceptions happen! That is why we need to ensure that the code that we write is error-proof. However, we have already done everything to do that\ud83d\ude33. As I said earlier, UnitOfWork pattern is used for transaction management. That means, that we use it to commit the data if everything is OK, or rollback the changes if there are errors. When we use context managers( with uow ) with UnitOfWork pattern we make it so that if there are errors, all the pending changes are dropped. So, if you want to add try and except to your code, then just do it like this: from assimilator.core.database import UnitOfWork , InvalidQueryError from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): try : with uow : # We use that to start the transaction repository = uow . repository new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () return new_user except InvalidQueryError : print ( \"Error in user creation\" ) return None # no user created created_user = create_user ( user_uow ) But, even if you do not use try and except, you are sure that your database does not have any weird changes in it! That's the power of UnitOfWork . If you ever want to rollback yourself, then use UnitOfWork.rollback() function. It will remove all the pending changes. That is the function that is called if there is an exception in the with uow block. Other functions you must know Here are more short examples regarding Database functions that you might want to use: Data querying from assimilator.core.database import Repository def example ( repository : Repository ): repository . get () # get one entity from the database # get() function can raise NotFoundError() or MultipleResultsError() repository . filter () # get many entities from the database # When you use those functions, you can add specifications to limit the results: adult_users = repository . filter ( repository . specs . filter ( # we use filter specification age__gte = 18 , # get all the users older than 18 years ) ) for adult_user in adult_users : print ( adult_user . username ) There are different filtering options inside of filter() specification: __eq = equal to. You can omit it and just use field=value as we did before __gt = greater than. Example: age__gt=18 == (age > 18) __gte = greater than equals. Example: age__gte=18 == (age >= 18) __lt = lower than. Example: age__lt=18 == (age < 18) __lte = lower than equals. Example: age__lte=18 == (age <= 18) __not = not equal. Example: age__not=18 == (age != 18) __is = is True or False. Example: validated__is=True == (validated is True) __like = like SQL expression. Converted to regex if not supported. Example: username__like=\"Andrey%\" == all usernames that start with Andrey __regex = regular expression. Example: username__regex=\"[1-3]+And.rey\\w+\" == regular expression, what is there to explain? You can use these options like that: from assimilator.core.database import Repository def filter_example ( repository : Repository ): # Get all users between ages 18 to 25 with username # that has \"And\" inside and those who are validated. repository . filter ( repository . specs . filter ( age__gt = 18 , age__lt = 25 , username__like = \"%And%\" , validated__is = True , ) ) All the users can be queried with Repository.filter() without any specifications: from assimilator.core.database import Repository def get_all_users ( repository : Repository ): all_users = repository . filter () # get all users from the database Pagination is added with paginate() specification: from assimilator.core.database import Repository def paginate ( repository : Repository ): paginated_users = repository . filter ( repository . specs . paginate ( limit = 10 , # limit the results by 10 offset = 20 , # offset the results by 20 ), ) Ordering is added with order() specification: from assimilator.core.database import Repository def order ( repository : Repository ): ordered_users = repository . filter ( repository . specs . order ( 'username' , # order users by username(Ascending ordering) '-balance' , # second order of the users is balance(descending order) ) ) # - in front means descending Entity joins are added with join() specification: from assimilator.core.database import Repository def join_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . join ( Order . id , # another model order that is joined with user Product . product_id , # another model product that is joined with user ) ) If you want to optimize your queries, you can do so by using only() . It will accept fields that will be the only ones on your model: from assimilator.core.database import Repository def only_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . only ( 'id' , 'username' ) # We only query `id` and `username` from the database. # That reduces results size and query execution time ) If you want to count something, you can use count() : from assimilator.core.database import Repository def count_example ( repository : Repository ): users_count : int = repository . count () # Count all users other_users_count : int = repository . count ( repository . specs . filter ( id__gt = 10 ) # Count all users with id > 10 ) Sometimes you want to check if your object was updated or not. You can use is_modified() : is_modified : bool = repository . is_modified ( user ) Lazy evaluation\ud83d\ude34 Let's say that you want to load all the users from your table. But, the thing is, you don't need to use them straight away. Maybe, you want to return the result to another function, or set it as an attribute of an object. If you use the patterns that we gave you, then your code is going to be clean, but memory-heavy. To avoid that, you can prepare your function to be executed with another pattern called LazyCommand . It saves the function and all the arguments that you want to provide, and executes it only when you need it! You can add lazy=True to enable it in your Repository : from assimilator.core.database import Repository def example ( repository : Repository ): # Executes on the spot users_list = repository . filter () # Creates a lazy command that can be executed later users_filter_lazy_command = repository . filter ( lazy = True ) Now, we can optimize our program like this: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User , user_repository # We use typing for LazyCommand and show that it returns a list of Users def caller ( repository : Repository ) -> LazyCommand [ List [ User ]]: return repository . filter ( repository . specs . filter ( age__gt = 18 ), lazy = True , # make it lazy ) def second_function (): return caller ( user_repository ) # Database query not executed yet def first_function (): results = second_function () # Execute more code... for user in results : # The query is executed here ... We can optimize our code drastically. Now, we will only make the queries whenever we need them! But, if your query returns an error, that error is returned to the query execution, not creation! That is going to be first_function() in our case. Be sure to handle exceptions in the right place. Here are the places when your command is executed: from assimilator.core.database import Repository , LazyCommand def lazy_command_execution ( repository : Repository ): lazy_command : LazyCommand = repository . filter ( lazy = True ) if lazy_command : # query is executed in boolean statements print ( \"Executed!\" ) for data in lazy_command : # query is executed in iterators print ( \"Executed!\" ) lazy_user : LazyCommand = repository . get ( repository . specs . filter ( id = 1 )) print ( \"Executed for User id:\" , lazy_user . id ) # attribute access execution print ( lazy_user > 10 ) # Boolean execution Another important thing about LazyCommand is its execution policy. The thing is that if you use the same LazyCommand object many times, the command is only going to be executed once. This code only runs the query once: lazy_command_obj () # command executed lazy_command_obj () lazy_command_obj () lazy_command_obj () lazy_command_obj () # the same result in every other call Another FAR MORE IMPORTANT thing is the return type of your LazyCommand . If you call Repository.filter() , it's going to be an Iterable. If you use Repository.get() , it is just one entity. We suggest you add types with Python typings: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User def lazy_type_example ( repository : Repository ): lazy_command_many : LazyCommand [ List [ User ]] = repository . filter ( lazy = True ) lazy_command_obj : LazyCommand [ User ] = repository . get ( lazy = True ) # Now we know what is returned when the command is executed: users : List [ User ] = lazy_command_many () one_user : User = lazy_command_obj () The last thing is building your own LazyCommand objects: from assimilator.core.patterns import LazyCommand def func ( a , b , c ): return a + b + c command : LazyCommand [ int ] = LazyCommand ( command = func , # NOT func() a = 1 , b = 2 , c = 3 , # function arguments ) assert command == 1 + 2 + 3 Also, you can use the decorator to make your whole function lazy: from assimilator.core.patterns import LazyCommand @LazyCommand . decorate def decorated_lazy ( a , b , c ): return a + b + c print ( decorated_lazy ( 1 , 2 , 3 )) # 6 print ( decorated_lazy ( 1 , 2 , 3 , lazy = True )) # LazyCommand More on filter specification You have probably wondered how to do OR statement in the filter specification. What about AND statement? How are we going to implement all these things without using direct coding? You can use special operations like these: # OR operation. username==\"Andrey\" or username==\"Python\": repository . specs . filter ( username = \"Andrey\" ) | repository . specs . filter ( username = \"Python\" ) # AND operation. username==\"Andrey\" and age==22: repository . specs . filter ( username = \"Andrey\" ) & repository . specs . filter ( age = 22 ) # AND operation, but shorter: repository . specs . fitler ( username = \"Andrey\" , age = 22 ) # NOT operation. age != 55 ~ repository . specs . filter ( age = 55 ) # Combining operations together. # (username=\"Andrey\" and age=22) or (username == \"Python\" and age > 18) repository . specs . filter ( username = \"Andrey\" , age = 22 ) | \\ repository . specs . filter ( username = \"Python\" ) & \\ ! repository . specs . filter ( age__gt = 18 ) Another question that you probably have is how to make all of that shorter. Writing the specification again and again can be tiring. Good thing you can save them(not only filter specification. Any specification in general): andrey_username_spec = repository . specs . filter ( username = \"Andrey\" ) andrey = repository . filter ( andrey_username_spec ) Data changes Let's finally change some data. Repository save() function can be used with arguments or provided model: repository . save ( # indirect method. username = \"Andrey\" , balance = 1000 , ) # OR user = User ( username = \"Andrey\" , balance = 1000 ) repository . save ( user ) # direct method. Repository update() function can be used to update models: user = repository . get ( repository . specs . limit ( limit = 1 )) user . balance += 100 repository . update ( user ) # update the user It can also be used to update a lot of entities at once: repository . update ( # you provide specifications to filter the results repository . specs . filter ( age__gt = 18 ), repository . specs . limit ( limit = 100 ), # then, you provide field=new_value pairs to update the fields is_validated = False , updated_field = \"New value\" , ) Use delete() to delete one model: repository . delete ( user ) Or many models at once: repository . delete ( # delete everyone under 18 repository . specs . filter ( age__lt = 18 ) ) delete() is partially safe. That means that you cannot delete your whole database, cause if you provide nothing, you will delete nothing. But, still check your specifications in mass delete statements. Use refresh() to update the values in your old object. It goes to the database and changes your old values to new if they were updated: repository . refresh ( old_user ) assert old_user . updated_field == repository . get ( repository . specs . filter ( id = old_user . id )) . updated_field Typical flows Data Creation 1) You create Repository and UnitOfWork. from assimilator.alchemy.database import AlchemyRepository , AlchemyUnitOfWork user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) 2) You provide them in the function as parameters. from assimilator.core.database import UnitOfWork def create_user ( new_username : str , uow : UnitOfWork ): ... # UnitOfWork is a parameter 3) You use context manager(with statement in Python) with UnitOfWork. def create_user ( new_username : str , uow : UnitOfWork ): with uow : # Start the transaction in the database ... 4) You get the repository from UnitOfWork and use save() to save the result. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user 5) You use UnitOfWork commit() to apply the changes to the database. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user uow . commit () # Save changes do the database return new_user # return new user Data Filtering 1) You create Repository. from assimilator.alchemy.database import AlchemyRepository user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) 2) You provide it in the function as a parameter. from assimilator.core.database import Repository def filter_users ( age : int , repository : Repository ): ... # Repository is a parameter 3) You use Repository filter() function to filter the results. def filter_users ( age : int , repository : Repository ): return repository . filter ( ... ) 4) You use repository.specs to access the specifications. Then, you choose filter to filter the users who are 18 or older: def filter_users ( age : int , repository : Repository ): return repository . filter ( repository . specs . filter ( age__gte = 18 ) # age >= 18 ) 5) Optional step You can use direct coding style to use the specification like this: from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( AlchemyFilter ( age__gte = 18 ) # age >= 18 ) 6) Optional step You use direct coding style with SQLAlchemy filter from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( User . age >= 18 # Your SQLAlchemy User model. age >= 18 )","title":"Database tutorial"},{"location":"tutorial/database/#database-patterns","text":"","title":"Database patterns"},{"location":"tutorial/database/#what-patterns-do-we-use","text":"Database, in our case, is any data storage. It can be PostgreSQL, MySQL, Redis, File, external API or others. We use 5 main patterns with databases: Repository - works with the data. Saves, reads, updates, deletes, modifies, checks, filters our data. UnitOfWork - works with transactions. Ensures data integrity. Only used when the data is changed. Specification - some sort of filter for the repository. Filters, paginates, joins, limits the results in Repository . SpecificationList - contains links to Specification patterns to completely remove imports inside of Repository . LazyCommand - database query that has been created, but not ran yet. Only runs the function when we need the results.","title":"What patterns do we use?"},{"location":"tutorial/database/#createread-example","text":"Let's say that we use SQLAlchemy library in Python. We want to make a program that can save and read our users. Each user has a username and balance. The first thing that we do is we need to create SQLAlchemy tables. There is no Assimilator in that step . # models.py from sqlalchemy import create_engine , Column , String , Float , Integer from sqlalchemy.orm import declarative_base , sessionmaker engine = create_engine ( url = \"sqlite:///:memory:\" ) # create engine to the SQLite Database Base = declarative_base () # Create a base class for our tables DatabaseSession = sessionmaker ( bind = engine ) # create a database connection(session) class User ( Base ): # Create user model __tablename__ = \"users\" id = Column ( Integer (), primary_key = True ) username = Column ( String ()) # username column balance = Column ( Float ()) # balance column Base . metadata . create_all ( engine ) # Create User table in the database! Most of you have probably seen that. We just create a new SQLAlchemy model in our project. Now, let's add our Assimilator patterns. Two patterns that we are going to use are Repository and UnitOfWork . Repository is responsible for data management. We use it to save, read, update, delete, modify, check and basically work with our database. UnitOfWork is responsible for transactions. We use it to apply the changes made by Repository . # dependencies.py # We import our patterns from alchemy submodule from assimilator.alchemy.database import AlchemyUnitOfWork , AlchemyRepository # We also import User and DatabaseSession we created before from models import DatabaseSession , User user_repository = AlchemyRepository ( session = DatabaseSession (), model = User , ) # We create our repository and pass session and model to it # UnitOfWork just gets the repository pattern inside it. user_uow = AlchemyUnitOfWork ( repository = user_repository ) Now, we will use those patterns to create a user. We need to do the following things: Start a new transaction using UnitOfWork . Create new user using Repository . Apply the changes using UnitOfWork . The main idea here is that even if we create millions of users, the changes will not be applied to the database until UnitOfWork.commit() function is called. We do that so that if there is an error during our operations, new changes are not applied. If you still don't get the idea of database transactions - watch this video on my channel . from assimilator.core.database import UnitOfWork from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): with uow : # We use that to start the transaction repository = uow . repository # access the repository from the UnitOfWork # Save the user using Repository.save() function and by passing all the arguments inside new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) # WARNING!!! We have not applied any changes up to that point. # We must call UnitOfWork.commit() to change that: uow . commit () # Changes are applied and used is in the database! return new_user created_user = create_user ( user_uow ) We saved the user in the database. Now, let's read it. We use Specification pattern to limit the results using different criteria. If we want to filter the results(SQL WHERE), then we must use filter() specification. We can either import the specifications from the alchemy submodule, or we can access them from the Repository.specs property. When you are sure that your function is only going to read your database, then you should only use Repository pattern without UnitOfWork. This way, we will not commit any data to our database, and can be sure that the function only reads the data, without changing it. from assimilator.core.database import Repository from dependencies import user_repository def get_user ( repository : Repository ): # only pass the Repository because we want to read the data return repository . get ( # use get() to query one user from the database repository . specs . filter ( # use filter specification to give your filtering criteria username = \"Andrey\" , ) ) user = get_user ( user_repository ) If you want to import the specification: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # everything else is the same except for the specification username = \"Andrey\" , ) ) user = get_user ( user_repository ) As you probably know, those were direct and indirect coding styles. You can read about them here . We would suggest you to use indirect(the first) coding style. You remove a lot of imports and can do pattern substitutions which are going to be discussed in the next example.","title":"Create/Read example\ud83d\ude00"},{"location":"tutorial/database/#pattern-substitution-example","text":"What is pattern substitution? It is a technique where you can change the external providers in one step. Let's see what that means... When we write code, it is a good idea to not have dependencies. They are really hard to get rid of, hard to update or change. So, if we have as little dependencies as possible, this is only going to be better. Sometimes we want to change our database to another one, or we want to change the ORM(database library) that we are using. There is a possibility that we need caching in our program, our we want to run tests of our code without using a real database. All these examples are perfect for pattern substitution. For that case, we will change SQLAlchemy patterns that we wrote in the first example to Internal patterns with one line of code. Internal patterns work with Python data structures. Your database is a dictionary, list, class or anything else within your program. Internal patterns are really useful for testing! The first thing that we need to do is change the models in our models.py file: # models.py from assimilator.core.database import BaseModel # BaseModel is just a Pydantic model with id class User ( BaseModel ): # id is supplied by default username : str balance : float So, we changed our SQLAlchemy models to a BaseModel . Then, we need to change the patterns: # dependencies.py # We import the same patterns from internal submodule from assimilator.internal.database import InternalRepository , InternalUnitOfWork from models import User # import our BaseModel User # Session is a dictionary in InternalRepository. # That means, that we will store all our data in there. session = {} # We create our repository and pass session and model to it user_repository = InternalRepository ( session = session , model = User ) # UnitOfWork just gets the repository pattern inside it. user_uow = InternalUnitOfWork ( repository = user_repository ) That's it. After you changed that, your other code like create_user() and get_user() will work with dictionary and your new User as a data storage! Now, imagine that you have 200 functions, and in order to change one data storage to another you just need 1 line of code!","title":"Pattern substitution example\ud83d\ude42"},{"location":"tutorial/database/#about-direct-coding","text":"All of that magic with pattern substitution was possible because we used indirect coding. But, if we use direct coding, the situation may not be that sweet: from assimilator.core.database import Repository from assimilator.alchemy.database import AlchemyFilter # import AlchemyFilter specification from dependencies import user_repository def get_user ( repository : Repository ): return repository . get ( AlchemyFilter ( # We use AlchemyFilter with InternalRepository. # Those do not work together\ud83d\ude2d username = \"Andrey\" , ) ) user = get_user ( user_repository ) AlchemyFilter will not work with InternalRepository . We must go into our code and change it to InternalFilter . That is why we advise you to use indirect coding whenever possible.","title":"About direct coding"},{"location":"tutorial/database/#errors-example","text":"We have already seen perfect examples of working code. But, errors and exceptions happen! That is why we need to ensure that the code that we write is error-proof. However, we have already done everything to do that\ud83d\ude33. As I said earlier, UnitOfWork pattern is used for transaction management. That means, that we use it to commit the data if everything is OK, or rollback the changes if there are errors. When we use context managers( with uow ) with UnitOfWork pattern we make it so that if there are errors, all the pending changes are dropped. So, if you want to add try and except to your code, then just do it like this: from assimilator.core.database import UnitOfWork , InvalidQueryError from dependencies import user_repository , user_uow def create_user ( uow : UnitOfWork ): try : with uow : # We use that to start the transaction repository = uow . repository new_user = repository . save ( username = \"Andrey\" , balance = 1000 ) uow . commit () return new_user except InvalidQueryError : print ( \"Error in user creation\" ) return None # no user created created_user = create_user ( user_uow ) But, even if you do not use try and except, you are sure that your database does not have any weird changes in it! That's the power of UnitOfWork . If you ever want to rollback yourself, then use UnitOfWork.rollback() function. It will remove all the pending changes. That is the function that is called if there is an exception in the with uow block.","title":"Errors example\ud83d\ude30"},{"location":"tutorial/database/#other-functions-you-must-know","text":"Here are more short examples regarding Database functions that you might want to use:","title":"Other functions you must know"},{"location":"tutorial/database/#data-querying","text":"from assimilator.core.database import Repository def example ( repository : Repository ): repository . get () # get one entity from the database # get() function can raise NotFoundError() or MultipleResultsError() repository . filter () # get many entities from the database # When you use those functions, you can add specifications to limit the results: adult_users = repository . filter ( repository . specs . filter ( # we use filter specification age__gte = 18 , # get all the users older than 18 years ) ) for adult_user in adult_users : print ( adult_user . username ) There are different filtering options inside of filter() specification: __eq = equal to. You can omit it and just use field=value as we did before __gt = greater than. Example: age__gt=18 == (age > 18) __gte = greater than equals. Example: age__gte=18 == (age >= 18) __lt = lower than. Example: age__lt=18 == (age < 18) __lte = lower than equals. Example: age__lte=18 == (age <= 18) __not = not equal. Example: age__not=18 == (age != 18) __is = is True or False. Example: validated__is=True == (validated is True) __like = like SQL expression. Converted to regex if not supported. Example: username__like=\"Andrey%\" == all usernames that start with Andrey __regex = regular expression. Example: username__regex=\"[1-3]+And.rey\\w+\" == regular expression, what is there to explain? You can use these options like that: from assimilator.core.database import Repository def filter_example ( repository : Repository ): # Get all users between ages 18 to 25 with username # that has \"And\" inside and those who are validated. repository . filter ( repository . specs . filter ( age__gt = 18 , age__lt = 25 , username__like = \"%And%\" , validated__is = True , ) ) All the users can be queried with Repository.filter() without any specifications: from assimilator.core.database import Repository def get_all_users ( repository : Repository ): all_users = repository . filter () # get all users from the database Pagination is added with paginate() specification: from assimilator.core.database import Repository def paginate ( repository : Repository ): paginated_users = repository . filter ( repository . specs . paginate ( limit = 10 , # limit the results by 10 offset = 20 , # offset the results by 20 ), ) Ordering is added with order() specification: from assimilator.core.database import Repository def order ( repository : Repository ): ordered_users = repository . filter ( repository . specs . order ( 'username' , # order users by username(Ascending ordering) '-balance' , # second order of the users is balance(descending order) ) ) # - in front means descending Entity joins are added with join() specification: from assimilator.core.database import Repository def join_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . join ( Order . id , # another model order that is joined with user Product . product_id , # another model product that is joined with user ) ) If you want to optimize your queries, you can do so by using only() . It will accept fields that will be the only ones on your model: from assimilator.core.database import Repository def only_example ( repository : Repository ): users_with_products = repository . filter ( repository . specs . only ( 'id' , 'username' ) # We only query `id` and `username` from the database. # That reduces results size and query execution time ) If you want to count something, you can use count() : from assimilator.core.database import Repository def count_example ( repository : Repository ): users_count : int = repository . count () # Count all users other_users_count : int = repository . count ( repository . specs . filter ( id__gt = 10 ) # Count all users with id > 10 ) Sometimes you want to check if your object was updated or not. You can use is_modified() : is_modified : bool = repository . is_modified ( user )","title":"Data querying"},{"location":"tutorial/database/#lazy-evaluation","text":"Let's say that you want to load all the users from your table. But, the thing is, you don't need to use them straight away. Maybe, you want to return the result to another function, or set it as an attribute of an object. If you use the patterns that we gave you, then your code is going to be clean, but memory-heavy. To avoid that, you can prepare your function to be executed with another pattern called LazyCommand . It saves the function and all the arguments that you want to provide, and executes it only when you need it! You can add lazy=True to enable it in your Repository : from assimilator.core.database import Repository def example ( repository : Repository ): # Executes on the spot users_list = repository . filter () # Creates a lazy command that can be executed later users_filter_lazy_command = repository . filter ( lazy = True ) Now, we can optimize our program like this: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User , user_repository # We use typing for LazyCommand and show that it returns a list of Users def caller ( repository : Repository ) -> LazyCommand [ List [ User ]]: return repository . filter ( repository . specs . filter ( age__gt = 18 ), lazy = True , # make it lazy ) def second_function (): return caller ( user_repository ) # Database query not executed yet def first_function (): results = second_function () # Execute more code... for user in results : # The query is executed here ... We can optimize our code drastically. Now, we will only make the queries whenever we need them! But, if your query returns an error, that error is returned to the query execution, not creation! That is going to be first_function() in our case. Be sure to handle exceptions in the right place. Here are the places when your command is executed: from assimilator.core.database import Repository , LazyCommand def lazy_command_execution ( repository : Repository ): lazy_command : LazyCommand = repository . filter ( lazy = True ) if lazy_command : # query is executed in boolean statements print ( \"Executed!\" ) for data in lazy_command : # query is executed in iterators print ( \"Executed!\" ) lazy_user : LazyCommand = repository . get ( repository . specs . filter ( id = 1 )) print ( \"Executed for User id:\" , lazy_user . id ) # attribute access execution print ( lazy_user > 10 ) # Boolean execution Another important thing about LazyCommand is its execution policy. The thing is that if you use the same LazyCommand object many times, the command is only going to be executed once. This code only runs the query once: lazy_command_obj () # command executed lazy_command_obj () lazy_command_obj () lazy_command_obj () lazy_command_obj () # the same result in every other call Another FAR MORE IMPORTANT thing is the return type of your LazyCommand . If you call Repository.filter() , it's going to be an Iterable. If you use Repository.get() , it is just one entity. We suggest you add types with Python typings: from typing import List from assimilator.core.database import Repository , LazyCommand from dependencies import User def lazy_type_example ( repository : Repository ): lazy_command_many : LazyCommand [ List [ User ]] = repository . filter ( lazy = True ) lazy_command_obj : LazyCommand [ User ] = repository . get ( lazy = True ) # Now we know what is returned when the command is executed: users : List [ User ] = lazy_command_many () one_user : User = lazy_command_obj () The last thing is building your own LazyCommand objects: from assimilator.core.patterns import LazyCommand def func ( a , b , c ): return a + b + c command : LazyCommand [ int ] = LazyCommand ( command = func , # NOT func() a = 1 , b = 2 , c = 3 , # function arguments ) assert command == 1 + 2 + 3 Also, you can use the decorator to make your whole function lazy: from assimilator.core.patterns import LazyCommand @LazyCommand . decorate def decorated_lazy ( a , b , c ): return a + b + c print ( decorated_lazy ( 1 , 2 , 3 )) # 6 print ( decorated_lazy ( 1 , 2 , 3 , lazy = True )) # LazyCommand","title":"Lazy evaluation\ud83d\ude34"},{"location":"tutorial/database/#more-on-filter-specification","text":"You have probably wondered how to do OR statement in the filter specification. What about AND statement? How are we going to implement all these things without using direct coding? You can use special operations like these: # OR operation. username==\"Andrey\" or username==\"Python\": repository . specs . filter ( username = \"Andrey\" ) | repository . specs . filter ( username = \"Python\" ) # AND operation. username==\"Andrey\" and age==22: repository . specs . filter ( username = \"Andrey\" ) & repository . specs . filter ( age = 22 ) # AND operation, but shorter: repository . specs . fitler ( username = \"Andrey\" , age = 22 ) # NOT operation. age != 55 ~ repository . specs . filter ( age = 55 ) # Combining operations together. # (username=\"Andrey\" and age=22) or (username == \"Python\" and age > 18) repository . specs . filter ( username = \"Andrey\" , age = 22 ) | \\ repository . specs . filter ( username = \"Python\" ) & \\ ! repository . specs . filter ( age__gt = 18 ) Another question that you probably have is how to make all of that shorter. Writing the specification again and again can be tiring. Good thing you can save them(not only filter specification. Any specification in general): andrey_username_spec = repository . specs . filter ( username = \"Andrey\" ) andrey = repository . filter ( andrey_username_spec )","title":"More on filter specification"},{"location":"tutorial/database/#data-changes","text":"Let's finally change some data. Repository save() function can be used with arguments or provided model: repository . save ( # indirect method. username = \"Andrey\" , balance = 1000 , ) # OR user = User ( username = \"Andrey\" , balance = 1000 ) repository . save ( user ) # direct method. Repository update() function can be used to update models: user = repository . get ( repository . specs . limit ( limit = 1 )) user . balance += 100 repository . update ( user ) # update the user It can also be used to update a lot of entities at once: repository . update ( # you provide specifications to filter the results repository . specs . filter ( age__gt = 18 ), repository . specs . limit ( limit = 100 ), # then, you provide field=new_value pairs to update the fields is_validated = False , updated_field = \"New value\" , ) Use delete() to delete one model: repository . delete ( user ) Or many models at once: repository . delete ( # delete everyone under 18 repository . specs . filter ( age__lt = 18 ) ) delete() is partially safe. That means that you cannot delete your whole database, cause if you provide nothing, you will delete nothing. But, still check your specifications in mass delete statements. Use refresh() to update the values in your old object. It goes to the database and changes your old values to new if they were updated: repository . refresh ( old_user ) assert old_user . updated_field == repository . get ( repository . specs . filter ( id = old_user . id )) . updated_field","title":"Data changes"},{"location":"tutorial/database/#typical-flows","text":"","title":"Typical flows"},{"location":"tutorial/database/#data-creation","text":"1) You create Repository and UnitOfWork. from assimilator.alchemy.database import AlchemyRepository , AlchemyUnitOfWork user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) user_uow = AlchemyUnitOfWork ( repository = user_repository ) 2) You provide them in the function as parameters. from assimilator.core.database import UnitOfWork def create_user ( new_username : str , uow : UnitOfWork ): ... # UnitOfWork is a parameter 3) You use context manager(with statement in Python) with UnitOfWork. def create_user ( new_username : str , uow : UnitOfWork ): with uow : # Start the transaction in the database ... 4) You get the repository from UnitOfWork and use save() to save the result. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user 5) You use UnitOfWork commit() to apply the changes to the database. def create_user ( new_username : str , uow : UnitOfWork ): with uow : new_user = uow . repository . save ( username = new_username , user_balance = 0 , ) # create new user uow . commit () # Save changes do the database return new_user # return new user","title":"Data Creation"},{"location":"tutorial/database/#data-filtering","text":"1) You create Repository. from assimilator.alchemy.database import AlchemyRepository user_repository = AlchemyRepository ( session = DatabaseSession (), # your SQLAlchemy session model = User , # User is your SQLAlchemy model ) 2) You provide it in the function as a parameter. from assimilator.core.database import Repository def filter_users ( age : int , repository : Repository ): ... # Repository is a parameter 3) You use Repository filter() function to filter the results. def filter_users ( age : int , repository : Repository ): return repository . filter ( ... ) 4) You use repository.specs to access the specifications. Then, you choose filter to filter the users who are 18 or older: def filter_users ( age : int , repository : Repository ): return repository . filter ( repository . specs . filter ( age__gte = 18 ) # age >= 18 ) 5) Optional step You can use direct coding style to use the specification like this: from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( AlchemyFilter ( age__gte = 18 ) # age >= 18 ) 6) Optional step You use direct coding style with SQLAlchemy filter from assimilator.alchemy.database import AlchemyFilter def filter_users ( age : int , repository : Repository ): return repository . filter ( User . age >= 18 # Your SQLAlchemy User model. age >= 18 )","title":"Data Filtering"},{"location":"tutorial/events/","text":"Events patterns Events and how they work Event shows changes in your system and listeners(consumers) respond to them. Events contain all the possible things that other parts of the system may need once they respond to them. That is useful in lots of systems, and this page will describe the basics of assimilator events. Events use Pydantic module to ease the process of creation, integration and parsing. Event based systems with Assimilator Event - representation of a change in your system that carries all the useful data EventProducer - something that produces events(executes the changes in the system and shows it with events) EventConsumer - something that waits for the producer to emit various events for it to consume them and execute various operations based on the other changes EventBus - combines both producers and consumers in one Entity that can produce and consume simultaneously Event example with user registration: User sends his registration data to our website We create a new user in the database and emit an UserCreated event using an EventProducer EventConsumers listen to our UserCreated event and executes all the operations that must be done once the user is registered Event id: int Unique identification for the event. event_name: str Name of the event. We can have different events in our system. For example, if we have an event for User creation and an event for User deletion, then we can name them: User creation: event_name = user_created User deletion: event_name = user_deleted Those names can help us sort and only listen to specific kind of events. All the names must be in the past, since an event is the change in the past. event_date: datetime Date of the event. You don't need to change this field since it is assigned by default when an event is created. from_json() from_json() is a function that is used to convert json data to an event. That method is in the JSONParsedMixin class, and it allows us to quickly convert json to a Python object. cls: Type['BaseModel'] - Any Pydantic class, but typically an Event data: str - json data for our event Create a custom event events.py : from assimilator.core.events import Event class UserCreated ( Event ): user_id : int username : str email : str # all the data that could be useful is in the event. # Since Event is a Pydantic model, we can just create new fields like this logic.py : from assimilator.core.database import UnitOfWork from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) event = UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) In that example, we only create an event without publishing it anywhere. Find out how to emit your events below. EventConsumer EventConsumer reads all the incoming events and yields them to the functions that use it. start() Starts the event consumer by connecting to all the required systems close() Closes the consumer and finishes the work consume() Yields incoming events EventConsumer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventConsumer : events_bus.py : from assimilator.core.events import EventConsumer , ExternalEvent class MyEventConsumer ( EventConsumer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def consume ( self ): while True : message = self . api . listen () # we receive a message from the API yield ExternalEvent ( ** message . convert_to_json ()) # parse it logic.py : from events_bus import MyEventConsumer def consume_events ( consumer : MyEventConsumer ): with consumer : for event in events_bus . consume (): if event . event_name == \"user_created\" : user_created_handler ( UserCreated ( ** event . data )) elif event . event_name == \"user_deleted\" : user_deleted_handler ( UserDeleted ( ** event . data )) We create a new EventConsumer called MyEventConsumer . Then, we use an api object to implement all the functions. After that, we use it in logic.py file where we consume all the events and handle them depending on the event_name . As you have already noticed, we use something called an ExternalEvent . We do that because all the events that are coming from external sources are unidentified and can only use specific later. ExternalEvent contains all the event data in the data: dict field which can be used later. ExternalEvent When we listen to external systems, it is sometimes hard to make an event class that represents a specific class. That is why we use an ExternalEvent . It contains all the data in the data: dict field, which can be accessed later in order to use an event class that represents that specific event. data: dict - all the data for the event AckEvent AckEvent is an event that has acknowledgement in it. If you want to show that your event was processed(acknowledged), then use AckEvent . ack: bool - whether an event was processed. False by default EventProducer EventProducer is the class that produces all the events and sends them. start() Starts the event producer by connecting to all the required systems. close() Closes the producer and finishes the work. produce() Sends an event to an external system for it to be consumed. event: Event - the event that must be sent. EventProducer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventProducer : events_bus.py : from assimilator.core.events import EventProducer class MyEventProducer ( EventProducer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def produce ( self , event : Event ) -> None : self . api . send_event ( event . json ()) # parse event to json and send it logic.py : from events_bus import MyEventProducer from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork , producer : MyEventProducer , ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) with producer : producer . produce ( UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) ) # send an event to an external system ExternalEvent must not be used in the producer, since when we emit the events we are the ones creating them, so we have a separate class for them with all the data inside. EventBus EventBus combines both EventProducer and EventConsumer together. You can use those classes separately, but sometimes you need one object that combines them. __init__() consumer: EventConsumer - the consumer that we want to use producer: EventProducer - the producer that we want to use produce() produces the event using producer event: Event - an event that has to be emitted consume() consumes the events using consumer . Returns an Iterator[Event] Event fails and transaction management Sometimes we want to be sure that our events are emitted. But, if we use normal event producers and Unit Of Work separately, we may run into a problem: 1) User is created(added in the database and unit of work committed it) 2) Event producer encounters an error(the event is not published) 3) Inconsistency: User exists, but consumers do not know about that Because of that, we may employ Outbox Relay. It is a pattern that allows us to save all the events in the database in the same transaction as the main entity. Then, another program(thread, task, function) gets all the events from the database and ensures that they are published. We basically save the events to the database in one transaction, emit them in a separate thing and delete them afterwards. OutboxRelay This class gets all the events using UnitOfWork provided, emits all events, and acknowledges them. __init__() uow: UnitOfWork - unit of work that is used in order to get the events, acknowledge them producer: EventProducer - event producer that we use to publish the events start() Start the relay. This function must run forever, must get the events from the repository from unit of work, and produce the events. After that, it must call acknowledge() to show that these events are produced. acknowledge() Acknowledges the events in the database. It might change a boolean column for these events, might delete them, but the idea is that those events will not be produced twice. events: Iterable[Event] - events that must be acknowledged","title":"Events tutorial"},{"location":"tutorial/events/#events-patterns","text":"","title":"Events patterns"},{"location":"tutorial/events/#events-and-how-they-work","text":"Event shows changes in your system and listeners(consumers) respond to them. Events contain all the possible things that other parts of the system may need once they respond to them. That is useful in lots of systems, and this page will describe the basics of assimilator events. Events use Pydantic module to ease the process of creation, integration and parsing.","title":"Events and how they work"},{"location":"tutorial/events/#event-based-systems-with-assimilator","text":"Event - representation of a change in your system that carries all the useful data EventProducer - something that produces events(executes the changes in the system and shows it with events) EventConsumer - something that waits for the producer to emit various events for it to consume them and execute various operations based on the other changes EventBus - combines both producers and consumers in one Entity that can produce and consume simultaneously","title":"Event based systems with Assimilator"},{"location":"tutorial/events/#event-example-with-user-registration","text":"User sends his registration data to our website We create a new user in the database and emit an UserCreated event using an EventProducer EventConsumers listen to our UserCreated event and executes all the operations that must be done once the user is registered","title":"Event example with user registration:"},{"location":"tutorial/events/#event","text":"","title":"Event"},{"location":"tutorial/events/#id-int","text":"Unique identification for the event.","title":"id: int"},{"location":"tutorial/events/#event_name-str","text":"Name of the event. We can have different events in our system. For example, if we have an event for User creation and an event for User deletion, then we can name them: User creation: event_name = user_created User deletion: event_name = user_deleted Those names can help us sort and only listen to specific kind of events. All the names must be in the past, since an event is the change in the past.","title":"event_name: str"},{"location":"tutorial/events/#event_date-datetime","text":"Date of the event. You don't need to change this field since it is assigned by default when an event is created.","title":"event_date: datetime"},{"location":"tutorial/events/#from_json","text":"from_json() is a function that is used to convert json data to an event. That method is in the JSONParsedMixin class, and it allows us to quickly convert json to a Python object. cls: Type['BaseModel'] - Any Pydantic class, but typically an Event data: str - json data for our event","title":"from_json()"},{"location":"tutorial/events/#create-a-custom-event","text":"events.py : from assimilator.core.events import Event class UserCreated ( Event ): user_id : int username : str email : str # all the data that could be useful is in the event. # Since Event is a Pydantic model, we can just create new fields like this logic.py : from assimilator.core.database import UnitOfWork from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) event = UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) In that example, we only create an event without publishing it anywhere. Find out how to emit your events below.","title":"Create a custom event"},{"location":"tutorial/events/#eventconsumer","text":"EventConsumer reads all the incoming events and yields them to the functions that use it.","title":"EventConsumer"},{"location":"tutorial/events/#start","text":"Starts the event consumer by connecting to all the required systems","title":"start()"},{"location":"tutorial/events/#close","text":"Closes the consumer and finishes the work","title":"close()"},{"location":"tutorial/events/#consume","text":"Yields incoming events EventConsumer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventConsumer : events_bus.py : from assimilator.core.events import EventConsumer , ExternalEvent class MyEventConsumer ( EventConsumer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def consume ( self ): while True : message = self . api . listen () # we receive a message from the API yield ExternalEvent ( ** message . convert_to_json ()) # parse it logic.py : from events_bus import MyEventConsumer def consume_events ( consumer : MyEventConsumer ): with consumer : for event in events_bus . consume (): if event . event_name == \"user_created\" : user_created_handler ( UserCreated ( ** event . data )) elif event . event_name == \"user_deleted\" : user_deleted_handler ( UserDeleted ( ** event . data )) We create a new EventConsumer called MyEventConsumer . Then, we use an api object to implement all the functions. After that, we use it in logic.py file where we consume all the events and handle them depending on the event_name . As you have already noticed, we use something called an ExternalEvent . We do that because all the events that are coming from external sources are unidentified and can only use specific later. ExternalEvent contains all the event data in the data: dict field which can be used later.","title":"consume()"},{"location":"tutorial/events/#externalevent","text":"When we listen to external systems, it is sometimes hard to make an event class that represents a specific class. That is why we use an ExternalEvent . It contains all the data in the data: dict field, which can be accessed later in order to use an event class that represents that specific event. data: dict - all the data for the event","title":"ExternalEvent"},{"location":"tutorial/events/#ackevent","text":"AckEvent is an event that has acknowledgement in it. If you want to show that your event was processed(acknowledged), then use AckEvent . ack: bool - whether an event was processed. False by default","title":"AckEvent"},{"location":"tutorial/events/#eventproducer","text":"EventProducer is the class that produces all the events and sends them.","title":"EventProducer"},{"location":"tutorial/events/#start_1","text":"Starts the event producer by connecting to all the required systems.","title":"start()"},{"location":"tutorial/events/#close_1","text":"Closes the producer and finishes the work.","title":"close()"},{"location":"tutorial/events/#produce","text":"Sends an event to an external system for it to be consumed. event: Event - the event that must be sent. EventProducer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventProducer : events_bus.py : from assimilator.core.events import EventProducer class MyEventProducer ( EventProducer ): def __init__ ( self , api ): # some object that connects to an external system self . api = api def start ( self ) -> None : self . api . connect () def close ( self ) -> None : self . api . disconnect () def produce ( self , event : Event ) -> None : self . api . send_event ( event . json ()) # parse event to json and send it logic.py : from events_bus import MyEventProducer from events import UserCreated from models import User def create_user ( username : str , email : str , uow : UnitOfWork , producer : MyEventProducer , ): with uow : user = User ( username = username , email = email ) uow . repository . save ( user ) uow . commit () # Refresh the object and get the user id from the database uow . repository . refresh ( user ) with producer : producer . produce ( UserCreated ( # we create an event user_id = user . id , username = user . username , email = user . email , ) ) # send an event to an external system ExternalEvent must not be used in the producer, since when we emit the events we are the ones creating them, so we have a separate class for them with all the data inside.","title":"produce()"},{"location":"tutorial/events/#eventbus","text":"EventBus combines both EventProducer and EventConsumer together. You can use those classes separately, but sometimes you need one object that combines them.","title":"EventBus"},{"location":"tutorial/events/#__init__","text":"consumer: EventConsumer - the consumer that we want to use producer: EventProducer - the producer that we want to use","title":"__init__()"},{"location":"tutorial/events/#produce_1","text":"produces the event using producer event: Event - an event that has to be emitted","title":"produce()"},{"location":"tutorial/events/#consume_1","text":"consumes the events using consumer . Returns an Iterator[Event]","title":"consume()"},{"location":"tutorial/events/#event-fails-and-transaction-management","text":"Sometimes we want to be sure that our events are emitted. But, if we use normal event producers and Unit Of Work separately, we may run into a problem: 1) User is created(added in the database and unit of work committed it) 2) Event producer encounters an error(the event is not published) 3) Inconsistency: User exists, but consumers do not know about that Because of that, we may employ Outbox Relay. It is a pattern that allows us to save all the events in the database in the same transaction as the main entity. Then, another program(thread, task, function) gets all the events from the database and ensures that they are published. We basically save the events to the database in one transaction, emit them in a separate thing and delete them afterwards.","title":"Event fails and transaction management"},{"location":"tutorial/events/#outboxrelay","text":"This class gets all the events using UnitOfWork provided, emits all events, and acknowledges them.","title":"OutboxRelay"},{"location":"tutorial/events/#__init___1","text":"uow: UnitOfWork - unit of work that is used in order to get the events, acknowledge them producer: EventProducer - event producer that we use to publish the events","title":"__init__()"},{"location":"tutorial/events/#start_2","text":"Start the relay. This function must run forever, must get the events from the repository from unit of work, and produce the events. After that, it must call acknowledge() to show that these events are produced.","title":"start()"},{"location":"tutorial/events/#acknowledge","text":"Acknowledges the events in the database. It might change a boolean column for these events, might delete them, but the idea is that those events will not be produced twice. events: Iterable[Event] - events that must be acknowledged","title":"acknowledge()"},{"location":"tutorial/important/","text":"Important things related to all patterns What is a pattern??? Pattern - typical solution to commonly occurring problems. In our case, the problems are: Database communication Data integrity Event-based systems Coding speed Good code External dependencies The best code that some people cannot write easily We solve all of them with different classes like: Repository , UnitOfWork , Producer , and so on. Each class is a pattern. Lots of them are used with each other: Repository is always used with UnitOfWork and Specification . Indirect vs Direct code When you write your code, you can choose two styles: direct and indirect. What does that mean? We use different libraries like SQLAlchemy, PyRedis, PyMongo and others to ease the use of our patterns. We did not want to create a module that allows you to completely remove these modules from your code. But, we made it so our patterns are interchangeable. That means that you can write some code for SQLAlchemy, and change it to Redis 2 minutes later, even if you coded 20 000 lines. Indirect coding style You do not import any functions from assimilator, every useful thing is directly in the pattern. You do not use anything from external providers(except for pattern creation) in your code. You only use our patterns. Indirect coding example: def create_user ( uow : UnitOfWork ): with uow : uow . repository . save ( username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( balance__gt = 20 )) # only using arguments # Patterns Configuration # External library(SQLAlchemy) is only found in the pattern creation repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository ) Direct coding style You import functions and objects from assimilator. You use things from external libraries in your code with assimilator patterns Direct coding example: def create_user ( uow : UnitOfWork ): with uow : new_user = User ( # SQLAlchemy model is used directly username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) uow . repository . save ( new_user ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( User . balance > 20 ), # SQLAlchemy filter user # AlchemyFilter(User.balance > 20), # AlchemyFilter is imported from assimilator, direct use ) # repository.specs.filter == AlchemyFilter for AlchemyRepository, but you either use it directly or indirectly # Patterns Configuration. Everything is the same repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository ) Why do you need all that? Indirect style pluses \u2714\ufe0f You won't have any external dependencies. For example, you don't want to use SQLAlchemy directly. You can change data storages by only changing the configuration: def create_user ( uow : UnitOfWork ): \"\"\" Stays the same using indirect coding \"\"\" def filter_users ( repository : Repository ): \"\"\" Stays the same using indirect coding \"\"\" # Patterns Configuration # You can change pattern creation and move to another data storage without any issues. repository = RedisRepository ( Redis (), model = RedisUser ) ####### LOOK HERE uow = RedisUnitOfWork ( repository ) Indirect minuses \u274c Indirect coding is a little slower than the direct one. It may not include all the features that your app needs. For example, what if you need to run a MongoDB pipeline with aggregation framework\ud83d\ude35(even though you can do this specific thing with indirect coding). Direct style pluses \u2714\ufe0f Your app is very complex, and you don't have all the features in indirect variant. You are 100% sure that you will not change your code to other external libraries with Assimilator patterns. A little faster since we do not parse anything, we just use external objects and methods. Direct minuses \u274c Very hard to move to other data storages or libraries since you are using external features directly. External dependencies in your code. How to choose? We prefer to use indirect style, since it hides dependencies. But, what you need to do is adapt to your project. Start with indirect style and use direct features only when needed.","title":"Important things"},{"location":"tutorial/important/#important-things-related-to-all-patterns","text":"","title":"Important things related to all patterns"},{"location":"tutorial/important/#what-is-a-pattern","text":"Pattern - typical solution to commonly occurring problems. In our case, the problems are: Database communication Data integrity Event-based systems Coding speed Good code External dependencies The best code that some people cannot write easily We solve all of them with different classes like: Repository , UnitOfWork , Producer , and so on. Each class is a pattern. Lots of them are used with each other: Repository is always used with UnitOfWork and Specification .","title":"What is a pattern???"},{"location":"tutorial/important/#indirect-vs-direct-code","text":"When you write your code, you can choose two styles: direct and indirect. What does that mean? We use different libraries like SQLAlchemy, PyRedis, PyMongo and others to ease the use of our patterns. We did not want to create a module that allows you to completely remove these modules from your code. But, we made it so our patterns are interchangeable. That means that you can write some code for SQLAlchemy, and change it to Redis 2 minutes later, even if you coded 20 000 lines.","title":"Indirect vs Direct code"},{"location":"tutorial/important/#indirect-coding-style","text":"You do not import any functions from assimilator, every useful thing is directly in the pattern. You do not use anything from external providers(except for pattern creation) in your code. You only use our patterns. Indirect coding example: def create_user ( uow : UnitOfWork ): with uow : uow . repository . save ( username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( balance__gt = 20 )) # only using arguments # Patterns Configuration # External library(SQLAlchemy) is only found in the pattern creation repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository )","title":"Indirect coding style"},{"location":"tutorial/important/#direct-coding-style","text":"You import functions and objects from assimilator. You use things from external libraries in your code with assimilator patterns Direct coding example: def create_user ( uow : UnitOfWork ): with uow : new_user = User ( # SQLAlchemy model is used directly username = \"Andrey\" , # No external library usage email = \"python.on.papyrus@gmail.com\" , ) uow . repository . save ( new_user ) def filter_users ( repository : Repository ): return repository . filter ( repository . specs . filter ( User . balance > 20 ), # SQLAlchemy filter user # AlchemyFilter(User.balance > 20), # AlchemyFilter is imported from assimilator, direct use ) # repository.specs.filter == AlchemyFilter for AlchemyRepository, but you either use it directly or indirectly # Patterns Configuration. Everything is the same repository = AlchemyRepository ( Session (), model = User ) uow = AlchemyUnitOfWork ( repository )","title":"Direct coding style"},{"location":"tutorial/important/#why-do-you-need-all-that","text":"","title":"Why do you need all that?"},{"location":"tutorial/important/#indirect-style-pluses","text":"You won't have any external dependencies. For example, you don't want to use SQLAlchemy directly. You can change data storages by only changing the configuration: def create_user ( uow : UnitOfWork ): \"\"\" Stays the same using indirect coding \"\"\" def filter_users ( repository : Repository ): \"\"\" Stays the same using indirect coding \"\"\" # Patterns Configuration # You can change pattern creation and move to another data storage without any issues. repository = RedisRepository ( Redis (), model = RedisUser ) ####### LOOK HERE uow = RedisUnitOfWork ( repository )","title":"Indirect style pluses \u2714\ufe0f"},{"location":"tutorial/important/#indirect-minuses","text":"Indirect coding is a little slower than the direct one. It may not include all the features that your app needs. For example, what if you need to run a MongoDB pipeline with aggregation framework\ud83d\ude35(even though you can do this specific thing with indirect coding).","title":"Indirect minuses \u274c"},{"location":"tutorial/important/#direct-style-pluses","text":"Your app is very complex, and you don't have all the features in indirect variant. You are 100% sure that you will not change your code to other external libraries with Assimilator patterns. A little faster since we do not parse anything, we just use external objects and methods.","title":"Direct style pluses \u2714\ufe0f"},{"location":"tutorial/important/#direct-minuses","text":"Very hard to move to other data storages or libraries since you are using external features directly. External dependencies in your code.","title":"Direct minuses \u274c"},{"location":"tutorial/important/#how-to-choose","text":"We prefer to use indirect style, since it hides dependencies. But, what you need to do is adapt to your project. Start with indirect style and use direct features only when needed.","title":"How to choose?"}]}