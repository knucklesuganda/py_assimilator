{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Assimilator - the best Python patterns for the best projects Install now pip install py_assimilator Source Github PyPI Documentation Github Author's YouTube RU Author's YouTube ENG About patterns in coding They are useful, but only to some extent. Most of them are not suitable for real life applications. DDD(Domain-driven design) is one of the most popular ways of development today, but nobody explains how to write most of DDD patterns in Python. Even if they do, life gives you another issue that cannot be solved with a simple algorithm. That is why Andrey created a library for the patterns that he uses in his projects daily. Types of patterns These are different use cases for the patterns implemented. Database - patterns for database/data layer interactions Events - projects with events or event-driven architecture Available providers Providers are different patterns for external modules like SQLAlchemy or FastAPI. Alchemy(Database, Events) - patterns for SQLAlchemy for both database and events Kafka(Events) - patterns in Kafka related to events Internal(Database, Events) - internal is the type of provider that saves everything in memory(dict, list and all the tools within your app) Redis(Database, Events) - redis allows us to work with Redis memory database","title":"Introduction"},{"location":"#assimilator-the-best-python-patterns-for-the-best-projects","text":"","title":"Assimilator - the best Python patterns for the best projects"},{"location":"#install-now","text":"pip install py_assimilator","title":"Install now"},{"location":"#source","text":"Github PyPI Documentation Github Author's YouTube RU Author's YouTube ENG","title":"Source"},{"location":"#about-patterns-in-coding","text":"They are useful, but only to some extent. Most of them are not suitable for real life applications. DDD(Domain-driven design) is one of the most popular ways of development today, but nobody explains how to write most of DDD patterns in Python. Even if they do, life gives you another issue that cannot be solved with a simple algorithm. That is why Andrey created a library for the patterns that he uses in his projects daily.","title":"About patterns in coding"},{"location":"#types-of-patterns","text":"These are different use cases for the patterns implemented. Database - patterns for database/data layer interactions Events - projects with events or event-driven architecture","title":"Types of patterns"},{"location":"#available-providers","text":"Providers are different patterns for external modules like SQLAlchemy or FastAPI. Alchemy(Database, Events) - patterns for SQLAlchemy for both database and events Kafka(Events) - patterns in Kafka related to events Internal(Database, Events) - internal is the type of provider that saves everything in memory(dict, list and all the tools within your app) Redis(Database, Events) - redis allows us to work with Redis memory database","title":"Available providers"},{"location":"concepts/","text":"How do we build these patterns 1) Dependency injection Dependency injection is a really important concept in assimilator. We do not use any additional dependency injection frameworks, but all the patterns inject different components into themselves. If you want to know more about DI 1) SOLID SOLID principles are highly used in assimilator. That means, that in theory you can replace one pattern to another and experience no trouble in using them. That is why it is not advised to create your own function in patterns, but you can easily override them. For example, you don't want to create: createUsers() in Repository pattern, but can override save() function without any problems . With that said, it is almost impossible to write such vast variety of patterns without breaking some of the principles. But, if you have any ideas on how to fix that, then be sure to check out our Github 1) Domain-driven design Most of the patterns here are used in Domain driven design. You do not really need to know all the intricacies, but make sure that you know the basics of it.","title":"Concepts"},{"location":"concepts/#how-do-we-build-these-patterns","text":"","title":"How do we build these patterns"},{"location":"concepts/#1-dependency-injection","text":"Dependency injection is a really important concept in assimilator. We do not use any additional dependency injection frameworks, but all the patterns inject different components into themselves. If you want to know more about DI","title":"1) Dependency injection"},{"location":"concepts/#1-solid","text":"SOLID principles are highly used in assimilator. That means, that in theory you can replace one pattern to another and experience no trouble in using them. That is why it is not advised to create your own function in patterns, but you can easily override them. For example, you don't want to create: createUsers() in Repository pattern, but can override save() function without any problems . With that said, it is almost impossible to write such vast variety of patterns without breaking some of the principles. But, if you have any ideas on how to fix that, then be sure to check out our Github","title":"1) SOLID"},{"location":"concepts/#1-domain-driven-design","text":"Most of the patterns here are used in Domain driven design. You do not really need to know all the intricacies, but make sure that you know the basics of it.","title":"1) Domain-driven design"},{"location":"alchemy/database/","text":"SQLAlchemy Database patterns a","title":"Database"},{"location":"alchemy/database/#sqlalchemy-database-patterns","text":"","title":"SQLAlchemy Database patterns"},{"location":"alchemy/database/#a","text":"","title":"a"},{"location":"patterns/database/","text":"Database patterns Repository Repository is the pattern that makes a virtual collection out of the database. When we use a database we often have some kind of library, language or protocol. If we want to make the database abstract, we use the repository pattern. It has basic functions that help us change and query our data from any source. The beauty of the pattern is that you can use it with SQL, text files, cache, S3, external API's or any kind of data storage. __init__() session - each repository has a session that works as the primary data source. It can be your database connection, a text file or a data structure. initial_query - the initial query that you use in the data storage. We will show how it works later. It can be an SQL query, a key in the dictionary or anything else. specifications: SpecificationList - an object that contains links to the specifications that are used to create your queries _get_initial_query() returns the initial query used in the _apply_specifications() _apply_specifications() Applies Specifications to the query. Must not be used directly. apply specifications gets a list of specifications and applies them to the query returned in _get_initial_query(). The idea is the following: each specification gets a query and adds some filters to it. At the end we get a fully working query modified with the specifications provided by the user. - specifications: Iterable[Specifications] - an iterable of specifications that can be used to specify some conditions in the query Specification is a pattern that adds filters or anything else that specifies what kind of data we want. get() get is the function used to query the data storage and return one entity. You supply a list of specifications that get you the entity from the storage. - specifications: Specifications - specifications that can be used to specify some conditions in the query - lazy: bool - whether you want to execute your query straight away or just build it for the future - initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument filter() filters is the function used to query the data storage and return many entities. You supply a list of specifications that filter entities in the storage. - specifications: Specifications - specifications that can be used to specify some conditions in the query - lazy: bool - whether you want to execute your query straight away or just build it for the future - initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument save() Adds the objects to the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just add them ). delete() Deletes the objects from the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just delete them from your session ). update() Updates the objects in the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just update them in your session ). is_modified() Checks whether an obj was modified or not. If any value changes within the object, then it must return True refresh() Updates the object values with the values in the data storage. That can be useful if you want to create an object and get its id that was generated in the storage, or if you just want to have the latest saved version of the object. count() Counts the objects while applying specifications to the query. Give no specifications to count the whole data storage. - specifications: Specifications - specifications that can be used to specify some conditions in the query - lazy: bool - whether you want to execute your query straight away or just build it for the future Creating your own repository: If you want to create your own repository, then you are going to have to override all the functions above. But, please, do not make new functions available to the outer world. You can do this: from assimilator.core.database import BaseRepository class UserRepository(BaseRepository): def _users_private_func(self): # Cannot be called outside return 'Do something' And call that function inside of your repository. But, never do this: from assimilator.core.database import BaseRepository class UserRepository(BaseRepository): def get_ser_by_id(self): # Cannot be called outside return self.get(filter_specification(id=1)) Since it is going to be really hard for you to replace one repository to another. Example: from assimilator.core.database import BaseRepository from users.repository import UserRepository from products.repository import ProductRepository def get_by_id(id, repository: BaseRepository): return repository.get(filter_specification(id=1)) get_by_id(UserRepository()) get_by_id(ProductRepository()) # You can call the function with both repositories, and it will probably work fine How to create my repositories? You want to create a repository for entities in your projects. That means, that if you have some auxiliary table in your app, then it probably should not have a repository. But, things like Users, Products, Orders and others might have their own repository. That does not mean that you will create a lot of classes, but please do not add repositories for every class in your system . If you want to read more, please, look into the Domain-Driven Development books. Specification Specification is the pattern that adds values, filters, joins or anything else to the query in your repository. It can also work as a filter for your objects. Class-based specifications class Specification(ABC): @abstractmethod def apply(self, query): raise NotImplementedError(\"Specification must specify apply()\") def __call__(self, query): return self.apply(query) apply(query) -> query apply is the main functions that you are going to override. It is used in order to specify new things in your query. You get the query in the specification and return an updated version of query. For example, if your query has a filter function, and you want to filter by username, then you can create this: from assimilator.core.database import Specification class UsernameSpecification(Specification): def __init__(self, username: str): super(UsernameSpecification, self).__init__() self.username = username def apply(self, query): return query.filter(username=username) Here we do the following: 1) save the username in the constructor 2) override apply() and return an updated version of the query provided The usage of this specification will look like this: repository = UserRepository(session) user = repository.get(UsernameSpecification(username=\"python.on.papyrus\")) The repository will apply the specification and return the results. Functional specifications __call__() functions receives the query and call apply() in the specification. This is used in order to make functional specifications possible. __call__() allows you to call an object as a function: obj() . This way, we can use functional specifications and class-based specifications together. If you want to create a functional specification, then you need to use the @specification decorator: from assimilator.core.database import specification @specification def username_filter(query, username: str): return query.filter(username=username) The function above is the equivalent of the UsernameSpecification class. Then, you are going to use it like this: repository = UserRepository(session) user = repository.get(username_filter(username=\"python.on.papyrus\")) Both types work the same, so you can choose the type of specifications that you like. But, you can also use them together: repository = UserRepository(session) user = repository.get( username_filter(username=\"python.on.papyrus\"), AgeGreaterSpecification(age_gt=18), JoinSpecification(Friends), ) SpecificationList SpecificationList is a static class that contains basic specifications for our repository. Specifications: filter() filters the data order() orders the data paginate() paginates the data(limits the results, offsets them). join() joins entities together(join a table, get related data). The reason we use SpecificationList is because we want to have an abstraction for our specifications. Take two examples: from dependencies import UserRepository from assimilator.alchemy.database import alchemy_filter def get_user(id: int, repository: UserRepository): return repository.get(alchemy_filter(id=id)) # we use alchemy_filter() directly In that example, we use alchemy_filter() directly, which may not seem as an issue, however, if we would want to change our UserRepository to work with RedisRepository , then we would have to change all of our specifications ourselves. In order to fix this, we can use SpecificationList: from dependencies import UserRepository def get_user(id: int, repository: UserRepository): return repository.get(repository.specifications.filter(id=id)) # we call the filter from repository specifications. Now, we only have to change the repository without worrying about other parts of the code. Here is how you can create your own SpecificationList: from assimilator.core.database import SpecificationList, specification, Specification from pagination_func import paginate_data @specification def filter_specification(filters, query): return query.do_filter(filters) @specification def join_specification(query): return query # we do not need joins in our data structure, so we leave it class OrderSpecification(Specification): def __init__(self, orders): self.orders = orders def apply(self, query): return query.make_order(self.orders) class MySpecificationList(SpecificationList): filter = filter_specification # we use function name as the specification order = OrderSpecification # lambda specification paginate = paginate_data # imported specification join = join_specification Notice that we never call the functions, cause the only thing we need are links to the specifications. Then, when you build your Repository: from specifications import MySpecificationList repository = MyRepository(session=session, specifications=MySpecificationList) Once you have done that, the repository will use your specifications. Of course, you can still use specifications directly, but if you ever need to change the repository, then it may be a problem. LazyCommand Sometimes we don't want to execute the query right away. For example, for optimization purposes or some other purpose that requires us to delay the execution. In that case, you want to find lazy argument in the function that you are calling and set it to True . After that, a LazyCommand is going to be returned. That object allows you to call it as a function or iterate over it to get the results: from assimilator.core.database import BaseRepository def print_all_usernames(repository: BaseRepository): for user in repository.filter(lazy=True): print(user.username) # we don't want to receive a list of all the users, but want to iterate # through it and only get 1 user at a time def count_users_if_argument_true(do_count, repository: BaseRepository): count_command = repository.count(lazy=True) # turn on lazy and get LazyCommand if do_count: return count_command() # call for the result return -1 Unit of Work Unit of work allows us to work with transactions and repositories that change the data. The problem with repository is the transaction management. We want to make our transaction management as easy as possible, but repositories are not responsible for that. That is why we have units of work. They allow us to do the following: 1. Start the transaction 2. Provide the repository to the client code 3. If there are exceptions, errors, issues with the client code, rollback the transaction and remove all the changes 4. If everything is good, the client code calls the commit() function and finishes the data change 5. Unit of work closes the transaction __init__() repository: BaseRepository - The repository is provided in the UnitOfWork when we create it. The session to the data storage is going to be taken from the repository. begin() Starts the transaction. The function is called automatically. rollback() Removes all the changes from the transaction. You do not need to call that function, as it is called automatically if there is an error in your code. commit() Saves the changes to the data storage. While the repository only adds the temporary, this function is responsible for the final save. You need to call it yourself, it will not be called automatically like rollback() close() Closes the transaction. The function is called automatically. Here is how you can use UnitOfWork in your code: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user(username: str, uow: UnitOfWork): with uow: # start the transaction # everything in here is within the transaction new_user = User(username=username) uow.repository.save(new_user) # we get the repository from UnitOfWork uow.commit() # commit the changes making them final. If the function is not called, nothing is saved. As you can see, you do not need to call any function except for commit() . You should also use context managers( with uow: ) to start the transaction and rollback if there is an exception: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user(username: str, uow: UnitOfWork): with uow: # start the transaction # everything in here is within the transaction new_user = User(username=username) uow.repository.save(new_user) # we get the repository from UnitOfWork 1 / 0 # ZeroDivisionError. UnitOfWork calls rollback automatically. uow.commit() # nothing is saved, since the rollback was called.","title":"Database"},{"location":"patterns/database/#database-patterns","text":"","title":"Database patterns"},{"location":"patterns/database/#repository","text":"Repository is the pattern that makes a virtual collection out of the database. When we use a database we often have some kind of library, language or protocol. If we want to make the database abstract, we use the repository pattern. It has basic functions that help us change and query our data from any source. The beauty of the pattern is that you can use it with SQL, text files, cache, S3, external API's or any kind of data storage.","title":"Repository"},{"location":"patterns/database/#__init__","text":"session - each repository has a session that works as the primary data source. It can be your database connection, a text file or a data structure. initial_query - the initial query that you use in the data storage. We will show how it works later. It can be an SQL query, a key in the dictionary or anything else. specifications: SpecificationList - an object that contains links to the specifications that are used to create your queries","title":"__init__()"},{"location":"patterns/database/#_get_initial_query","text":"returns the initial query used in the _apply_specifications()","title":"_get_initial_query()"},{"location":"patterns/database/#_apply_specifications","text":"Applies Specifications to the query. Must not be used directly. apply specifications gets a list of specifications and applies them to the query returned in _get_initial_query(). The idea is the following: each specification gets a query and adds some filters to it. At the end we get a fully working query modified with the specifications provided by the user. - specifications: Iterable[Specifications] - an iterable of specifications that can be used to specify some conditions in the query Specification is a pattern that adds filters or anything else that specifies what kind of data we want.","title":"_apply_specifications()"},{"location":"patterns/database/#get","text":"get is the function used to query the data storage and return one entity. You supply a list of specifications that get you the entity from the storage. - specifications: Specifications - specifications that can be used to specify some conditions in the query - lazy: bool - whether you want to execute your query straight away or just build it for the future - initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument","title":"get()"},{"location":"patterns/database/#filter","text":"filters is the function used to query the data storage and return many entities. You supply a list of specifications that filter entities in the storage. - specifications: Specifications - specifications that can be used to specify some conditions in the query - lazy: bool - whether you want to execute your query straight away or just build it for the future - initial_query = None - if you want to change the initial query for this query only, then you can provide it as an argument","title":"filter()"},{"location":"patterns/database/#save","text":"Adds the objects to the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just add them ).","title":"save()"},{"location":"patterns/database/#delete","text":"Deletes the objects from the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just delete them from your session ).","title":"delete()"},{"location":"patterns/database/#update","text":"Updates the objects in the session, so you can commit it latter. This method should not change the final state of the storage, we have UnitOfWork for this( do not commit your changes, just update them in your session ).","title":"update()"},{"location":"patterns/database/#is_modified","text":"Checks whether an obj was modified or not. If any value changes within the object, then it must return True","title":"is_modified()"},{"location":"patterns/database/#refresh","text":"Updates the object values with the values in the data storage. That can be useful if you want to create an object and get its id that was generated in the storage, or if you just want to have the latest saved version of the object.","title":"refresh()"},{"location":"patterns/database/#count","text":"Counts the objects while applying specifications to the query. Give no specifications to count the whole data storage. - specifications: Specifications - specifications that can be used to specify some conditions in the query - lazy: bool - whether you want to execute your query straight away or just build it for the future","title":"count()"},{"location":"patterns/database/#creating-your-own-repository","text":"If you want to create your own repository, then you are going to have to override all the functions above. But, please, do not make new functions available to the outer world. You can do this: from assimilator.core.database import BaseRepository class UserRepository(BaseRepository): def _users_private_func(self): # Cannot be called outside return 'Do something' And call that function inside of your repository. But, never do this: from assimilator.core.database import BaseRepository class UserRepository(BaseRepository): def get_ser_by_id(self): # Cannot be called outside return self.get(filter_specification(id=1)) Since it is going to be really hard for you to replace one repository to another. Example: from assimilator.core.database import BaseRepository from users.repository import UserRepository from products.repository import ProductRepository def get_by_id(id, repository: BaseRepository): return repository.get(filter_specification(id=1)) get_by_id(UserRepository()) get_by_id(ProductRepository()) # You can call the function with both repositories, and it will probably work fine","title":"Creating your own repository:"},{"location":"patterns/database/#how-to-create-my-repositories","text":"You want to create a repository for entities in your projects. That means, that if you have some auxiliary table in your app, then it probably should not have a repository. But, things like Users, Products, Orders and others might have their own repository. That does not mean that you will create a lot of classes, but please do not add repositories for every class in your system . If you want to read more, please, look into the Domain-Driven Development books.","title":"How to create  my repositories?"},{"location":"patterns/database/#specification","text":"Specification is the pattern that adds values, filters, joins or anything else to the query in your repository. It can also work as a filter for your objects.","title":"Specification"},{"location":"patterns/database/#class-based-specifications","text":"class Specification(ABC): @abstractmethod def apply(self, query): raise NotImplementedError(\"Specification must specify apply()\") def __call__(self, query): return self.apply(query) apply(query) -> query apply is the main functions that you are going to override. It is used in order to specify new things in your query. You get the query in the specification and return an updated version of query. For example, if your query has a filter function, and you want to filter by username, then you can create this: from assimilator.core.database import Specification class UsernameSpecification(Specification): def __init__(self, username: str): super(UsernameSpecification, self).__init__() self.username = username def apply(self, query): return query.filter(username=username) Here we do the following: 1) save the username in the constructor 2) override apply() and return an updated version of the query provided The usage of this specification will look like this: repository = UserRepository(session) user = repository.get(UsernameSpecification(username=\"python.on.papyrus\")) The repository will apply the specification and return the results.","title":"Class-based specifications"},{"location":"patterns/database/#functional-specifications","text":"__call__() functions receives the query and call apply() in the specification. This is used in order to make functional specifications possible. __call__() allows you to call an object as a function: obj() . This way, we can use functional specifications and class-based specifications together. If you want to create a functional specification, then you need to use the @specification decorator: from assimilator.core.database import specification @specification def username_filter(query, username: str): return query.filter(username=username) The function above is the equivalent of the UsernameSpecification class. Then, you are going to use it like this: repository = UserRepository(session) user = repository.get(username_filter(username=\"python.on.papyrus\")) Both types work the same, so you can choose the type of specifications that you like. But, you can also use them together: repository = UserRepository(session) user = repository.get( username_filter(username=\"python.on.papyrus\"), AgeGreaterSpecification(age_gt=18), JoinSpecification(Friends), )","title":"Functional specifications"},{"location":"patterns/database/#specificationlist","text":"SpecificationList is a static class that contains basic specifications for our repository. Specifications:","title":"SpecificationList"},{"location":"patterns/database/#filter_1","text":"filters the data","title":"filter()"},{"location":"patterns/database/#order","text":"orders the data","title":"order()"},{"location":"patterns/database/#paginate","text":"paginates the data(limits the results, offsets them).","title":"paginate()"},{"location":"patterns/database/#join","text":"joins entities together(join a table, get related data). The reason we use SpecificationList is because we want to have an abstraction for our specifications. Take two examples: from dependencies import UserRepository from assimilator.alchemy.database import alchemy_filter def get_user(id: int, repository: UserRepository): return repository.get(alchemy_filter(id=id)) # we use alchemy_filter() directly In that example, we use alchemy_filter() directly, which may not seem as an issue, however, if we would want to change our UserRepository to work with RedisRepository , then we would have to change all of our specifications ourselves. In order to fix this, we can use SpecificationList: from dependencies import UserRepository def get_user(id: int, repository: UserRepository): return repository.get(repository.specifications.filter(id=id)) # we call the filter from repository specifications. Now, we only have to change the repository without worrying about other parts of the code. Here is how you can create your own SpecificationList: from assimilator.core.database import SpecificationList, specification, Specification from pagination_func import paginate_data @specification def filter_specification(filters, query): return query.do_filter(filters) @specification def join_specification(query): return query # we do not need joins in our data structure, so we leave it class OrderSpecification(Specification): def __init__(self, orders): self.orders = orders def apply(self, query): return query.make_order(self.orders) class MySpecificationList(SpecificationList): filter = filter_specification # we use function name as the specification order = OrderSpecification # lambda specification paginate = paginate_data # imported specification join = join_specification Notice that we never call the functions, cause the only thing we need are links to the specifications. Then, when you build your Repository: from specifications import MySpecificationList repository = MyRepository(session=session, specifications=MySpecificationList) Once you have done that, the repository will use your specifications. Of course, you can still use specifications directly, but if you ever need to change the repository, then it may be a problem.","title":"join()"},{"location":"patterns/database/#lazycommand","text":"Sometimes we don't want to execute the query right away. For example, for optimization purposes or some other purpose that requires us to delay the execution. In that case, you want to find lazy argument in the function that you are calling and set it to True . After that, a LazyCommand is going to be returned. That object allows you to call it as a function or iterate over it to get the results: from assimilator.core.database import BaseRepository def print_all_usernames(repository: BaseRepository): for user in repository.filter(lazy=True): print(user.username) # we don't want to receive a list of all the users, but want to iterate # through it and only get 1 user at a time def count_users_if_argument_true(do_count, repository: BaseRepository): count_command = repository.count(lazy=True) # turn on lazy and get LazyCommand if do_count: return count_command() # call for the result return -1","title":"LazyCommand"},{"location":"patterns/database/#unit-of-work","text":"Unit of work allows us to work with transactions and repositories that change the data. The problem with repository is the transaction management. We want to make our transaction management as easy as possible, but repositories are not responsible for that. That is why we have units of work. They allow us to do the following: 1. Start the transaction 2. Provide the repository to the client code 3. If there are exceptions, errors, issues with the client code, rollback the transaction and remove all the changes 4. If everything is good, the client code calls the commit() function and finishes the data change 5. Unit of work closes the transaction","title":"Unit of Work"},{"location":"patterns/database/#__init___1","text":"repository: BaseRepository - The repository is provided in the UnitOfWork when we create it. The session to the data storage is going to be taken from the repository.","title":"__init__()"},{"location":"patterns/database/#begin","text":"Starts the transaction. The function is called automatically.","title":"begin()"},{"location":"patterns/database/#rollback","text":"Removes all the changes from the transaction. You do not need to call that function, as it is called automatically if there is an error in your code.","title":"rollback()"},{"location":"patterns/database/#commit","text":"Saves the changes to the data storage. While the repository only adds the temporary, this function is responsible for the final save. You need to call it yourself, it will not be called automatically like rollback()","title":"commit()"},{"location":"patterns/database/#close","text":"Closes the transaction. The function is called automatically.","title":"close()"},{"location":"patterns/database/#here-is-how-you-can-use-unitofwork-in-your-code","text":"from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user(username: str, uow: UnitOfWork): with uow: # start the transaction # everything in here is within the transaction new_user = User(username=username) uow.repository.save(new_user) # we get the repository from UnitOfWork uow.commit() # commit the changes making them final. If the function is not called, nothing is saved. As you can see, you do not need to call any function except for commit() . You should also use context managers( with uow: ) to start the transaction and rollback if there is an exception: from assimilator.core.database import UnitOfWork from users.unit_of_work import UserUnitOfWork from users.models import User def create_user(username: str, uow: UnitOfWork): with uow: # start the transaction # everything in here is within the transaction new_user = User(username=username) uow.repository.save(new_user) # we get the repository from UnitOfWork 1 / 0 # ZeroDivisionError. UnitOfWork calls rollback automatically. uow.commit() # nothing is saved, since the rollback was called.","title":"Here is how you can use UnitOfWork in your code:"},{"location":"patterns/events/","text":"Events patterns Events and how they work Event shows changes in your system and listeners(consumers) respond to them. Events contain all the possible things that other parts of the system may need once they respond to them. That is useful in lots of systems, and this page will describe the basics of assimilator events. Events use Pydantic module to ease the process of creation, integration and parsing. Event based systems with Assimilator Event - representation of a change in your system that carries all the useful data EventProducer - something that produces events(executes the changes in the system and shows it with events) EventConsumer - something that waits for the producer to emit various events for it to consume them and execute various operations based on the other changes EventBus - combines both producers and consumers in one Entity that can produce and consume simultaneously Event example with user registration: User sends his registration data to our website We create a new user in the database and emit an UserCreated event using an EventProducer EventConsumers listen to our UserCreated event and executes all the operations that must be done once the user is registered Event id: int Unique identification for the event. event_name: str Name of the event. We can have different events in our system. For example, if we have an event for User creation and an event for User deletion, then we can name them: User creation: event_name = user_created User deletion: event_name = user_deleted Those names can help us sort and only listen to specific kind of events. All the names must be in the past, since an event is the change in the past. event_date: datetime Date of the event. You don't need to change this field since it is assigned by default when an event is created. from_json() from_json() is a function that is used to convert json data to an event. That method is in the JSONParsedMixin class, and it allows us to quickly convert json to a Python object. cls: Type['BaseModel'] - Any Pydantic class, but typically an Event data: str - json data for our event Create a custom event events.py : from assimilator.core.events import Event class UserCreated(Event): user_id: int username: str email: str # all the data that could be useful is in the event. # Since Event is a Pydantic model, we can just create new fields like this logic.py : from assimilator.core.database import UnitOfWork from events import UserCreated from models import User def create_user(username: str, email: str, uow: UnitOfWork): with uow: user = User(username=username, email=email) uow.repository.save(user) uow.commit() # Refresh the object and get the user id from the database uow.repository.refresh(user) event = UserCreated( # we create an event user_id=user.id, username=user.username, email=user.email, ) In that example, we only create an event without publishing it anywhere. Find out how to emit your events below. EventConsumer EventConsumer reads all the incoming events and yields them to the functions that use it. start() Starts the event consumer by connecting to all the required systems close() Closes the consumer and finishes the work consume() Yields incoming events EventConsumer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventConsumer : events_bus.py : from assimilator.core.events import EventConsumer, ExternalEvent class MyEventConsumer(EventConsumer): def __init__(self, api): # some object that connects to an external system self.api = api def start(self) -> None: self.api.connect() def close(self) -> None: self.api.disconnect() def consume(self): while True: message = self.api.listen() # we receive a message from the API yield ExternalEvent(**message.convert_to_json()) # parse it logic.py : from events_bus import MyEventConsumer def consume_events(consumer: MyEventConsumer): with consumer: for event in events_bus.consume(): if event.event_name == \"user_created\": user_created_handler(UserCreated(**event.data)) elif event.event_name == \"user_deleted\": user_deleted_handler(UserDeleted(**event.data)) We create a new EventConsumer called MyEventConsumer . Then, we use an api object to implement all the functions. After that, we use it in logic.py file where we consume all the events and handle them depending on the event_name . As you have already noticed, we use something called an ExternalEvent . We do that because all the events that are coming from external sources are unidentified and can only use specific later. ExternalEvent contains all the event data in the data: dict field which can be used later. ExternalEvent When we listen to external systems, it is sometimes hard to make an event class that represents a specific class. That is why we use an ExternalEvent . It contains all the data in the data: dict field, which can be accessed later in order to use an event class that represents that specific event. data: dict - all the data for the event AckEvent AckEvent is an event that has acknowledgement in it. If you want to show that your event was processed(acknowledged), then use AckEvent . ack: bool - whether an event was processed. False by default EventProducer EventProducer is the class that produces all the events and sends them. start() Starts the event producer by connecting to all the required systems. close() Closes the producer and finishes the work. produce() Sends an event to an external system for it to be consumed. event: Event - the event that must be sent. EventProducer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventProducer : events_bus.py : from assimilator.core.events import EventProducer class MyEventProducer(EventProducer): def __init__(self, api): # some object that connects to an external system self.api = api def start(self) -> None: self.api.connect() def close(self) -> None: self.api.disconnect() def produce(self, event: Event) -> None: self.api.send_event(event.json()) # parse event to json and send it logic.py : from events_bus import MyEventProducer from events import UserCreated from models import User def create_user( username: str, email: str, uow: UnitOfWork, producer: MyEventProducer, ): with uow: user = User(username=username, email=email) uow.repository.save(user) uow.commit() # Refresh the object and get the user id from the database uow.repository.refresh(user) with producer: producer.produce( UserCreated( # we create an event user_id=user.id, username=user.username, email=user.email, ) ) # send an event to an external system ExternalEvent must not be used in the producer, since when we emit the events we are the ones creating them, so we have a separate class for them with all the data inside. EventBus EventBus combines both EventProducer and EventConsumer together. You can use those classes separately, but sometimes you need one object that combines them. __init__() consumer: EventConsumer - the consumer that we want to use producer: EventProducer - the producer that we want to use produce() produces the event using producer event: Event - an event that has to be emitted consume() consumes the events using consumer . Returns an Iterator[Event] Event fails and transaction management Sometimes we want to be sure that our events are emitted. But, if we use normal event producers and Unit Of Work separately, we may run into a problem: 1) User is created(added in the database and unit of work committed it) 2) Event producer encounters an error(the event is not published) 3) Inconsistency: User exists, but consumers do not know about that Because of that, we may employ Outbox Relay. It is a pattern that allows us to save all the events in the database in the same transaction as the main entity. Then, another program(thread, task, function) gets all the events from the database and ensures that they are published. We basically save the events to the database in one transaction, emit them in a separate thing and delete them afterwards. OutboxRelay This class gets all the events using UnitOfWork provided, emits all events, and acknowledges them. __init__() uow: UnitOfWork - unit of work that is used in order to get the events, acknowledge them producer: EventProducer - event producer that we use to publish the events start() Start the relay. This function must run forever, must get the events from the repository from unit of work, and produce the events. After that, it must call acknowledge() to show that these events are produced. acknowledge() Acknowledges the events in the database. It might change a boolean column for these events, might delete them, but the idea is that those events will not be produced twice. events: Iterable[Event] - events that must be acknowledged","title":"Events"},{"location":"patterns/events/#events-patterns","text":"","title":"Events patterns"},{"location":"patterns/events/#events-and-how-they-work","text":"Event shows changes in your system and listeners(consumers) respond to them. Events contain all the possible things that other parts of the system may need once they respond to them. That is useful in lots of systems, and this page will describe the basics of assimilator events. Events use Pydantic module to ease the process of creation, integration and parsing.","title":"Events and how they work"},{"location":"patterns/events/#event-based-systems-with-assimilator","text":"Event - representation of a change in your system that carries all the useful data EventProducer - something that produces events(executes the changes in the system and shows it with events) EventConsumer - something that waits for the producer to emit various events for it to consume them and execute various operations based on the other changes EventBus - combines both producers and consumers in one Entity that can produce and consume simultaneously","title":"Event based systems with Assimilator"},{"location":"patterns/events/#event-example-with-user-registration","text":"User sends his registration data to our website We create a new user in the database and emit an UserCreated event using an EventProducer EventConsumers listen to our UserCreated event and executes all the operations that must be done once the user is registered","title":"Event example with user registration:"},{"location":"patterns/events/#event","text":"","title":"Event"},{"location":"patterns/events/#id-int","text":"Unique identification for the event.","title":"id: int"},{"location":"patterns/events/#event_name-str","text":"Name of the event. We can have different events in our system. For example, if we have an event for User creation and an event for User deletion, then we can name them: User creation: event_name = user_created User deletion: event_name = user_deleted Those names can help us sort and only listen to specific kind of events. All the names must be in the past, since an event is the change in the past.","title":"event_name: str"},{"location":"patterns/events/#event_date-datetime","text":"Date of the event. You don't need to change this field since it is assigned by default when an event is created.","title":"event_date: datetime"},{"location":"patterns/events/#from_json","text":"from_json() is a function that is used to convert json data to an event. That method is in the JSONParsedMixin class, and it allows us to quickly convert json to a Python object. cls: Type['BaseModel'] - Any Pydantic class, but typically an Event data: str - json data for our event","title":"from_json()"},{"location":"patterns/events/#create-a-custom-event","text":"events.py : from assimilator.core.events import Event class UserCreated(Event): user_id: int username: str email: str # all the data that could be useful is in the event. # Since Event is a Pydantic model, we can just create new fields like this logic.py : from assimilator.core.database import UnitOfWork from events import UserCreated from models import User def create_user(username: str, email: str, uow: UnitOfWork): with uow: user = User(username=username, email=email) uow.repository.save(user) uow.commit() # Refresh the object and get the user id from the database uow.repository.refresh(user) event = UserCreated( # we create an event user_id=user.id, username=user.username, email=user.email, ) In that example, we only create an event without publishing it anywhere. Find out how to emit your events below.","title":"Create a custom event"},{"location":"patterns/events/#eventconsumer","text":"EventConsumer reads all the incoming events and yields them to the functions that use it.","title":"EventConsumer"},{"location":"patterns/events/#start","text":"Starts the event consumer by connecting to all the required systems","title":"start()"},{"location":"patterns/events/#close","text":"Closes the consumer and finishes the work","title":"close()"},{"location":"patterns/events/#consume","text":"Yields incoming events EventConsumer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventConsumer : events_bus.py : from assimilator.core.events import EventConsumer, ExternalEvent class MyEventConsumer(EventConsumer): def __init__(self, api): # some object that connects to an external system self.api = api def start(self) -> None: self.api.connect() def close(self) -> None: self.api.disconnect() def consume(self): while True: message = self.api.listen() # we receive a message from the API yield ExternalEvent(**message.convert_to_json()) # parse it logic.py : from events_bus import MyEventConsumer def consume_events(consumer: MyEventConsumer): with consumer: for event in events_bus.consume(): if event.event_name == \"user_created\": user_created_handler(UserCreated(**event.data)) elif event.event_name == \"user_deleted\": user_deleted_handler(UserDeleted(**event.data)) We create a new EventConsumer called MyEventConsumer . Then, we use an api object to implement all the functions. After that, we use it in logic.py file where we consume all the events and handle them depending on the event_name . As you have already noticed, we use something called an ExternalEvent . We do that because all the events that are coming from external sources are unidentified and can only use specific later. ExternalEvent contains all the event data in the data: dict field which can be used later.","title":"consume()"},{"location":"patterns/events/#externalevent","text":"When we listen to external systems, it is sometimes hard to make an event class that represents a specific class. That is why we use an ExternalEvent . It contains all the data in the data: dict field, which can be accessed later in order to use an event class that represents that specific event. data: dict - all the data for the event","title":"ExternalEvent"},{"location":"patterns/events/#ackevent","text":"AckEvent is an event that has acknowledgement in it. If you want to show that your event was processed(acknowledged), then use AckEvent . ack: bool - whether an event was processed. False by default","title":"AckEvent"},{"location":"patterns/events/#eventproducer","text":"EventProducer is the class that produces all the events and sends them.","title":"EventProducer"},{"location":"patterns/events/#start_1","text":"Starts the event producer by connecting to all the required systems.","title":"start()"},{"location":"patterns/events/#close_1","text":"Closes the producer and finishes the work.","title":"close()"},{"location":"patterns/events/#produce","text":"Sends an event to an external system for it to be consumed. event: Event - the event that must be sent. EventProducer uses StartCloseContextMixin class that allows us to use context managers(with) without calling start() or close() ourselves Here is an example of how you would create and use your EventProducer : events_bus.py : from assimilator.core.events import EventProducer class MyEventProducer(EventProducer): def __init__(self, api): # some object that connects to an external system self.api = api def start(self) -> None: self.api.connect() def close(self) -> None: self.api.disconnect() def produce(self, event: Event) -> None: self.api.send_event(event.json()) # parse event to json and send it logic.py : from events_bus import MyEventProducer from events import UserCreated from models import User def create_user( username: str, email: str, uow: UnitOfWork, producer: MyEventProducer, ): with uow: user = User(username=username, email=email) uow.repository.save(user) uow.commit() # Refresh the object and get the user id from the database uow.repository.refresh(user) with producer: producer.produce( UserCreated( # we create an event user_id=user.id, username=user.username, email=user.email, ) ) # send an event to an external system ExternalEvent must not be used in the producer, since when we emit the events we are the ones creating them, so we have a separate class for them with all the data inside.","title":"produce()"},{"location":"patterns/events/#eventbus","text":"EventBus combines both EventProducer and EventConsumer together. You can use those classes separately, but sometimes you need one object that combines them.","title":"EventBus"},{"location":"patterns/events/#__init__","text":"consumer: EventConsumer - the consumer that we want to use producer: EventProducer - the producer that we want to use","title":"__init__()"},{"location":"patterns/events/#produce_1","text":"produces the event using producer event: Event - an event that has to be emitted","title":"produce()"},{"location":"patterns/events/#consume_1","text":"consumes the events using consumer . Returns an Iterator[Event]","title":"consume()"},{"location":"patterns/events/#event-fails-and-transaction-management","text":"Sometimes we want to be sure that our events are emitted. But, if we use normal event producers and Unit Of Work separately, we may run into a problem: 1) User is created(added in the database and unit of work committed it) 2) Event producer encounters an error(the event is not published) 3) Inconsistency: User exists, but consumers do not know about that Because of that, we may employ Outbox Relay. It is a pattern that allows us to save all the events in the database in the same transaction as the main entity. Then, another program(thread, task, function) gets all the events from the database and ensures that they are published. We basically save the events to the database in one transaction, emit them in a separate thing and delete them afterwards.","title":"Event fails and transaction management"},{"location":"patterns/events/#outboxrelay","text":"This class gets all the events using UnitOfWork provided, emits all events, and acknowledges them.","title":"OutboxRelay"},{"location":"patterns/events/#__init___1","text":"uow: UnitOfWork - unit of work that is used in order to get the events, acknowledge them producer: EventProducer - event producer that we use to publish the events","title":"__init__()"},{"location":"patterns/events/#start_2","text":"Start the relay. This function must run forever, must get the events from the repository from unit of work, and produce the events. After that, it must call acknowledge() to show that these events are produced.","title":"start()"},{"location":"patterns/events/#acknowledge","text":"Acknowledges the events in the database. It might change a boolean column for these events, might delete them, but the idea is that those events will not be produced twice. events: Iterable[Event] - events that must be acknowledged","title":"acknowledge()"}]}